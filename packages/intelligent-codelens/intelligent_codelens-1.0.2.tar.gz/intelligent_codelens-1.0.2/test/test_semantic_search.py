#!/usr/bin/env python3
"""
è¯­ä¹‰æœç´¢å¼•æ“æµ‹è¯•è„šæœ¬
æµ‹è¯•è¯­ä¹‰æœç´¢ã€å‘é‡åŒ–ã€ç›¸ä¼¼åº¦è®¡ç®—ç­‰åŠŸèƒ½
"""

import os
import sys
import tempfile
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from semantic_search import SemanticSearchEngine
from database import CodeDatabase

def create_test_repository():
    """åˆ›å»ºæµ‹è¯•ä»£ç ä»“åº“"""
    test_dir = tempfile.mkdtemp(prefix="semantic_test_")
    
    # åˆ›å»ºå¤šä¸ªä¸åŒåŠŸèƒ½çš„ä»£ç æ–‡ä»¶
    test_files = {
        'math_operations.py': '''
def calculate_sum(numbers):
    """è®¡ç®—æ•°å­—åˆ—è¡¨çš„æ€»å’Œ"""
    return sum(numbers)

def calculate_average(numbers):
    """è®¡ç®—æ•°å­—åˆ—è¡¨çš„å¹³å‡å€¼"""
    if not numbers:
        return 0
    return sum(numbers) / len(numbers)

def find_maximum(numbers):
    """æ‰¾åˆ°æ•°å­—åˆ—è¡¨ä¸­çš„æœ€å¤§å€¼"""
    return max(numbers) if numbers else None

def find_minimum(numbers):
    """æ‰¾åˆ°æ•°å­—åˆ—è¡¨ä¸­çš„æœ€å°å€¼"""
    return min(numbers) if numbers else None
''',
        
        'string_utils.py': '''
def reverse_string(text):
    """åè½¬å­—ç¬¦ä¸²"""
    return text[::-1]

def capitalize_words(text):
    """å°†å­—ç¬¦ä¸²ä¸­æ¯ä¸ªå•è¯çš„é¦–å­—æ¯å¤§å†™"""
    return ' '.join(word.capitalize() for word in text.split())

def count_characters(text):
    """ç»Ÿè®¡å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦æ•°é‡"""
    return len(text)

def remove_whitespace(text):
    """ç§»é™¤å­—ç¬¦ä¸²ä¸­çš„ç©ºç™½å­—ç¬¦"""
    return ''.join(text.split())
''',
        
        'file_operations.py': '''
import os
import json

def read_text_file(file_path):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def write_text_file(file_path, content):
    """å†™å…¥æ–‡æœ¬åˆ°æ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

def load_json_data(file_path):
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json_data(file_path, data):
    """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
''',
        
        'data_structures.py': '''
class Stack:
    """æ ˆæ•°æ®ç»“æ„å®ç°"""
    
    def __init__(self):
        self.items = []
    
    def push(self, item):
        """å‘æ ˆä¸­æ·»åŠ å…ƒç´ """
        self.items.append(item)
    
    def pop(self):
        """ä»æ ˆä¸­ç§»é™¤å¹¶è¿”å›é¡¶éƒ¨å…ƒç´ """
        return self.items.pop() if self.items else None
    
    def peek(self):
        """æŸ¥çœ‹æ ˆé¡¶å…ƒç´ ä½†ä¸ç§»é™¤"""
        return self.items[-1] if self.items else None

class Queue:
    """é˜Ÿåˆ—æ•°æ®ç»“æ„å®ç°"""
    
    def __init__(self):
        self.items = []
    
    def enqueue(self, item):
        """å‘é˜Ÿåˆ—æœ«å°¾æ·»åŠ å…ƒç´ """
        self.items.append(item)
    
    def dequeue(self):
        """ä»é˜Ÿåˆ—å‰ç«¯ç§»é™¤å¹¶è¿”å›å…ƒç´ """
        return self.items.pop(0) if self.items else None
'''
    }
    
    # å†™å…¥æµ‹è¯•æ–‡ä»¶
    for filename, content in test_files.items():
        with open(os.path.join(test_dir, filename), 'w', encoding='utf-8') as f:
            f.write(content)
    
    return test_dir

def test_search_engine_initialization():
    """æµ‹è¯•æœç´¢å¼•æ“åˆå§‹åŒ–"""
    print("ğŸš€ æµ‹è¯•è¯­ä¹‰æœç´¢å¼•æ“åˆå§‹åŒ–...")
    
    try:
        # ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–æœç´¢å¼•æ“
        search_engine = SemanticSearchEngine()
        
        if hasattr(search_engine, 'db'):
            print("âœ… æœç´¢å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            print(f"   - æ•°æ®åº“è¿æ¥: {type(search_engine.db).__name__}")
        else:
            print("âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        
        if hasattr(search_engine, 'nlp'):
            print(f"âœ… NLPæ¨¡å‹çŠ¶æ€: {'å·²åŠ è½½' if search_engine.nlp else 'æœªåŠ è½½'}")
        else:
            print("âŒ NLPæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
        
        if hasattr(search_engine, 'search_weights'):
            print("âœ… æœç´¢æƒé‡é…ç½®æˆåŠŸ")
        else:
            print("âŒ æœç´¢æƒé‡é…ç½®å¤±è´¥")
        
    except Exception as e:
        print(f"âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¼‚å¸¸: {e}")
    
    print()

def test_text_vectorization():
    """æµ‹è¯•æ–‡æœ¬å‘é‡åŒ–"""
    print("ğŸ”¢ æµ‹è¯•æ–‡æœ¬å‘é‡åŒ–...")
    
    try:
        search_engine = SemanticSearchEngine()
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "è®¡ç®—æ•°å­—çš„æ€»å’Œ",
            "åè½¬å­—ç¬¦ä¸²",
            "è¯»å–æ–‡ä»¶å†…å®¹",
            "æ ˆæ•°æ®ç»“æ„",
            "def calculate_sum(numbers): return sum(numbers)"
        ]
        
        for text in test_texts:
            try:
                # æµ‹è¯•æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆé—´æ¥æµ‹è¯•å‘é‡åŒ–ï¼‰
                similarity = search_engine._text_similarity(text, text)
                if similarity > 0:
                    print(f"âœ… æ–‡æœ¬å¤„ç†æˆåŠŸ: '{text[:20]}...' -> ç›¸ä¼¼åº¦: {similarity:.3f}")
                else:
                    print(f"âŒ æ–‡æœ¬å¤„ç†å¤±è´¥: '{text[:20]}...'")
            except Exception as e:
                print(f"âŒ æ–‡æœ¬å¤„ç†å¼‚å¸¸: '{text[:20]}...' - {e}")
        
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–æµ‹è¯•å¼‚å¸¸: {e}")
    
    print()

def test_semantic_search():
    """æµ‹è¯•è¯­ä¹‰æœç´¢åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•è¯­ä¹‰æœç´¢åŠŸèƒ½...")
    
    test_dir = create_test_repository()
    
    try:
        # ä½¿ç”¨é»˜è®¤æœç´¢å¼•æ“ï¼ˆä¼šä½¿ç”¨é»˜è®¤æ•°æ®åº“ï¼‰
        search_engine = SemanticSearchEngine()
        
        # æ‰‹åŠ¨æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®åˆ°é»˜è®¤æ•°æ®åº“
        test_functions = [
            {
                'file_path': '/test/math_operations.py',
                'language': 'python',
                'content': 'def calculate_sum(numbers): return sum(numbers)',
                'functions': [
                    {
                        'name': 'calculate_sum',
                        'start_line': 1,
                        'end_line': 3,
                        'parameters': ['numbers'],
                        'docstring': 'è®¡ç®—æ•°å­—åˆ—è¡¨çš„æ€»å’Œ',
                        'body': 'return sum(numbers)'
                    }
                ],
                'classes': [],
                'imports': [],
                'comments': []
            },
            {
                'file_path': '/test/string_utils.py',
                'language': 'python',
                'content': 'def reverse_string(text): return text[::-1]',
                'functions': [
                    {
                        'name': 'reverse_string',
                        'start_line': 1,
                        'end_line': 3,
                        'parameters': ['text'],
                        'docstring': 'åè½¬å­—ç¬¦ä¸²',
                        'body': 'return text[::-1]'
                    }
                ],
                'classes': [],
                'imports': [],
                'comments': []
            }
        ]
        
        # å­˜å‚¨æµ‹è¯•æ•°æ®
        for file_data in test_functions:
            search_engine.db.store_file_data(file_data)
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„æœç´¢æŸ¥è¯¢
        search_queries = [
            ("è®¡ç®—æ€»å’Œ", "æ•°å­¦è®¡ç®—ç›¸å…³"),
            ("å­—ç¬¦ä¸²åè½¬", "å­—ç¬¦ä¸²æ“ä½œç›¸å…³"),
            ("sum numbers", "è‹±æ–‡æ•°å­¦è®¡ç®—"),
            ("reverse text", "è‹±æ–‡å­—ç¬¦ä¸²æ“ä½œ"),
            ("ä¸å­˜åœ¨çš„åŠŸèƒ½", "ä¸å­˜åœ¨çš„åŠŸèƒ½æµ‹è¯•")
        ]
        
        for query, description in search_queries:
            try:
                results = search_engine.search(query, limit=3)
                print(f"âœ… æœç´¢ '{query}' ({description}): æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                
                for i, result in enumerate(results, 1):
                    score = result.get('relevance_score', 0)
                    func_name = result.get('name', 'Unknown')
                    file_path = result.get('file_path', 'Unknown')
                    print(f"   {i}. {func_name} (ç›¸å…³åº¦: {score:.3f}) - {file_path}")
                
            except Exception as e:
                print(f"âŒ æœç´¢ '{query}' å¼‚å¸¸: {e}")
        
    except Exception as e:
        print(f"âŒ è¯­ä¹‰æœç´¢æµ‹è¯•å¼‚å¸¸: {e}")
    
    finally:
        # æ¸…ç†
        shutil.rmtree(test_dir)
    
    print()

def test_similarity_calculation():
    """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—"""
    print("ğŸ“ æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—...")
    
    try:
        search_engine = SemanticSearchEngine()
        
        # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
        similarity_tests = [
            ("è®¡ç®—æ€»å’Œ", "sum calculation", "ä¸­è‹±æ–‡ç›¸ä¼¼æ¦‚å¿µ"),
            ("åè½¬å­—ç¬¦ä¸²", "reverse string", "ä¸­è‹±æ–‡ç›¸ä¼¼æ¦‚å¿µ"),
            ("è¯»å–æ–‡ä»¶", "read file", "ä¸­è‹±æ–‡ç›¸ä¼¼æ¦‚å¿µ"),
            ("è®¡ç®—æ€»å’Œ", "å­—ç¬¦ä¸²åè½¬", "ä¸åŒåŠŸèƒ½æ¦‚å¿µ"),
            ("hello world", "ä½ å¥½ä¸–ç•Œ", "ä¸­è‹±æ–‡é—®å€™è¯­")
        ]
        
        for text1, text2, description in similarity_tests:
            try:
                similarity = search_engine._text_similarity(text1, text2)
                print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®— ({description}): {similarity:.3f}")
                print(f"   '{text1}' vs '{text2}'")
                    
            except Exception as e:
                print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¼‚å¸¸: '{text1}' vs '{text2}' - {e}")
        
    except Exception as e:
        print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•å¼‚å¸¸: {e}")
    
    print()

def test_search_performance():
    """æµ‹è¯•æœç´¢æ€§èƒ½"""
    print("âš¡ æµ‹è¯•æœç´¢æ€§èƒ½...")
    
    try:
        search_engine = SemanticSearchEngine()
        
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        print("   åˆ›å»ºæµ‹è¯•æ•°æ®...")
        for i in range(20):  # åˆ›å»º20ä¸ªå‡½æ•°ï¼ˆå‡å°‘æ•°é‡ä»¥æé«˜æµ‹è¯•é€Ÿåº¦ï¼‰
            file_data = {
                'file_path': f'/test/module_{i}.py',
                'language': 'python',
                'content': f'def function_{i}(): pass',
                'functions': [
                    {
                        'name': f'function_{i}',
                        'start_line': 1,
                        'end_line': 1,
                        'parameters': [],
                        'docstring': f'è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•å‡½æ•°',
                        'body': 'pass'
                    }
                ],
                'classes': [],
                'imports': [],
                'comments': []
            }
            search_engine.db.store_file_data(file_data)
        
        # æµ‹è¯•æœç´¢æ€§èƒ½
        import time
        
        queries = ["æµ‹è¯•å‡½æ•°", "function", "ç¬¬10ä¸ª", "pass"]
        
        for query in queries:
            start_time = time.time()
            results = search_engine.search(query, limit=10)
            end_time = time.time()
            
            search_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            print(f"âœ… æœç´¢ '{query}': {len(results)} ä¸ªç»“æœ, è€—æ—¶ {search_time:.2f}ms")
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}")
    
    print()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¯­ä¹‰æœç´¢å¼•æ“æµ‹è¯•")
    print("=" * 50)
    
    test_search_engine_initialization()
    test_text_vectorization()
    test_semantic_search()
    test_similarity_calculation()
    test_search_performance()
    
    print("âœ… è¯­ä¹‰æœç´¢å¼•æ“æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()