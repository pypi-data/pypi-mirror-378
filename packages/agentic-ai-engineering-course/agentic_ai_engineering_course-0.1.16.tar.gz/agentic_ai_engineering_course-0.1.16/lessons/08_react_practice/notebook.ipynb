{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOgiJKN3MqB1"
      },
      "source": [
        "# Lesson 8: ReAct Practice\n",
        "\n",
        "This notebook explores practical the ReAct (Reasoning and Acting) pattern with Google's Gemini API. We will use the `google-genai` library to interact with Gemini models. It includes a mock search tool, a thought generation phase using structured outputs, and an action phase with function calling, all orchestrated by a ReAct control loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Learning Objectives:**\n",
        "\n",
        "1. Understand how ReAct breaks problems into Thought → Action → Observation.\n",
        "2. Practice orchestrating the full ReAct loop end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GRVoMgfLMsBP"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and use it in the Notebook, follow the step-by-step instructions from the [Course Admin](https://academy.towardsai.net/courses/take/agent-engineering/multimedia/67469688-lesson-1-part-2-course-admin) lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To configure the Gemini API, follow the step-by-step instructions from the [Course Admin](https://academy.towardsai.net/courses/take/agent-engineering/multimedia/67469688-lesson-1-part-2-course-admin) lesson.\n",
        "\n",
        "But here is a quick check on what you need to run this Notebook:\n",
        "\n",
        "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  From the root of your project, run: `cp .env.example .env` \n",
        "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
        "\n",
        "Now, the code below will load the key from the `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying to load environment variables from `/Users/omar/Documents/ai_repos/course-ai-agents/.env`\n",
            "Environment variables loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAXJg6nMqB2"
      },
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdZUi2E44tIQ",
        "outputId": "2059055b-6b66-4956-ca87-ce4b78335d34"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from typing import Callable\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from utils import pretty_print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize the Gemini Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOx4eD-yM-jO"
      },
      "outputs": [],
      "source": [
        "client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kLgTbeTBvwt"
      },
      "source": [
        "### Define Constants\n",
        "\n",
        "We will use the `gemini-2.5-flash` model, which is fast and cost-effective:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09szFdH86K6A"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id1R6LRKDRcv"
      },
      "source": [
        "## 2. Tools Definition\n",
        "\n",
        "Let's implement our mock search tool that will serve as the external knowledge source for our agent. This simplified version focuses on the ReAct mechanics rather than real API integration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search(query: str) -> str:\n",
        "    \"\"\"Search for information about a specific topic or query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query or topic to look up.\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # Predefined responses for demonstration\n",
        "    if all(word in query_lower for word in [\"capital\", \"france\"]):\n",
        "        return \"Paris is the capital of France and is known for the Eiffel Tower.\"\n",
        "    elif \"react\" in query_lower:\n",
        "        return \"The ReAct (Reasoning and Acting) framework enables LLMs to solve complex tasks by interleaving thought generation, action execution, and observation processing.\"\n",
        "\n",
        "    # Generic response for unhandled queries\n",
        "    return f\"Information about '{query}' was not found.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We maintain a mapping from tool name to tool function (the tool registry). This lets the model plan with symbolic tool names, while our code safely resolves those names to actual Python functions to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOOL_REGISTRY: dict[str, Callable[..., str]] = {\n",
        "    search.__name__: search,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyqJqrKRMqB5"
      },
      "source": [
        "## 3. ReAct Thought Phase\n",
        "\n",
        "Now let's implement the thought generation phase. This component analyzes the current situation and determines what the agent should do next, potentially suggesting using tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we prepare the prompt for the thinking part. We implement a function that converts the `TOOL_REGISTRY` to a string XML representation of it, which we insert into the prompt. This way, the LLM knows which tools available and can reason around them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tools_xml_description(tool_registry: dict[str, Callable[..., str]]) -> str:\n",
        "    \"\"\"Build a minimal XML description of tools using only their docstrings.\"\"\"\n",
        "    lines = []\n",
        "    for tool_name, fn in tool_registry.items():\n",
        "        doc = (fn.__doc__ or \"\").strip()\n",
        "        lines.append(f'\\t<tool name=\"{tool_name}\">')\n",
        "        if doc:\n",
        "            lines.append(\"\\t\\t<description>\")\n",
        "            for line in doc.split(\"\\n\"):\n",
        "                lines.append(f\"\\t\\t\\t{line}\")\n",
        "            lines.append(\"\\t\\t</description>\")\n",
        "        lines.append(\"\\t</tool>\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# Build a string of XML describing the tools\n",
        "tools_xml = build_tools_xml_description(TOOL_REGISTRY)\n",
        "\n",
        "PROMPT_TEMPLATE_THOUGHT = \"\"\"\n",
        "You are deciding the next best step for reaching the user goal. You have some tools available to you.\n",
        "\n",
        "Available tools:\n",
        "<tools>\n",
        "{tools_xml}\n",
        "</tools>\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "State your next **thought** about what to do next as one short paragraph focused on the next action you intend to take and why.\n",
        "Avoid repeating the same strategies that didn't work previously. Prefer different approaches.\n",
        "\n",
        "Remember:\n",
        "- Return ONLY plain natural language text.\n",
        "- Do NOT emit JSON, XML, function calls, or code.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we `print` the prompt with the tool definitions inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are deciding the next best step for reaching the user goal. You have some tools available to you.\n",
            "\n",
            "Available tools:\n",
            "<tools>\n",
            "\t<tool name=\"search\">\n",
            "\t\t<description>\n",
            "\t\t\tSearch for information about a specific topic or query.\n",
            "\t\t\t\n",
            "\t\t\tArgs:\n",
            "\t\t\t    query (str): The search query or topic to look up.\n",
            "\t\t</description>\n",
            "\t</tool>\n",
            "</tools>\n",
            "\n",
            "Conversation so far:\n",
            "<conversation>\n",
            "\n",
            "</conversation>\n",
            "\n",
            "State your next **thought** about what to do next as one short paragraph focused on the next action you intend to take and why.\n",
            "Avoid repeating the same strategies that didn't work previously. Prefer different approaches.\n",
            "\n",
            "Remember:\n",
            "- Return ONLY plain natural language text.\n",
            "- Do NOT emit JSON, XML, function calls, or code.\n"
          ]
        }
      ],
      "source": [
        "print(PROMPT_TEMPLATE_THOUGHT.format(tools_xml=tools_xml, conversation=\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now implement the `generate_thought` function, which reasons on the best next action to take according to the conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvS9ww0xflyx"
      },
      "outputs": [],
      "source": [
        "def generate_thought(conversation: str, tool_registry: dict[str, Callable[..., str]]) -> str:\n",
        "    \"\"\"Generate a thought as plain text (no structured output).\"\"\"\n",
        "    tools_xml: str = build_tools_xml_description(tool_registry)\n",
        "    prompt: str = PROMPT_TEMPLATE_THOUGHT.format(tools_xml=tools_xml, conversation=conversation)\n",
        "\n",
        "    response: types.GenerateContentResponse = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=prompt,\n",
        "    )\n",
        "    return response.text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtGC4m-Xsz8R"
      },
      "source": [
        "## 4. ReAct Action Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-qP8k3ZMqB5"
      },
      "source": [
        "Next, let's implement the action phase using function calling. This component determines whether to use a tool or provide a final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMH6u54tDQyr"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE_ACTION = \"\"\"\n",
        "You are selecting the best next action to reach the user goal.\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "Respond either with a tool call (with arguments) or a final answer, but only if you can confidently conclude.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Dedicated prompt used when we must force a final answer\n",
        "PROMPT_TEMPLATE_ACTION_FORCED = \"\"\"\n",
        "You must now provide a final answer to the user.\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "Provide a concise final answer that best addresses the user's goal.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "class ToolCallRequest(BaseModel):\n",
        "    \"\"\"A request to call a tool with its name and arguments.\"\"\"\n",
        "\n",
        "    tool_name: str = Field(description=\"The name of the tool to call.\")\n",
        "    arguments: dict = Field(description=\"The arguments to pass to the tool.\")\n",
        "\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    \"\"\"A final answer to present to the user when no further action is needed.\"\"\"\n",
        "\n",
        "    text: str = Field(description=\"The final answer text to present to the user.\")\n",
        "\n",
        "\n",
        "def generate_action(\n",
        "    conversation: str, tool_registry: dict[str, Callable[..., str]] | None = None, force_final: bool = False\n",
        ") -> ToolCallRequest | FinalAnswer:\n",
        "    \"\"\"Generate an action by passing tools to the LLM and parsing function calls or final text.\n",
        "\n",
        "    When force_final is True or no tools are provided, the model is instructed to produce a final answer\n",
        "    and tool calls are disabled.\n",
        "    \"\"\"\n",
        "    # Use a dedicated prompt when forcing a final answer or no tools are provided\n",
        "    if force_final or not tool_registry:\n",
        "        prompt: str = PROMPT_TEMPLATE_ACTION_FORCED.format(conversation=conversation)\n",
        "        response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
        "        return FinalAnswer(text=response.text.strip())\n",
        "\n",
        "    # Default action prompt\n",
        "    prompt = PROMPT_TEMPLATE_ACTION.format(conversation=conversation)\n",
        "\n",
        "    # Provide the available tools to the model; disable auto-calling so we can parse and run it ourselves\n",
        "    tools: list[Callable[..., str]] = list(tool_registry.values())\n",
        "    config = types.GenerateContentConfig(\n",
        "        tools=tools, automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n",
        "    )\n",
        "    response: types.GenerateContentResponse = client.models.generate_content(\n",
        "        model=MODEL_ID, contents=prompt, config=config\n",
        "    )\n",
        "\n",
        "    # From the reponse, we parse each \"part\" and check if it's a function call\n",
        "    candidate = response.candidates[0]\n",
        "    for part in candidate.content.parts:\n",
        "        if getattr(part, \"function_call\", None):\n",
        "            name = part.function_call.name\n",
        "            args = dict(part.function_call.args or {})\n",
        "            return ToolCallRequest(tool_name=name, arguments=args)\n",
        "\n",
        "    # Otherwise, it's a final answer\n",
        "    final_answer = \"\".join(part.text for part in candidate.content.parts)\n",
        "    return FinalAnswer(text=final_answer.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why we provide an option to force the final answer? In a ReAct loop we sometimes need to terminate cleanly after a budget of turns (e.g., to avoid infinite loops or excessive tool calls). The force flag lets us ask the model to conclude with a final answer even if, under normal conditions, it might keep calling tools. This ensures graceful shutdown and a usable output at the end of the loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: In the Action phase we do not inline tool descriptions into the prompt (unlike the Thought phase). Instead, we pass the available Python tool functions through the `tools` parameter to `generate_content`. The client automatically parses these tools and incorporates their definitions/arguments into the model's prompt context, enabling function calling without duplicating tool specs in our prompt text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ReAct Observation Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the third main component of the ReAct loop. In this step, we take the ToolCallRequest created by the generate_action function, run the tool, and return the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def observe(action_request: ToolCallRequest, tool_registry: dict[str, Callable[..., str]]) -> str:\n",
        "    \"\"\"\n",
        "    Execute the selected tool and return the observation text\n",
        "    (either a result or an error message)\n",
        "    \"\"\"\n",
        "    name = action_request.tool_name\n",
        "    args = action_request.arguments\n",
        "\n",
        "    if name not in tool_registry:\n",
        "        return f\"Unknown tool '{name}'. Available: {', '.join(tool_registry)}\"\n",
        "\n",
        "    try:\n",
        "        return tool_registry[name](**args)\n",
        "    except Exception as e:\n",
        "        return f\"Error executing tool '{name}': {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paris is the capital of France and is known for the Eiffel Tower.\n"
          ]
        }
      ],
      "source": [
        "req = ToolCallRequest(tool_name=\"search\", arguments={\"query\": \"capital of France\"})\n",
        "print(observe(req, TOOL_REGISTRY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moxbYJ2_EqYi"
      },
      "source": [
        "## 6. ReAct Control Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmJ6zg2YMqB6"
      },
      "source": [
        "Now we build the main ReAct control loop that orchestrates the Thought → Action → Observation cycle end-to-end. We treat the conversation between the user and the agent as a sequence of messages. Each message is a step in the dialogue, and each step corresponds to one ReAct unit: it can be a user message, an internal thought, a tool request, the tool's observation, or the final answer.\n",
        "\n",
        "We'll start by defining the data structures for these messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH0rKt2Pcf3W"
      },
      "outputs": [],
      "source": [
        "class MessageRole(str, Enum):\n",
        "    \"\"\"Enumeration for the different roles a message can have.\"\"\"\n",
        "\n",
        "    USER = \"user\"\n",
        "    THOUGHT = \"thought\"\n",
        "    TOOL_REQUEST = \"tool request\"\n",
        "    OBSERVATION = \"observation\"\n",
        "    FINAL_ANSWER = \"final answer\"\n",
        "\n",
        "\n",
        "class Message(BaseModel):\n",
        "    \"\"\"A message with a role and content, used for all message types.\"\"\"\n",
        "\n",
        "    role: MessageRole = Field(description=\"The role of the message in the ReAct loop.\")\n",
        "    content: str = Field(description=\"The textual content of the message.\")\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"Provides a user-friendly string representation of the message.\"\"\"\n",
        "        return f\"{self.role.value.capitalize()}: {self.content}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also add a small printer that uses our `pretty_print` module to render each message nicely in the notebook. This makes it easy to follow how the agent alternates between Thought, Action (tool call), and Observation across turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretty_print_message(\n",
        "    message: Message,\n",
        "    turn: int,\n",
        "    max_turns: int,\n",
        "    header_color: str = pretty_print.Color.YELLOW,\n",
        "    is_forced_final_answer: bool = False,\n",
        ") -> None:\n",
        "    if not is_forced_final_answer:\n",
        "        title = f\"{message.role.value.capitalize()} (Turn {turn}/{max_turns}):\"\n",
        "    else:\n",
        "        title = f\"{message.role.value.capitalize()} (Forced):\"\n",
        "\n",
        "    pretty_print.wrapped(\n",
        "        text=message.content,\n",
        "        title=title,\n",
        "        header_color=header_color,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use a `Scratchpad` class that wraps a list of `Message` objects and provides `append(..., verbose=False)` to both store and (optionally) pretty-print messages with role-based colors. The scratchpad is serialized each turn so the model can plan the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Scratchpad:\n",
        "    \"\"\"Container for ReAct messages with optional pretty-print on append.\"\"\"\n",
        "\n",
        "    def __init__(self, max_turns: int) -> None:\n",
        "        self.messages: list[Message] = []\n",
        "        self.max_turns: int = max_turns\n",
        "        self.current_turn: int = 1\n",
        "\n",
        "    def set_turn(self, turn: int) -> None:\n",
        "        self.current_turn = turn\n",
        "\n",
        "    def append(self, message: Message, verbose: bool = False, is_forced_final_answer: bool = False) -> None:\n",
        "        self.messages.append(message)\n",
        "        if verbose:\n",
        "            role_to_color = {\n",
        "                MessageRole.USER: pretty_print.Color.RESET,\n",
        "                MessageRole.THOUGHT: pretty_print.Color.ORANGE,\n",
        "                MessageRole.TOOL_REQUEST: pretty_print.Color.GREEN,\n",
        "                MessageRole.OBSERVATION: pretty_print.Color.YELLOW,\n",
        "                MessageRole.FINAL_ANSWER: pretty_print.Color.CYAN,\n",
        "            }\n",
        "            header_color = role_to_color.get(message.role, pretty_print.Color.YELLOW)\n",
        "            pretty_print_message(\n",
        "                message=message,\n",
        "                turn=self.current_turn,\n",
        "                max_turns=self.max_turns,\n",
        "                header_color=header_color,\n",
        "                is_forced_final_answer=is_forced_final_answer,\n",
        "            )\n",
        "\n",
        "    def to_string(self) -> str:\n",
        "        return \"\\n\".join(str(m) for m in self.messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now implement the control loop.\n",
        "- On the first turn, we add the user question.\n",
        "- Then, at each turn: (1) we get a Thought from the model; (2) we get an Action. If the action is a `FinalAnswer`, we stop. If it's a `ToolCallRequest`, we execute the tool and append the resulting `Observation`, then continue. If we reach the maximum number of turns, we run the action selector one last time with a flag that forces a final answer (no tool calls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-PSlQ34f1RM"
      },
      "outputs": [],
      "source": [
        "def react_agent_loop(\n",
        "    initial_question: str, tool_registry: dict[str, Callable[..., str]], max_turns: int = 5, verbose: bool = False\n",
        ") -> str | None:\n",
        "    \"\"\"\n",
        "    Implements the main ReAct (Thought -> Action -> Observation) control loop.\n",
        "    Uses a unified message class for the scratchpad.\n",
        "    \"\"\"\n",
        "    scratchpad = Scratchpad(max_turns=max_turns)\n",
        "\n",
        "    # Add the user's question to the scratchpad\n",
        "    user_message = Message(role=MessageRole.USER, content=initial_question)\n",
        "    scratchpad.append(user_message, verbose=verbose)\n",
        "\n",
        "    for turn in range(1, max_turns + 1):\n",
        "        scratchpad.set_turn(turn)\n",
        "\n",
        "        # Generate a thought based on the current scratchpad\n",
        "        thought_content = generate_thought(\n",
        "            scratchpad.to_string(),\n",
        "            tool_registry,\n",
        "        )\n",
        "        thought_message = Message(role=MessageRole.THOUGHT, content=thought_content)\n",
        "        scratchpad.append(thought_message, verbose=verbose)\n",
        "\n",
        "        # Generate an action based on the current scratchpad\n",
        "        action_result = generate_action(\n",
        "            scratchpad.to_string(),\n",
        "            tool_registry=tool_registry,\n",
        "        )\n",
        "\n",
        "        # If the model produced a final answer, return it\n",
        "        if isinstance(action_result, FinalAnswer):\n",
        "            final_answer = action_result.text\n",
        "            final_message = Message(role=MessageRole.FINAL_ANSWER, content=final_answer)\n",
        "            scratchpad.append(final_message, verbose=verbose)\n",
        "            return final_answer\n",
        "\n",
        "        # Otherwise, it is a tool request\n",
        "        if isinstance(action_result, ToolCallRequest):\n",
        "            # Log the tool request\n",
        "            params_str = \", \".join(f\"{k}={repr(v)}\" for k, v in action_result.arguments.items())\n",
        "            scratchpad.append(\n",
        "                Message(role=MessageRole.TOOL_REQUEST, content=f\"{action_result.tool_name}({params_str})\"),\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            # Execute and capture the observation (pure function)\n",
        "            observation_content = observe(action_result, tool_registry)\n",
        "\n",
        "            # Log the observation\n",
        "            scratchpad.append(\n",
        "                Message(role=MessageRole.OBSERVATION, content=observation_content),\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "        # Check if the maximum number of turns has been reached. If so, force the action selector to produce a final answer\n",
        "        if turn == max_turns:\n",
        "            forced_action = generate_action(\n",
        "                scratchpad.to_string(),\n",
        "                force_final=True,\n",
        "            )\n",
        "            if isinstance(forced_action, FinalAnswer):\n",
        "                final_answer = forced_action.text\n",
        "            else:\n",
        "                final_answer = \"Unable to produce a final answer within the allotted turns.\"\n",
        "            final_message = Message(role=MessageRole.FINAL_ANSWER, content=final_answer)\n",
        "            scratchpad.append(final_message, verbose=verbose, is_forced_final_answer=True)\n",
        "            return final_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utJSwfTGTAhh"
      },
      "source": [
        "Let's test our ReAct agent with a simple factual question that requires a search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGC0R4VO4oEo",
        "outputId": "3d245e8e-1c7f-442d-cef9-160ac144abf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/2): -----------------------------------------\u001b[0m\n",
            "  What is the capital of France?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/2): ---------------------------------------\u001b[0m\n",
            "  I should use the `search` tool to find the capital of France, as the user is asking a factual question that can be answered by searching for information.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  search(query='capital of France')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France and is known for the Eiffel Tower.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/2): ---------------------------------------\u001b[0m\n",
            "  Now that I have successfully retrieved the capital of France from the search tool, I should directly state the answer to the user to fulfill their request.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m------------------------------------- Final answer (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# A straightforward question requiring a search.\n",
        "question = \"What is the capital of France?\"\n",
        "final_answer = react_agent_loop(question, TOOL_REGISTRY, max_turns=2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Last, let's test it with a question that our mock search tool doesn't have knowledge about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0ubuT6akHli",
        "outputId": "1b84cc8f-e730-4df7-ace6-941ff7ce4982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/2): -----------------------------------------\u001b[0m\n",
            "  What is the capital of Italy?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/2): ---------------------------------------\u001b[0m\n",
            "  The user is asking a factual question about the capital of Italy, which can be directly answered using the search tool. I will use the search tool to find this information.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  search(query='capital of Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  Information about 'capital of Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/2): ---------------------------------------\u001b[0m\n",
            "  The previous search for \"capital of Italy\" failed. I will try a slightly rephrased and more specific search query, \"Italy capital city,\" to see if it yields the desired information.The previous search for \"capital of Italy\" failed. I will try a slightly rephrased and more specific search query, \"Italy capital city,\" to see if it yields the desired information.The previous search for \"capital of Italy\" failed. I will try a slightly rephrased and more specific search query, \"Italy capital city,\" to see if it yields the desired information.The previous search for \"capital of Italy\" failed. I will try a slightly rephrased and more specific search query, \"Italy capital city,\" to see if it yields the desired information.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  search(query='Italy capital city')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  Information about 'Italy capital city' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m-------------------------------------- Final answer (Forced): --------------------------------------\u001b[0m\n",
            "  I am sorry, but I couldn't find information about the capital of Italy.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# A question about a concept the mock search tool doesn't know.\n",
        "question = \"What is the capital of Italy?\"\n",
        "final_answer = react_agent_loop(question, TOOL_REGISTRY, max_turns=2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the ReAct agent tried different strategies to find an answer for the user query, demonstrating live adaptation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ReAct with a reasoning model (use built‑in “thinking”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we set a thinking budget, and define two helpers. The first helper extracts any thought summary text the API returns. \n",
        "\n",
        "The second finds the first function call in a response when the model decides to use a tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "THINKING_CONFIG = types.ThinkingConfig(\n",
        "    include_thoughts=True,  # human-readable summaries for transparency/debugging\n",
        "    thinking_budget=1024,  # tune for latency vs. depth; -1 lets the model decide\n",
        ")\n",
        "\n",
        "\n",
        "def extract_thought_summary(response: types.GenerateContentResponse) -> str | None:\n",
        "    \"\"\"Collect human-readable thought summaries if present.\"\"\"\n",
        "    parts = getattr(response.candidates[0].content, \"parts\", []) or []\n",
        "    chunks = [p.text for p in parts if getattr(p, \"thought\", False) and getattr(p, \"text\", None)]\n",
        "    return \"\\n\".join(chunks).strip() if chunks else None\n",
        "\n",
        "\n",
        "def extract_first_function_call(response: types.GenerateContentResponse):\n",
        "    \"\"\"Return (name, args) for the first function call, or None if the model produced a final answer.\"\"\"\n",
        "    if getattr(response, \"function_calls\", None):\n",
        "        fc = response.function_calls[0]\n",
        "        return fc.name, dict(fc.args or {})\n",
        "    parts = getattr(response.candidates[0].content, \"parts\", []) or []\n",
        "    for p in parts:\n",
        "        if getattr(p, \"function_call\", None):\n",
        "            return p.function_call.name, dict(p.function_call.args or {})\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we build the request configuration. We provide the Python functions as tools and enable built-in thinking. Automatic function calling is disabled, so we can log each step and run tools ourselves with the `observe` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_config_with_tools(tools: list[Callable[..., str]]) -> types.GenerateContentConfig:\n",
        "    return types.GenerateContentConfig(\n",
        "        tools=tools,\n",
        "        thinking_config=THINKING_CONFIG,\n",
        "        # We disable the automatic execution of tools, we will use the observe function to run them instead.\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following is the alternative ReAct loop. The conversation is maintained as a list of `types.Content`. \n",
        "\n",
        "After each model turn, we append `response.candidates[0].content` back into `contents` to preserve thought signatures. \n",
        "\n",
        "When the model calls a tool, we execute it, log the observation, and then append a `function_response` part so the model can use that observation on the next turn. \n",
        "\n",
        "For the visible trace, we keep using the `Scratchpad` and our `pretty_print_message` helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def react_agent_loop_thinking(\n",
        "    initial_question: str,\n",
        "    tool_registry: dict[str, Callable[..., str]],\n",
        "    max_turns: int = 5,\n",
        "    verbose: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    ReAct loop that relies on model-native reasoning:\n",
        "      - optional thought summaries for visibility,\n",
        "      - thought signatures preserved by appending model Content back into `contents`,\n",
        "      - pretty-printed trace using Lesson 8's Scratchpad utilities.\n",
        "    \"\"\"\n",
        "\n",
        "    scratchpad = Scratchpad(max_turns=max_turns)\n",
        "    scratchpad.append(Message(role=MessageRole.USER, content=initial_question), verbose=verbose)\n",
        "\n",
        "    # Structured \"contents\" conversation for thought signatures\n",
        "    contents: list[types.Content] = [types.Content(role=\"user\", parts=[types.Part(text=initial_question)])]\n",
        "    tools = list(tool_registry.values())\n",
        "    config = build_config_with_tools(tools)\n",
        "\n",
        "    for turn in range(1, max_turns + 1):\n",
        "        scratchpad.set_turn(turn)\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=contents,\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        # 1) Thought summary (if any) — log as your THOUGHT message\n",
        "        thoughts = extract_thought_summary(response)\n",
        "        if thoughts:\n",
        "            scratchpad.append(Message(role=MessageRole.THOUGHT, content=thoughts), verbose=verbose)\n",
        "\n",
        "        # 2) Function/Tool call?\n",
        "        fc = extract_first_function_call(response)\n",
        "        if fc:\n",
        "            name, args = fc\n",
        "\n",
        "            # We keep the model's full response content to preserve the thought signatures\n",
        "            contents.append(response.candidates[0].content)\n",
        "\n",
        "            # Log the tool request\n",
        "            params_str = \", \".join(f\"{k}={repr(v)}\" for k, v in args.items())\n",
        "            scratchpad.append(\n",
        "                Message(role=MessageRole.TOOL_REQUEST, content=f\"{name}({params_str})\"),\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            # Execute the tool\n",
        "            action_request = ToolCallRequest(tool_name=name, arguments=args)\n",
        "            observation = observe(action_request, tool_registry)\n",
        "\n",
        "            # Log observation\n",
        "            scratchpad.append(Message(role=MessageRole.OBSERVATION, content=observation), verbose=verbose)\n",
        "\n",
        "            # Send the function response back (standard function-calling protocol)\n",
        "            fn_resp = types.Part.from_function_response(\n",
        "                name=name,\n",
        "                response={\"result\": observation},\n",
        "            )\n",
        "            contents.append(types.Content(role=\"user\", parts=[fn_resp]))\n",
        "            continue  # next turn\n",
        "\n",
        "        # 3) No function call => final text\n",
        "        final_text = (response.text or \"\").strip()\n",
        "        scratchpad.append(Message(role=MessageRole.FINAL_ANSWER, content=final_text), verbose=verbose)\n",
        "        return final_text\n",
        "\n",
        "    # 4) Forced finish if we hit max turns: disable tool-calling for the last shot\n",
        "    forced_config = types.GenerateContentConfig(\n",
        "        thinking_config=THINKING_CONFIG,\n",
        "        tool_config=types.ToolConfig(\n",
        "            function_calling_config=types.FunctionCallingConfig(mode=types.FunctionCallingConfigMode.NONE)\n",
        "        ),\n",
        "    )\n",
        "    forced_response = client.models.generate_content(model=MODEL_ID, contents=contents, config=forced_config)\n",
        "    final_text = (forced_response.text or \"Unable to produce a final answer within the allotted turns.\").strip()\n",
        "    scratchpad.append(\n",
        "        Message(role=MessageRole.FINAL_ANSWER, content=final_text),\n",
        "        verbose=verbose,\n",
        "        is_forced_final_answer=True,\n",
        "    )\n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s test this new loop using the same questions we used earlier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/3): -----------------------------------------\u001b[0m\n",
            "  What is the capital of France?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/3): ---------------------------------------\u001b[0m\n",
            "  **Let's Find the Capital of France**\n",
            "\n",
            "Okay, I've got a straightforward, factual question here: \"What is the capital of France?\"  This is prime territory for a quick search.  Since I need a direct, verifiable answer, the `default_api.search` tool is definitely the way to go.  To ensure the search yields the most accurate results, I'll structure the query precisely as \"capital of France\". That should provide the necessary information efficiently.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  search(query='capital of France')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France and is known for the Eiffel Tower.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m------------------------------------- Final answer (Turn 2/3): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital of France?\"\n",
        "final_answer = react_agent_loop_thinking(question, TOOL_REGISTRY, max_turns=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the same answer, but now the Thought comes from the summarized version of the model’s internal thinking. Notice how “verbose” these thought summaries are by default. \n",
        "\n",
        "Now let’s ask our agent the second question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/3): -----------------------------------------\u001b[0m\n",
            "  What is the capital of Italy?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/3): ---------------------------------------\u001b[0m\n",
            "  **Navigating a Simple Inquiry**\n",
            "\n",
            "Okay, so I see a straightforward factual question: what's the capital of Italy?  Knowing my way around things, I immediately recognize this as something easily solved with a quick search.  Instead of racking my brain for a fact I should already know, I'll leverage the `search` tool. My internal process is now to formulate the precise query: \"capital of Italy.\"  The `search` tool should be able to instantly provide the answer, and I can then offer it to the user.  Simple and efficient.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  search(query='capital of Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  Information about 'capital of Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/3): ---------------------------------------\u001b[0m\n",
            "  **Frustration with a Simple Search**\n",
            "\n",
            "I was honestly a bit surprised the first search for \"capital of Italy\" failed.  I mean, it's a basic piece of knowledge! The system simply stated the information wasn't available. That's not what I anticipated.  Given my expertise, I know the nuances of search engines.  Perhaps the specificity of the initial query confused the system.  I'll try a broader approach. Let's start with \"Italy\" as the core search term. I'm hoping that general information about Italy will be sufficient to reveal its capital. It's a fundamental fact, and I trust that this strategy will work better.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 2/3): -------------------------------------\u001b[0m\n",
            "  search(query='Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 2/3): -------------------------------------\u001b[0m\n",
            "  Information about 'Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 3/3): ---------------------------------------\u001b[0m\n",
            "  **Troubleshooting a Search Tool's Failure**\n",
            "\n",
            "Okay, so I'm running into a serious issue here. I tried a couple of simple queries, \"capital of Italy\" and \"Italy,\" expecting straightforward factual answers. However, the search tool is returning \"Information about '[query]' was not found\" for both. This strongly indicates a problem with the tool itself – it's either malfunctioning, facing some sort of technical glitch, or perhaps its knowledge base is severely limited.\n",
            "\n",
            "Given my expertise, I know I can't conjure up this information out of thin air. Without a functional search tool, I'm essentially hamstrung when it comes to answering direct factual questions. Therefore, I need to be upfront with the user. I'll have to explain that I'm unable to provide the requested information because the tool I'm relying on isn't working as expected. This will prevent any misunderstanding and allow me to manage expectations realistically. It's a frustrating situation, but transparency is key.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m------------------------------------- Final answer (Turn 3/3): -------------------------------------\u001b[0m\n",
            "  I am sorry, but I was unable to find the capital of Italy using my current tools.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital of Italy?\"\n",
        "final_answer = react_agent_loop_thinking(question, TOOL_REGISTRY, max_turns=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also get the same answer as our classic ReAct agent. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
