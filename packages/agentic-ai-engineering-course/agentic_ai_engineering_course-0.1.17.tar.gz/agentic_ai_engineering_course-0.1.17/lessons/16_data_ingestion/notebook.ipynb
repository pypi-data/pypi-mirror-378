{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 16: Initial Data Ingestion and Tooling\n",
        "\n",
        "In this lesson, we focus on building the first set of essential MCP tools for data gathering in our research agent. We'll implement tools that read article guideline files, extract web URLs programmatically, and scrape their content in parallel. This lesson demonstrates how file-based approaches can save tokens for the orchestrating agent, which only needs to process simple success or failure messages rather than large content blocks.\n",
        "\n",
        "Learning Objectives:\n",
        "- Learn how to build MCP tools that extract URLs and references from text files\n",
        "- Understand the benefits of file-based tool outputs for token efficiency\n",
        "- Implement robust web scraping tools using external services\n",
        "- Handle error cases gracefully thanks to appropriate policies in the MCP prompt instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To run this lesson, you'll need several API keys configured:\n",
        "\n",
        "1. **Gemini API Key**, `GOOGLE_API_KEY` variable: Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2. **Firecrawl API Key**, `FIRECRAWL_API_KEY` variable: Get your key from [Firecrawl](https://firecrawl.dev/). They have a free tier that allows you to scrape 500 pages, which is enough for testing the agent for free.\n",
        "3. **GitHub token (optional)**, `GITHUB_TOKEN` variable: If you want to process private GitHub repositories, you'll need a GitHub token with access to them. In case you want to test this functionality, you can get a token from [here](https://github.com/settings/personal-access-tokens). However, this is not required for the lesson, as we can easily use public repositories for explaining the functionalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables loaded from `/Users/fabio/Desktop/course-ai-agents/.env`\n",
            "Environment variables loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\", \"FIRECRAWL_API_KEY\", \"GITHUB_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply() # Allow nested async usage in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding the Research Agent Workflow\n",
        "\n",
        "As we saw in the previous lesson, the research agent follows a systematic workflow for data ingestion. The MCP prompt defines a clear two-phase approach regarding the data ingestion:\n",
        "\n",
        "- **Step 1**: Extract URLs and file references from the article guidelines.\n",
        "- **Step 2**: Process all the extracted resources in parallel (local files, web URLs, GitHub repos, YouTube videos).\n",
        "\n",
        "Here's a snapshot of the MCP prompt that defines the first two steps of the workflow:\n",
        "```markdown\n",
        "1. Setup:\n",
        "\n",
        "    1.1. Explain to the user the numbered steps of the workflow. Be concise. Keep them numbered so that the user\n",
        "    can easily refer to them later.\n",
        "    \n",
        "    1.2. Ask the user for the research directory, if not provided. Ask the user if any modification is needed for the\n",
        "    workflow (e.g. running from a specific step, or adding user feedback to specific steps).\n",
        "\n",
        "    1.3 Extract the URLs from the ARTICLE_GUIDELINE_FILE with the \"extract_guidelines_urls\" tool. This tool reads the\n",
        "    ARTICLE_GUIDELINE_FILE and extracts three groups of references from the guidelines:\n",
        "    • \"github_urls\" - all GitHub links;\n",
        "    • \"youtube_videos_urls\" - all YouTube video links;\n",
        "    • \"other_urls\" - all remaining HTTP/HTTPS links;\n",
        "    • \"local_files\" - relative paths to local files mentioned in the guidelines (e.g. \"code.py\", \"src/main.py\").\n",
        "    Only extensions allowed are: \".py\", \".ipynb\", and \".md\".\n",
        "    The extracted data is saved to the GUIDELINES_FILENAMES_FILE within the NOVA_FOLDER directory.\n",
        "\n",
        "2. Process the extracted resources in parallel:\n",
        "\n",
        "    You can run the following sub-steps (2.1 to 2.4) in parallel. In a single turn, you can call all the\n",
        "    necessary tools for these steps.\n",
        "\n",
        "    2.1 Local files - run the \"process_local_files\" tool to read every file path listed under \"local_files\" in the\n",
        "    GUIDELINES_FILENAMES_FILE and copy its content into the LOCAL_FILES_FROM_RESEARCH_FOLDER subfolder within\n",
        "    NOVA_FOLDER, giving each copy an appropriate filename (path separators are replaced with underscores).\n",
        "\n",
        "    2.2 Other URL links - run the \"scrape_and_clean_other_urls\" tool to read the `other_urls` list from the\n",
        "    GUIDELINES_FILENAMES_FILE and scrape/clean them. The tool writes the cleaned markdown files inside the\n",
        "    URLS_FROM_GUIDELINES_FOLDER subfolder within NOVA_FOLDER.\n",
        "\n",
        "    2.3 GitHub URLs - run the \"process_github_urls\" tool to process the `github_urls` list from the\n",
        "    GUIDELINES_FILENAMES_FILE with gitingest and save a Markdown summary for each URL inside the\n",
        "    URLS_FROM_GUIDELINES_CODE_FOLDER subfolder within NOVA_FOLDER.\n",
        "\n",
        "    2.4 YouTube URLs - run the \"transcribe_youtube_urls\" tool to process the `youtube_videos_urls` list from the\n",
        "    GUIDELINES_FILENAMES_FILE, transcribe each video, and save the transcript as a Markdown file inside the\n",
        "    URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder within NOVA_FOLDER.\n",
        "        Note: Please be aware that video transcription can be a time-consuming process. For reference,\n",
        "        transcribing a 39-minute video can take approximately 4.5 minutes.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's examine the MCP tools involved in these first two steps of the workflow. As we saw in the previous lesson, the MCP tool endpoints are defined in the `src/routers/tools.py` file.\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/routers/tools.py_\n",
        "\n",
        "```python\n",
        "def register_mcp_tools(mcp: FastMCP) -> None:\n",
        "    \"\"\"Register all MCP tools with the server instance.\"\"\"\n",
        "    \n",
        "    # Step 1: Extract URLs and file references from guidelines\n",
        "    @mcp.tool()\n",
        "    async def extract_guidelines_urls(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract URLs and local file references from article guidelines.\n",
        "        \n",
        "        Reads the ARTICLE_GUIDELINE_FILE file in the research directory and extracts:\n",
        "        - GitHub URLs\n",
        "        - Other HTTP/HTTPS URLs  \n",
        "        - Local file references (files mentioned in quotes with extensions)\n",
        "        \n",
        "        Results are saved to GUIDELINES_FILENAMES_FILE in the research directory.\n",
        "        \"\"\"\n",
        "        result = extract_guidelines_urls_tool(research_directory)\n",
        "        return result\n",
        "\n",
        "    # Step 2.1: Process local files\n",
        "    @mcp.tool()\n",
        "    async def process_local_files(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process local files referenced in the article guidelines.\"\"\"\n",
        "        result = process_local_files_tool(research_directory)\n",
        "        return result\n",
        "        \n",
        "    # Step 2.2: Scrape web URLs\n",
        "    @mcp.tool() \n",
        "    async def scrape_and_clean_other_urls(research_directory: str, concurrency_limit: int = 4) -> Dict[str, Any]:\n",
        "        \"\"\"Scrape and clean other URLs from GUIDELINES_FILENAMES_FILE.\"\"\"\n",
        "        result = await scrape_and_clean_other_urls_tool(research_directory, concurrency_limit)\n",
        "        return result\n",
        "\n",
        "    # Step 2.3: Process GitHub repositories\n",
        "    @mcp.tool()\n",
        "    async def process_github_urls(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process GitHub URLs from GUIDELINES_FILENAMES_FILE using gitingest.\n",
        "        \n",
        "        Reads the GUIDELINES_FILENAMES_FILE file and processes each URL listed\n",
        "        under 'github_urls' using gitingest to extract repository summaries, file trees,\n",
        "        and content. The results are saved as markdown files in the\n",
        "        URLS_FROM_GUIDELINES_CODE_FOLDER subfolder.\n",
        "        \"\"\"\n",
        "        result = await process_github_urls_tool(research_directory)\n",
        "        return result\n",
        "        \n",
        "    # Step 2.4: Transcribe YouTube videos\n",
        "    @mcp.tool()\n",
        "    async def transcribe_youtube_urls(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Transcribe YouTube video URLs from GUIDELINES_FILENAMES_FILE using Gemini 2.5 Pro.\n",
        "        \n",
        "        Reads the GUIDELINES_FILENAMES_FILE file and processes each URL listed\n",
        "        under 'youtube_videos_urls'. Each video is transcribed, and the results are\n",
        "        saved as markdown files in the URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder.\n",
        "        \"\"\"\n",
        "        result = await transcribe_youtube_videos_tool(research_directory)\n",
        "        return result\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how this tool returns a concise summary rather than the full extracted content. We'll see the exact outputs in the next sections. This design choice has several advantages:\n",
        "\n",
        "1. **Token Efficiency**: The agent receives only essential information (counts, status, file path) rather than large content blocks.\n",
        "2. **Context Management**: Keeps the agent's context window manageable for complex workflows.\n",
        "3. **Selective Reading**: The agent can choose to read the output file only if needed for decision-making. However, the ability to read files must be implemented as a tool (or another MCP server) for the MCP client. To do this, it would be possible to add a separate MCP server to the MCP client, or to use an MCP client that has already this capability (e.g. Cursor).\n",
        "4. **Error Handling**: Clear status messages help the agent understand what succeeded or failed, and how to proceed.\n",
        "\n",
        "Let's now see how each of these MCP tools is implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extracting URLs from Guidelines\n",
        "\n",
        "The first tool in our data ingestion pipeline reads an article guideline file and programmatically extracts all URLs and file references it contains.\n",
        "\n",
        "Here's its implementation:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/extract_guidelines_urls_tool.py_\n",
        "\n",
        "```python\n",
        "def extract_guidelines_urls_tool(research_folder: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract URLs and local file references from the article guidelines in the research folder.\n",
        "    \n",
        "    Reads the ARTICLE_GUIDELINE_FILE file and extracts:\n",
        "    - GitHub URLs\n",
        "    - YouTube video URLs  \n",
        "    - Other HTTP/HTTPS URLs\n",
        "    - Local file references\n",
        "    \n",
        "    Results are saved to GUIDELINES_FILENAMES_FILE in the research folder.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "    # Convert to Path object\n",
        "    research_path = Path(research_folder)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "    guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
        "    ...\n",
        "    \n",
        "    # Read guidelines content\n",
        "    guidelines_content = read_file_safe(guidelines_path)\n",
        "    \n",
        "    # Extract URLs\n",
        "    urls = extract_urls(guidelines_content)\n",
        "    github_source_urls = [u for u in all_urls if \"github.com\" in u]\n",
        "    youtube_source_urls = [u for u in all_urls if \"youtube.com\" in u]\n",
        "    web_source_urls = [u for u in all_urls if \"github.com\" not in u and \"youtube.com\" not in u]\n",
        "\n",
        "    # Extract local file paths\n",
        "    local_paths = extract_local_paths(guidelines_content)\n",
        "    \n",
        "    # Prepare the extracted data structure\n",
        "    extracted_data = {\n",
        "        \"github_urls\": urls[\"github_urls\"],\n",
        "        \"youtube_videos_urls\": urls[\"youtube_videos_urls\"], \n",
        "        \"other_urls\": urls[\"other_urls\"],\n",
        "        \"local_file_paths\": local_paths,\n",
        "    }\n",
        "    \n",
        "    # Save to JSON file\n",
        "    output_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"github_sources_count\": len(urls[\"github_urls\"]),\n",
        "        \"youtube_sources_count\": len(urls[\"youtube_videos_urls\"]),\n",
        "        \"web_sources_count\": len(urls[\"other_urls\"]),\n",
        "        \"local_files_count\": len(local_paths),\n",
        "        \"output_path\": str(output_path),\n",
        "        \"message\": f\"Successfully extracted URLs from article guidelines in '{research_folder}'. \"\n",
        "                  f\"Found {len(urls['github_urls'])} GitHub URLs, {len(urls['youtube_videos_urls'])} YouTube videos URLs, \"\n",
        "                  f\"{len(urls['other_urls'])} other URLs, and {len(local_paths)} local file references. \"\n",
        "                  f\"Results saved to: {output_path}\"\n",
        "    }\n",
        "```\n",
        "\n",
        "The code:\n",
        "1. Identifies the location of the article guidelines file,\n",
        "2. Uses the `extract_urls` function to extract the URLs from the guidelines content,\n",
        "3. Extracts local file paths with the `extract_local_paths` function,\n",
        "4. Saves the extracted data to a JSON file, and\n",
        "5. Returns a summary of the results.\n",
        "\n",
        "Let's now see how the URLs are extracted from the guidelines content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 URLs Extraction\n",
        "\n",
        "The `extract_urls` function from the guideline extractions handler finds all HTTP/HTTPS URLs:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/app/guideline_extractions_handler.py_\n",
        "\n",
        "```python\n",
        "def extract_urls(text: str) -> list[str]:\n",
        "    \"\"\"Extract all HTTP/HTTPS URLs from the given text.\"\"\"\n",
        "    url_pattern = re.compile(r\"https?://[^\\s)>\\\"',]+\")\n",
        "    return url_pattern.findall(text)\n",
        "```\n",
        "\n",
        "This regular expression pattern:\n",
        "- `https?://` - Matches both HTTP and HTTPS protocols\n",
        "- `[^\\s)>\\\"',]+` - Matches any characters except whitespace, closing parentheses, greater-than signs, quotes, or commas\n",
        "- This ensures URLs are extracted cleanly from markdown links, plain text, and various formatting contexts\n",
        "\n",
        "After extraction, the URLs are categorized by domain to enable specialized processing for each type of content source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Local File Path Extraction\n",
        "\n",
        "The `extract_local_paths` function is used to extract local file paths from the guidelines content, and it is defined in the `app/guideline_extractions_handler.py` file.\n",
        "\n",
        "We won't show its code here as it's not interesting for teaching how AI agents work. You can check how it works in the code if you're curious. You only need to know the following:\n",
        "- It only looks for specific file extensions (`.py`, `.ipynb`, `.md`)\n",
        "- It excludes anything that looks like a URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Running the Tool\n",
        "\n",
        "Let's test this tool programmatically to get an idea of its output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'success', 'github_sources_count': 1, 'youtube_sources_count': 1, 'web_sources_count': 2, 'local_files_count': 0, 'output_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json', 'message': \"Successfully extracted URLs from article guidelines in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Found 1 GitHub URLs, 1 YouTube videos URLs, 2 other URLs, and 0 local file references. Results saved to: /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json\"}\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import extract_guidelines_urls_tool\n",
        "\n",
        "# Update this path to your actual sample research folder\n",
        "research_folder = \"/path/to/research_folder\"\n",
        "result = extract_guidelines_urls_tool(research_folder=research_folder)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output will show a structured summary like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"status\": \"success\",\n",
        "  \"github_sources_count\": 1,\n",
        "  \"youtube_sources_count\": 2, \n",
        "  \"web_sources_count\": 6,\n",
        "  \"local_files_count\": 0,\n",
        "  \"output_path\": \"/path/to/research_folder/.nova/guidelines_filenames.json\",\n",
        "  \"message\": \"Successfully extracted URLs from article guidelines in '/path/to/research_folder'. Found 1 GitHub URLs, 2 YouTube videos URLs, 6 other URLs, and 0 local file references. Results saved to: /path/to/research_folder/.nova/guidelines_filenames.json\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this summary, the agent can understand if everything worked fine or not, and how to proceed in case of errors (e.g. by asking the user for help)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Processing Local Files\n",
        "\n",
        "The `process_local_files_tool` tool handles local file references found in the guidelines. It copies referenced files to an organized folder structure and formats them for LLM consumption.\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/process_local_files_tool.py_\n",
        "\n",
        "```python\n",
        "def process_local_files_tool(research_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process local files referenced in the article guidelines.\n",
        "\n",
        "    Reads the guidelines JSON file and copies each referenced local file\n",
        "    to the local files subfolder. Path separators in filenames are\n",
        "    replaced with underscores to avoid creating nested folders.\n",
        "\n",
        "    Args:\n",
        "        research_directory: Path to the research directory containing the guidelines JSON file\n",
        "\n",
        "    Returns:\n",
        "        Dict with status, processing results, and file paths\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "    # Convert to Path object\n",
        "    research_path = Path(research_directory)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "\n",
        "    # Look for GUIDELINES_FILENAMES_FILE\n",
        "    metadata_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "\n",
        "    ...\n",
        "\n",
        "    # Load JSON metadata\n",
        "    data = json.loads(metadata_path.read_text(encoding=\"utf-8\"))\n",
        "    local_files = data.get(\"local_files\", [])\n",
        "\n",
        "    if not local_files:\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"message\": f\"No local files to process in research folder '{research_directory}'.\",\n",
        "            \"files_processed\": 0,\n",
        "            \"files_total\": 0,\n",
        "            \"warnings\": [],\n",
        "            \"errors\": [],\n",
        "        }\n",
        "\n",
        "    # Create destination folder if it doesn't exist\n",
        "    dest_folder = nova_path / LOCAL_FILES_FROM_RESEARCH_FOLDER\n",
        "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    processed = 0\n",
        "    warnings = []\n",
        "    errors = []\n",
        "    processed_files = []\n",
        "\n",
        "    # Initialize notebook converter for .ipynb files\n",
        "    notebook_converter = NotebookToMarkdownConverter(include_outputs=True, include_metadata=False)\n",
        "\n",
        "    for rel_path in local_files:\n",
        "        # Local files are relative to the research folder\n",
        "        src_path = research_path / rel_path\n",
        "        ...\n",
        "\n",
        "        # Sanitize destination filename (replace path separators with underscores)\n",
        "        dest_name = rel_path.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "        try:\n",
        "            # Handle .ipynb files specially by converting to markdown\n",
        "            if src_path.suffix.lower() == \".ipynb\":\n",
        "                # Convert .ipynb to .md extension for destination\n",
        "                dest_name = dest_name.rsplit(\".ipynb\", 1)[0] + \".md\"\n",
        "                dest_path = dest_folder / dest_name\n",
        "\n",
        "                # Convert notebook to markdown string\n",
        "                markdown_content = notebook_converter.convert_notebook_to_string(src_path)\n",
        "\n",
        "                # Write markdown content to destination\n",
        "                dest_path.write_text(markdown_content, encoding=\"utf-8\")\n",
        "            else:\n",
        "                # For other file types, copy as before\n",
        "                dest_path = dest_folder / dest_name\n",
        "                shutil.copy2(src_path, dest_path)\n",
        "\n",
        "            processed += 1\n",
        "            processed_files.append(dest_name)\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Failed to process {rel_path}: {str(e)}\")\n",
        "\n",
        "    # Build result message using the dedicated function\n",
        "    result_message = build_result_message(research_directory, processed, local_files, dest_folder, warnings, errors)\n",
        "\n",
        "    return {\n",
        "        \"status\": \"success\" if processed > 0 else \"warning\",\n",
        "        \"files_processed\": processed,\n",
        "        \"files_total\": len(local_files),\n",
        "        \"processed_files\": processed_files,\n",
        "        \"warnings\": warnings,\n",
        "        \"errors\": errors,\n",
        "        \"output_directory\": str(dest_folder.resolve()),\n",
        "        \"message\": result_message,\n",
        "    }\n",
        "```\n",
        "\n",
        "This local file processing tool looks for the local files extracted by the `extract_guidelines_urls_tool` tool and copies them to an organized folder structure. It distinguishes between different file types (where it copy its content as is) and notebooks (where it converts the content to markdown).\n",
        "\n",
        "The `NotebookToMarkdownConverter` class can be found in the `app/notebook_handler.py` file. We won't show its code here as it's not interesting for teaching how AI agents work. You can check how it works in the code if you're curious. You only need to know that it keeps both markdown cells and code cells, and it also keeps the outputs of the executed cells truncated to a maximum amount of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Web Scraping with Firecrawl and LLM Cleaning\n",
        "\n",
        "This is the most complex tool in our data ingestion pipeline. It scrapes web URLs and cleans the content using both external services and LLM processing. Here's its implementation:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/scrape_and_clean_other_urls_tool.py_\n",
        "\n",
        "```python\n",
        "async def scrape_and_clean_other_urls_tool(research_directory: str, concurrency_limit: int = 4) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Scrape and clean other URLs from guidelines file in the research folder.\n",
        "    \n",
        "    Reads the guidelines file and scrapes/cleans each URL listed\n",
        "    under 'other_urls'. The cleaned markdown content is saved to the\n",
        "    URLS_FROM_GUIDELINES_FOLDER subfolder with appropriate filenames.\n",
        "    \"\"\"    \n",
        "    # Convert to Path object\n",
        "    research_path = Path(research_directory)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "    \n",
        "    # Look for GUIDELINES_FILENAMES_FILE file\n",
        "    guidelines_file_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "    \n",
        "    # Read the guidelines filenames file\n",
        "    guidelines_data = json.loads(read_file_safe(guidelines_file_path))\n",
        "    urls_to_scrape = guidelines_data.get(\"other_urls\", [])\n",
        "    \n",
        "    if not urls_to_scrape:\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"urls_processed\": [],\n",
        "            \"urls_failed\": [],\n",
        "            \"total_urls\": 0,\n",
        "            \"successful_urls_count\": 0,\n",
        "            \"failed_urls_count\": 0,\n",
        "            \"output_directory\": str(nova_path / URLS_FROM_GUIDELINES_FOLDER),\n",
        "            \"message\": \"No other URLs found to scrape in the guidelines filenames file.\"\n",
        "        }\n",
        "    \n",
        "    # Read article guidelines for context\n",
        "    guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
        "    guidelines_content = read_file_safe(guidelines_path)\n",
        "    \n",
        "    # Scrape URLs concurrently\n",
        "    completed_results = await scrape_urls_concurrently(\n",
        "        urls_to_scrape, \n",
        "        concurrency_limit, \n",
        "        guidelines_content\n",
        "    )\n",
        "    \n",
        "    # Write results to files\n",
        "    output_dir = nova_path / URLS_FROM_GUIDELINES_FOLDER\n",
        "    saved_files, successful_scrapes = write_scraped_results_to_files(completed_results, output_dir)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    failed_urls = [res[\"url\"] for res in completed_results if not res.get(\"success\", False)]\n",
        "    successful_urls = [res[\"url\"] for res in completed_results if res.get(\"success\", False)]\n",
        "    \n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"urls_processed\": successful_urls,\n",
        "        \"urls_failed\": failed_urls,\n",
        "        \"total_urls\": len(urls_to_scrape),\n",
        "        \"successful_urls_count\": successful_scrapes,\n",
        "        \"failed_urls_count\": len(failed_urls),\n",
        "        \"output_directory\": str(output_dir),\n",
        "        \"message\": f\"Successfully processed {successful_scrapes}/{len(urls_to_scrape)} URLs. \"\n",
        "                  f\"Results saved to: {output_dir}\"\n",
        "    }\n",
        "```\n",
        "\n",
        "Here's how it works:\n",
        "1. It looks for the URLs to scrape in the guidelines filenames file.\n",
        "2. It uses the `scrape_urls_concurrently` function to scrape the URLs concurrently using Firecrawl and clean the content using an LLM.\n",
        "3. It saves the cleaned content to the `URLS_FROM_GUIDELINES_FOLDER` folder.\n",
        "4. It returns a summary of the results.\n",
        "\n",
        "Let's see in more detail how the `scrape_urls_concurrently` function works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 The Two-Stage Cleaning Process\n",
        "\n",
        "The scraping process uses a two-stage approach:\n",
        "\n",
        "1. **Firecrawl for Initial Scraping**: Firecrawl is a specialized service that handles the complexity of modern web scraping, including:\n",
        "   - JavaScript rendering\n",
        "   - Dynamic content loading  \n",
        "   - Anti-bot protection\n",
        "   - Clean markdown extraction\n",
        "\n",
        "2. **LLM for Content Refinement**: After Firecrawl extracts the raw content, an LLM (Gemini 2.5 Flash) further cleans and structures the content by:\n",
        "   - Removing irrelevant sections (ads, navigation, footers)\n",
        "   - Focusing on content relevant to the article guidelines\n",
        "   - Maintaining proper markdown formatting\n",
        "   - Preserving important links and references"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Firecrawl scraping function handles the complexity of modern web scraping:\n",
        "\n",
        "```python\n",
        "async def scrape_url(url: str, firecrawl_app: AsyncFirecrawl) -> dict:\n",
        "    \"\"\"\n",
        "    Scrape a URL using Firecrawl with retries and return a dict with url, title, markdown.\n",
        "\n",
        "    Uses maxAge=1 week for 500% faster scraping by leveraging cached data when available.\n",
        "    This optimization significantly improves performance for documentation, articles, and\n",
        "    relatively static content while maintaining freshness within acceptable limits.\n",
        "    \"\"\"\n",
        "    max_retries = 3\n",
        "    base_delay = 5  # seconds\n",
        "    timeout_seconds = 120000  # 2 minutes timeout per request\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Add timeout to individual Firecrawl request\n",
        "            # Use maxAge=1 week for 500% faster scraping with cached data\n",
        "            res = await firecrawl_app.scrape(\n",
        "                url, formats=[\"markdown\"], maxAge=MAX_AGE_ONE_WEEK, timeout=timeout_seconds\n",
        "            )\n",
        "            title = res.metadata.title if res and res.metadata and res.metadata.title else \"N/A\"\n",
        "            markdown_content = res.markdown if res and res.markdown else \"\"\n",
        "            return {\"url\": url, \"title\": title, \"markdown\": markdown_content, \"success\": True}\n",
        "        except asyncio.TimeoutError:\n",
        "            # Manage retries\n",
        "            ...\n",
        "        except Exception as e:\n",
        "            # Manage retries\n",
        "            ...\n",
        "    \n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": \"Scraping Failed\",\n",
        "        \"markdown\": f\"⚠️ Error scraping {url} after {max_retries} attempts.\",\n",
        "        \"success\": False,\n",
        "    }\n",
        "```\n",
        "\n",
        "The core of it is the `firecrawl_app.scrape` function that scrapes the URL and returns the markdown content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LLM cleaning process is handled by the `clean_markdown` function:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/app/scraping_handler.py_\n",
        "\n",
        "```python\n",
        "async def clean_markdown(\n",
        "    markdown_content: str, article_guidelines: str, url_for_log: str, chat_model: BaseChatModel\n",
        ") -> str:\n",
        "    \"\"\"Clean markdown content via LLM and convert image syntax to URLs.\"\"\"\n",
        "    if not markdown_content.strip():\n",
        "        return markdown_content\n",
        "\n",
        "    prompt_text = PROMPT_CLEAN_MARKDOWN.format(article_guidelines=article_guidelines, markdown_content=markdown_content)\n",
        "    timeout_seconds = 180  # 3 minutes timeout for LLM call\n",
        "\n",
        "    try:\n",
        "        # Add timeout to LLM API call\n",
        "        response = await asyncio.wait_for(chat_model.ainvoke(prompt_text), timeout=timeout_seconds)\n",
        "        cleaned_content = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "        if isinstance(cleaned_content, list):\n",
        "            cleaned_content = \"\".join(str(part) for part in cleaned_content)\n",
        "\n",
        "        # Post-process: convert markdown images to just URLs\n",
        "        cleaned_content = convert_markdown_images_to_urls(cleaned_content)\n",
        "\n",
        "        return cleaned_content\n",
        "    except asyncio.TimeoutError:\n",
        "        logger.error(f\"LLM API call timed out after {timeout_seconds}s for {url_for_log}. Using original content.\")\n",
        "        return markdown_content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error cleaning markdown for {url_for_log}: {e}. Using original content.\", exc_info=True)\n",
        "        return markdown_content\n",
        "```\n",
        "\n",
        "The `clean_markdown` function simply uses Gemini 2.5 Flash to clean the markdown content using an appropriate prompt.\n",
        "Here's the prompt used (`PROMPT_CLEAN_MARKDOWN`) to clean the markdown content:\n",
        "\n",
        "```markdown\n",
        "Your task is to clean markdown content scraped from a webpage by *only removing* all irrelevant sections such as\n",
        "headers, footers, navigation bars, advertisements, sidebars, self-promotion, call-to-actions, etc.\n",
        "Focus on keeping only the core textual content (and code content if there are code sections) that is pertinent to\n",
        "the article guidelines provided below.\n",
        "Return *only* the cleaned markdown.\n",
        "Do not summarize or rewrite the original content. This task is only about *removing* irrelevant content.\n",
        "Good content should be kept as is, do not touch it.\n",
        "\n",
        "Here are the article guidelines:\n",
        "<article_guidelines>\n",
        "{article_guidelines}\n",
        "</article_guidelines>\n",
        "\n",
        "Here is the markdown content to clean:\n",
        "<markdown_content>\n",
        "{markdown_content}\n",
        "</markdown_content>\n",
        "```\n",
        "\n",
        "The cleaning process significantly reduces token count while preserving the most relevant information for research purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Why Use External Scraping Services?\n",
        "\n",
        "Web scraping is notoriously complex due to:\n",
        "\n",
        "- **Dynamic Content**: Modern websites heavily use JavaScript\n",
        "- **Anti-Bot Measures**: CAPTCHAs, rate limiting, IP blocking\n",
        "- **Varied Formats**: Inconsistent HTML structures across sites\n",
        "- **Performance Issues**: Slow loading, timeouts, redirects\n",
        "\n",
        "Rather than building a robust scraper from scratch (which would require significant effort and still fall short), using a specialized service like Firecrawl allows us to:\n",
        "\n",
        "- Focus on our core research logic\n",
        "- Get reliable results across diverse websites  \n",
        "- Benefit from ongoing improvements to the scraping infrastructure\n",
        "- Handle edge cases that would be time-consuming to solve ourselves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now test the scraping tool to see what is its output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'success', 'urls_processed': 2, 'urls_total': 2, 'files_saved': 2, 'output_directory': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/urls_from_guidelines', 'saved_files': ['function-calling-with-the-gemini-api-google-ai-for-developer.md', 'openai-platform.md'], 'message': \"Scraped and cleaned 2/2 other URLs from guidelines_filenames.json in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'.\\nSaved 2 files to urls_from_guidelines folder: function-calling-with-the-gemini-api-google-ai-for-developer.md, openai-platform.md\"}\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import scrape_and_clean_other_urls_tool\n",
        "\n",
        "# Test the scraping tool\n",
        "result = await scrape_and_clean_other_urls_tool(research_directory=research_folder, concurrency_limit=2)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output lists the number of URLs processed, the number of URLs that failed, and the output directory where the cleaned content is saved. You can now open the output directory (in the `.nova/urls_from_guidelines` folder) to see the cleaned content.\n",
        "\n",
        "Now, let's see how GitHub URLs are processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Processing GitHub URLs\n",
        "\n",
        "For GitHub repositories, we use a different approach optimized for code analysis. The `process_github_urls_tool` function (from `mcp_server/src/tools/process_github_urls_tool.py`) leverages the `gitingest` library to extract comprehensive information from GitHub repositories, making code and documentation available for research purposes.\n",
        "\n",
        "We won't show the code here as it's not interesting for teaching how AI agents work. You can check how it works in the code if you're curious. You only need to know that it uses the `gitingest` library to extract the information from the GitHub repositories.\n",
        "\n",
        "Let's test the GitHub processing tool here. The GitHub URL from the sample article guideline refer to a prompting guide for GPT-5 and is available in a [public repository](https://github.com/openai/openai-cookbook/blob/main/examples/gpt-5/gpt-5_prompting_guide.ipynb), so you don't need to provide a GitHub token for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'success', 'urls_processed': 1, 'urls_total': 1, 'files_saved': 1, 'output_directory': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/urls_from_guidelines_code', 'message': \"Processed 1/1 GitHub URLs from guidelines_filenames.json in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Saved markdown summaries to urls_from_guidelines_code folder.\"}\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import process_github_urls_tool\n",
        "\n",
        "# Test GitHub URL processing\n",
        "result = await process_github_urls_tool(research_directory=research_folder)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From its result, you can see that the tool has extracted the information from the GitHub repository and saved it in the `.nova/github_urls_from_guidelines_code` folder. You can now open the output directory to see the extracted information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. YouTube Video Transcription\n",
        "\n",
        "The `transcribe_youtube_videos_tool` (from the `mcp_server/src/tools/transcribe_youtube_videos_tool.py` file) leverages Gemini's multimodal capabilities to process video content directly and generate structured transcripts for research purposes.\n",
        "\n",
        "The core of it is the `transcribe_youtube` function, which is the one that actually transcribes the YouTube video. Here's its implementation:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/app/youtube_handler.py_\n",
        "\n",
        "```python\n",
        "async def transcribe_youtube(\n",
        "    url: str,\n",
        "    output_path: Path,\n",
        "    timestamp: int = 30,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Transcribes a public YouTube video using a Gemini model and saves the\n",
        "    result to a file.\n",
        "\n",
        "    Args:\n",
        "        url: The public URL of the YouTube video.\n",
        "        output_path: The path to save the transcription markdown file.\n",
        "        timestamp: The interval in seconds for inserting timestamps in the\n",
        "                   transcription.\n",
        "    \"\"\"\n",
        "    # Create client internally using settings and track with Opik if configured\n",
        "    base_client = genai.Client(api_key=settings.google_api_key.get_secret_value())\n",
        "    client = track_genai_client(base_client)\n",
        "    model_name = settings.youtube_transcription_model\n",
        "\n",
        "    prompt = PROMPT_YOUTUBE_TRANSCRIPTION.format(timestamp=timestamp)\n",
        "\n",
        "    parts: list[types.Part] = [\n",
        "        types.Part(\n",
        "            file_data=types.FileData(file_uri=url)  # YouTube URL - no download needed\n",
        "        ),\n",
        "        types.Part(text=prompt),\n",
        "    ]\n",
        "\n",
        "    ...\n",
        "    response: types.GenerateContentResponse = await client.aio.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=types.Content(parts=parts),\n",
        "    )\n",
        "    ...\n",
        "\n",
        "    output_path.write_text(response.text, encoding=\"utf-8\")\n",
        "```\n",
        "\n",
        "The Gemini API can [transcribe YouTube videos](https://ai.google.dev/gemini-api/docs/video-understanding#transcribe-video) by adding the video URL as a `types.Part` to the request, as shown above. Visit the provided link to learn more about the Gemini API's video understanding capabilities.\n",
        "\n",
        "You can run the following code to test the YouTube transcription tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'success', 'videos_processed': 1, 'videos_total': 1, 'output_directory': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/urls_from_guidelines_youtube_videos', 'message': \"Processed 1 YouTube URLs from guidelines_filenames.json in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Saved transcriptions to urls_from_guidelines_youtube_videos folder.\"}\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import transcribe_youtube_videos_tool\n",
        "\n",
        "# Test YouTube transcription (note: this can be time-consuming)\n",
        "result = await transcribe_youtube_videos_tool(research_directory=research_folder)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output lists the number of videos processed, the number of videos that failed, and the output directory where the transcription is saved. You can now open the output directory to see the transcription.\n",
        "\n",
        "**Note**: Video transcription is time-intensive. A 39-minute video typically takes about 4.5 minutes to process. The tool processes videos concurrently but with controlled concurrency to respect API limits and avoid overwhelming the service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Running the Full Agent with MCP Prompt\n",
        "\n",
        "We're now ready to run the MCP client and see how these tools work together! Run the following code cell to start the client.\n",
        "\n",
        "Once the client is running, you can:\n",
        "\n",
        "1. **Start the workflow**: Type `/prompt/full_research_instructions_prompt` to load the complete research workflow. It will load the MCP prompt with all the instructions and feed it to the LLM, which will in turn write a message to the user asking for the research directory path and whether the workflow should be run with modifications.\n",
        "2. **Answer the agent**: Give the path to your sample research folder, and tell the agent to run only the first two steps of the workflow, and to stop after that.\n",
        "3. **Watch the agent work**: Observe how it runs the tools in sequence\n",
        "4. **Examine outputs**: Check the `.nova` folder for generated files\n",
        "\n",
        "Try these commands in sequence:\n",
        "- `/prompt/full_research_instructions_prompt`\n",
        "- `The research folder is /path/to/research_folder. Run only the first two steps of the workflow and stop after that, and ask me how to proceed.` Replace the `/path/to/research_folder` with the path to your sample research folder.\n",
        "- `/quit` after the agent has finished running the tools and asked you how to proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🛠️  Available tools: 11\n",
            "📚 Available resources: 2\n",
            "💬 Available prompts: 1\n",
            "\n",
            "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m🤔 LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Understanding the Research Task**\n",
            "\n",
            "Okay, so the user wants me to run a research workflow. My first step is clear: I need to concisely explain the entire process to them, breaking it down into manageable chunks. Numbered steps will be the most straightforward approach here. Once I've laid out the plan, I'll need to gather some crucial information: the location where the research will be saved (`research_directory`) and whether they want to customize the workflow at all. This is important – I want to make sure I'm aligned with their expectations before diving in. Then, and only then, can I actually begin running the research workflow.\u001b[0m\n",
            "\n",
            "\u001b[37m💬 LLM Response: \u001b[0mHello! I will help you with the research workflow. Here are the steps:\n",
            "\n",
            "1.  **Setup:** Extract URLs and local file references from your article guidelines.\n",
            "2.  **Process Extracted Resources:** In parallel, I will process local files, scrape other web URLs, process GitHub repository links, and transcribe YouTube video URLs found in your guidelines.\n",
            "3.  **Research Loop:** I will run a research loop for 3 rounds. In each round, I'll generate new search queries based on existing data and execute them using Perplexity to gather more information.\n",
            "4.  **Filter Perplexity Results:** I will evaluate the Perplexity research results and select only the high-quality, relevant sources.\n",
            "5.  **Scrape Selected Research Sources:** I will identify the most valuable sources from the filtered Perplexity results and scrape their full content.\n",
            "6.  **Write Final Research File:** Finally, I will compile all the gathered and processed information into a comprehensive `research.md` file.\n",
            "\n",
            "To begin, please provide the path to your research directory. Also, let me know if you need any modifications to this workflow, such as starting from a specific step or adding user feedback points.\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m🤔 LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Executing the Next Phase of the Workflow**\n",
            "\n",
            "Okay, so I'm ready to move forward. My focus right now is definitely on steps 1 and 2 of this workflow. I remember step 1.3 is all about pulling those guidelines URLs, which is straightforward enough. But step 2, that's where the real action is. It's calling for some serious parallel processing! I've got four distinct sub-steps within step 2 that need to run concurrently: `process_local_files`, `scrape_and_clean_other_urls`, `process_github_urls`, and `transcribe_youtube_urls`.  \n",
            "\n",
            "All of these tools, the core tools for these steps and sub-steps, need to be executed while taking into account the `research_directory`.  I'll manage the inputs and outputs, and make sure everything is coordinated.  Time to get these functions moving!\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36m🔧 Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mextract_guidelines_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m⚡ Executing tool 'extract_guidelines_urls' via MCP server...\u001b[0m\n",
            "\u001b[36m✅ Tool execution successful!\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95m🤔 LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Pausing for Direction After Initial Setup**\n",
            "\n",
            "Okay, so I've finished the preliminary setup steps, specifically step 1.3, which is part of the \"Setup\" phase. Now, I need to move onto the second major part of this workflow: processing the extracted resources in parallel. This involves running steps 2.1 through 2.4.\n",
            "\n",
            "Based on the output from `extract_guidelines_urls`, I have some work to do. It looks like:\n",
            "\n",
            "*   I have 1 Github source, so I need to execute `process_github_urls`.\n",
            "*   There's 1 YouTube source, which means I have to run `transcribe_youtube_urls`.\n",
            "*   And there are 2 web sources, calling for `scrape_and_clean_other_urls`.\n",
            "*   Thankfully, there are no local files, so I can skip `process_local_files`.\n",
            "\n",
            "My plan is to launch `process_github_urls`, `transcribe_youtube_urls`, and `scrape_and_clean_other_urls` concurrently to speed things up.  I'm ready to proceed with these parallel processes, but before I commit further, I'd like to check in and see if you have any preferences or further instructions at this stage.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36m🔧 Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mprocess_github_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m⚡ Executing tool 'process_github_urls' via MCP server...\u001b[0m\n",
            "\u001b[36m✅ Tool execution successful!\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95m🤔 LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Workflow Execution Plan**\n",
            "\n",
            "Okay, so I've been given a research directory and instructed to run only the first two steps of this workflow. Step 1.3, `extract_guidelines_urls`, is already done, which is good. Now, I need to execute Step 2, and the instructions are to run its substeps in parallel where possible.\n",
            "\n",
            "Looking at the substeps, I have:\n",
            "*   `process_local_files`\n",
            "*   `scrape_and_clean_other_urls`\n",
            "*   `process_github_urls`\n",
            "*   `transcribe_youtube_urls`\n",
            "\n",
            "Based on the instructions, I need to check the output of the previous step, `extract_guidelines_urls`, to determine what needs to be run.\n",
            "\n",
            "From the output, it looks like:\n",
            "\n",
            "*   `github_sources_count`: 1 (already processed by `process_github_urls`)\n",
            "*   `youtube_sources_count`: 1\n",
            "*   `web_sources_count`: 2\n",
            "*   `local_files_count`: 0\n",
            "\n",
            "Great, no local files to worry about. Since `local_files_count` is 0, I can skip `process_local_files`. The output tells me that `process_github_urls` has already been run successfully.\n",
            "\n",
            "So, this means I need to execute `scrape_and_clean_other_urls` for those two web sources and `transcribe_youtube_urls` for that single YouTube source. After these two are done, I should stop and seek further instructions from the user, as requested.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36m🔧 Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mscrape_and_clean_other_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m⚡ Executing tool 'scrape_and_clean_other_urls' via MCP server...\u001b[0m\n",
            "\u001b[36m✅ Tool execution successful!\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95m🤔 LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Ready to Execute Step 2.4 - Transcribe YouTube URLs**\n",
            "\n",
            "Okay, so the user wants me to stop after the first two steps. Step 1.3, `extract_guidelines_urls`, is already done. Now I'm on to Step 2, and specifically, I need to look at what sub-tasks I actually have to execute. I've already confirmed that `process_github_urls` and `scrape_and_clean_other_urls` have been successfully run.\n",
            "\n",
            "Given the architecture, this leaves `process_local_files` and `transcribe_youtube_urls` as potential work. I'll check the output of `extract_guidelines_urls`, as that's where the relevant data will be. The user's research directory contains the results.\n",
            "\n",
            "Alright, the output confirms `\"local_files_count\":0`, so I can safely skip `process_local_files`. However, the output also tells me `\"youtube_sources_count\":1`.  This means I *do* need to run `transcribe_youtube_urls` (step 2.4).  Great, I'm ready to proceed with that!\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36m🔧 Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mtranscribe_youtube_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m⚡ Executing tool 'transcribe_youtube_urls' via MCP server...\u001b[0m\n",
            "\u001b[36m✅ Tool execution successful!\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95m🤔 LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Analyzing the Workflow and Preparing to Halt**\n",
            "\n",
            "Okay, so I've been given a research directory and asked to execute only the first two steps of a workflow. I've already handled step 1, which was extracting guideline URLs. Now, I'm looking at step 2, which typically involves running `process_local_files`, `scrape_and_clean_other_urls`, `process_github_urls`, and `transcribe_youtube_urls` in parallel.\n",
            "\n",
            "Looking at the tool outputs, though, things are interesting. It seems like `extract_guidelines_urls`, `process_github_urls`, `scrape_and_clean_other_urls`, and `transcribe_youtube_urls` have all been successfully executed at some point. The only one that hasn't been explicitly called, based on what I'm seeing, is `process_local_files`.\n",
            "\n",
            "But wait! The output of `extract_guidelines_urls` tells me that there are *zero* local files to process! It says `local_files_count: 0`. Given the instruction to skip unnecessary steps, and knowing that `process_local_files` won't actually do anything, it makes complete sense to skip it. This avoids wasted processing time.\n",
            "\n",
            "Since I've determined that either parts of step 2 have already been executed, or are unnecessary (given the zero local files), I've effectively completed step 2. Now, I need to stop here and get the user's input on how to proceed. I'm ready for the next instruction.\u001b[0m\n",
            "\n",
            "\u001b[37m💬 LLM Response: \u001b[0mI have completed the first two steps of the workflow for the research directory `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder`.\n",
            "\n",
            "Step 1, \"Setup,\" involved extracting URLs and local file references from your article guidelines. This was completed successfully, identifying 1 GitHub URL, 1 YouTube URL, 2 other web URLs, and 0 local file references.\n",
            "\n",
            "For Step 2, \"Process Extracted Resources,\" I have:\n",
            "*   Processed the GitHub URLs.\n",
            "*   Scraped and cleaned the other web URLs.\n",
            "*   Transcribed the YouTube video URLs.\n",
            "*   Since no local files were found in the guidelines (`local_files_count: 0`), there was no need to run the `process_local_files` tool.\n",
            "\n",
            "As requested, I will now stop. How would you like me to proceed?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "import sys\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "# Start client with in-memory server \n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, read the above output and notice the following:\n",
        "- Since there isn't any local file to extract, the agent skipped the `process_local_files_tool` tool.\n",
        "- Read the agent thoughts to understand the reasoning behind the choices it made. They usually refer to the previous tool outputs.\n",
        "- Read the final message from the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Exploring Generated Files\n",
        "\n",
        "After running the tools, examine the organized file structure in your research directory:\n",
        "\n",
        "```\n",
        "research_directory/\n",
        "├── article_guideline.md                     # Input guidelines\n",
        "├── .nova/                                   # Hidden folder with all data\n",
        "│   ├── guidelines_filenames.json           # Extracted URLs and files\n",
        "│   ├── local_files_from_research/          # Copied local files  \n",
        "│   ├── urls_from_guidelines/               # Scraped web content\n",
        "│   ├── urls_from_guidelines_code/          # GitHub repo summaries\n",
        "│   └── urls_from_guidelines_youtube/       # Video transcripts\n",
        "```\n",
        "\n",
        "Each folder contains processed content ready for the next stages of the research workflow. The file-based approach ensures that:\n",
        "\n",
        "- **Content is persistent** across agent sessions\n",
        "- **Large content blocks** don't overwhelm the agent's context\n",
        "- **Selective access** allows the agent to read only relevant files\n",
        "- **Human inspection** is possible for debugging and verification\n",
        "\n",
        "In a production setting, these files can be replaced with a database to enable more efficient querying and retrieval."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
