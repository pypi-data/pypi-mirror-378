{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 15: FastMCP ‚Äî MCP Server and Client Quickstart\n",
        "\n",
        "In this lesson, you will run a Model Context Protocol (MCP) server and MCP client using the FastMCP library, then explore how our research agent exposes MCP tools, MCP resources, and MCP prompts. We‚Äôll start with a quick demo that runs the MCP client with an in-memory MCP server directly from this notebook, so you can get to try its capabilities immediately. Then, we‚Äôll examine the MCP server and MCP client code structure.\n",
        "\n",
        "Learning Objectives:\n",
        "- Learn how to create an MCP server using `fastmcp`\n",
        "- Learn how to create an MCP client using `fastmcp`\n",
        "- Learn how to use the `fastmcp` library to expose MCP tools, MCP resources, and MCP prompts\n",
        "- Learn how to use the `fastmcp` library to interact with an MCP server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To configure the Gemini API, follow the step-by-step instructions from the `Course Admin` lesson.\n",
        "\n",
        "But here is a quick check on what you need to run this Notebook:\n",
        "\n",
        "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  From the root of your project, run: `cp .env.example .env` \n",
        "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
        "\n",
        "Now, the code below will load the key from the `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables loaded from `/Users/fabio/Desktop/course-ai-agents/.env`\n",
            "Environment variables loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply() # Allow nested async usage in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Try the agent (MCP client quickstart)\n",
        "\n",
        "The research agent is made of an MCP server and an MCP client.\n",
        "\n",
        "The MCP server is a `fastmcp` server that registers MCP tools, MCP resources, and MCP prompt via router modules. The MCP client is a `fastmcp` client that connects to the MCP server and allows you to interact with it, along with interacting with the LLM agent.\n",
        "\n",
        "This quickstart runs the MCP client of the research agent inside the notebook kernel. It connects to the MCP server running in‚Äëmemory (same process), which is the only transport supported for running everything in the same notebook. So, we'll always run the MCP server in-memory in the notebooks.\n",
        "\n",
        "Run the next code cell to start the MCP client. You will see some texts and can type commands directly in the input box that appears. The input box will be in different locations depending on where you are running the notebook from.\n",
        "\n",
        "Once the client is running, you can type commands when prompted, such as:\n",
        "\n",
        "- `/tools`: list all available MCP tools with names and descriptions.\n",
        "- `/resources`: list all available MCP resources with their URIs.\n",
        "- `/prompts`: list all available MCP prompts by name and description.\n",
        "- `/prompt/full_research_instructions_prompt`: fetch the research workflow prompt and inject it into the conversation.\n",
        "- `/resource/system://memory`: read and print the server memory stats (an example of running an MCP resource).\n",
        "- `/model-thinking-switch`: toggle model ‚Äúthinking‚Äù traces on/off. By default it is true, which means that you'll see the agent's thoughts in the conversation before each answer or tool call.\n",
        "- Any other text: treated as a normal user message for the agent, which may use the MCP server tools for answering.\n",
        "- `/quit`: terminate the client.\n",
        "\n",
        "At first, try with the following commands and see what happens:\n",
        "- `Hello! Who are you?`\n",
        "- `/tools`\n",
        "- `/resource/system://memory`\n",
        "- `/quit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è  Available tools: 11\n",
            "üìö Available resources: 2\n",
            "üí¨ Available prompts: 1\n",
            "\n",
            "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
            "\n",
            "\u001b[97m============================================================\u001b[0m\n",
            "\u001b[1m\u001b[96müí¨ Available Prompts\u001b[0m\n",
            "\u001b[97m============================================================\u001b[0m\n",
            "\n",
            "\u001b[92m1. \u001b[0m\u001b[97mfull_research_instructions_prompt\u001b[0m\u001b[33m\n",
            "   Complete Nova research agent workflow instructions.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**My Understanding of the Research Agent Workflow**\n",
            "\n",
            "Okay, so I'm being asked to kick off this research agent workflow. My first step is to be completely transparent with the user, so I need to lay out each stage of the process.  Let me break it down clearly:\n",
            "\n",
            "First, we're going to dive into the `ARTICLE_GUIDELINE_FILE` to extract any URLs and local file references using the `extract_guidelines_urls` tool.  This seems straightforward enough.\n",
            "\n",
            "Next, we'll hit things in parallel to speed things up. I'll handle the local files mentioned in the guidelines with `process_local_files`, and then I'll use separate tools for scraping and cleaning other URLs (`scrape_and_clean_other_urls`), dealing with GitHub links (`process_github_urls`), and transcribing YouTube videos (`transcribe_youtube_urls`).\n",
            "\n",
            "Now, the core of the research:  we'll go through a loop, three times.  Each round involves `generate_next_queries` to refine our search terms, `run_perplexity_research` to get search results, then `select_research_sources_to_keep` to pull out the most promising sources.\n",
            "\n",
            "After filtering Perplexity results, we're going to zoom in on the most crucial resources. We'll decide which URLs to scrape in full with `select_research_sources_to_scrape` and use `scrape_research_urls` to do the actual scraping.\n",
            "\n",
            "Finally, the grand finale:  I'll pull it all together, combining all the research data into a comprehensive `research.md` file using `create_research_file`.\n",
            "\n",
            "Importantly, I need to keep a close eye on any \"Critical Failure\" scenarios. If any tool processes *zero* items successfully, that's a red flag. I'll stop immediately, tell the user exactly what failed and the error message, and then wait for instructions.\n",
            "\n",
            "Before I get started executing this, I need to make sure I have the necessary information: namely, the `research_directory` where I should be working.  Do you have that ready for me? And, are there any modifications or optimizations you want me to incorporate into this plan?\u001b[0m\n",
            "\n",
            "\u001b[37müí¨ LLM Response: \u001b[0mHello! I will help you with the research workflow. Here are the steps:\n",
            "\n",
            "1.  **Setup:** Extract all URLs (GitHub, YouTube, other web links) and local file references from your `ARTICLE_GUIDELINE_FILE`.\n",
            "2.  **Process Resources:** In parallel, I will:\n",
            "    *   Copy all referenced local files to a dedicated folder.\n",
            "    *   Scrape and clean content from general web URLs.\n",
            "    *   Process GitHub URLs to summarize repositories and code.\n",
            "    *   Transcribe YouTube video URLs and save their transcripts.\n",
            "3.  **Research Loop:** For three rounds, I will:\n",
            "    *   Generate new web-search queries based on the guidelines and current research.\n",
            "    *   Execute these queries using Perplexity and save the results.\n",
            "4.  **Filter Results:** I will evaluate the Perplexity results for quality, trustworthiness, and relevance, then select the best sources.\n",
            "5.  **Scrape Research Sources:** I will identify the most valuable accepted sources and scrape their full content.\n",
            "6.  **Final Research File:** Finally, I will compile all the gathered and processed information into a comprehensive `research.md` file.\n",
            "\n",
            "To begin, please provide the path to your research directory. Also, let me know if you need any modifications to this workflow (e.g., starting from a specific step, or adding user feedback to certain steps).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the MCP client in-kernel\n",
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "import sys\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "# Start client with in-memory server \n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Whenever you want, you can run the previous cell again to try the client.\n",
        "\n",
        "Now, let's see how the MCP server works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MCP Server Overview\n",
        "\n",
        "The purpose of this section is to show how the MCP server is created with `fastmcp` and how it wires MCP tools, MCP resources, and MCP prompts.\n",
        "\n",
        "The MCP server is a `fastmcp` server that registers MCP tools (actions with side effects like scraping webpages, transcribing videos, etc.), MCP resources (read-only endpoints for information like system status or memory), and MCP prompts (reusable instruction blocks, such as our agent workflow) via router modules.\n",
        "\n",
        "The MCP server follows a FastAPI‚Äëlike layout for clarity and scalability. It is structured as follows:\n",
        "\n",
        "- `server.py`: Entry point exposing `create_mcp_server()` and a `__main__` runner.\n",
        "- `routers/`: Functions that attach endpoints to the FastMCP instance.\n",
        "  - `tools.py`: registers all MCP tools.\n",
        "  - `resources.py`: registers all MCP resources.\n",
        "  - `prompts.py`: registers all MCP prompts.\n",
        "- `tools/`: MCP tools implementations.\n",
        "- `resources/`: MCP resources implementations.\n",
        "- `prompts/`: MCP prompts implementations (e.g. full workflow instructions for the agent).\n",
        "- `app/`: Functions implementing business logic.\n",
        "- `utils/`: Utility functions.\n",
        "- `config/`: Pydantic settings (`settings.py`) for server name/version, logging, model choices, and API keys.\n",
        "\n",
        "This separation keeps orchestration thin at the server boundary while allowing each capability (tool/resource/prompt) to evolve independently.\n",
        "\n",
        "Let's see now how the MCP server is created.\n",
        "\n",
        "Source:\n",
        "_mcp_server/src/server.py_\n",
        "\n",
        "```python\n",
        "from fastmcp import FastMCP\n",
        "\n",
        "from .config.settings import settings\n",
        "from .routers.prompts import register_mcp_prompts\n",
        "from .routers.resources import register_mcp_resources\n",
        "from .routers.tools import register_mcp_tools\n",
        "\n",
        "\n",
        "def create_mcp_server() -> FastMCP:\n",
        "    \"\"\"\n",
        "    Create and configure the MCP server instance.\n",
        "\n",
        "    This function can be imported to get a configured MCP server\n",
        "    for use with in-memory transport in clients.\n",
        "\n",
        "    Returns:\n",
        "        FastMCP: Configured MCP server instance\n",
        "    \"\"\"\n",
        "    # Create the FastMCP server instance\n",
        "    mcp = FastMCP(\n",
        "        name=settings.server_name,\n",
        "        version=settings.version,\n",
        "    )\n",
        "\n",
        "    # Register all MCP endpoints\n",
        "    register_mcp_tools(mcp)\n",
        "    register_mcp_resources(mcp)\n",
        "    register_mcp_prompts(mcp)\n",
        "\n",
        "    return mcp\n",
        "```\n",
        "\n",
        "Notice how the `FastMCP` instance is created and how the `mcp` object is passed to the `register_mcp_tools`, `register_mcp_resources`, and `register_mcp_prompts` functions. It is pretty similar to how you would create a FastAPI app and attach endpoints to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Registering MCP Tools\n",
        "\n",
        "Let's see now in particular how to register an MCP tool with `fastmcp`. This specific tool reads the article guidelines and extracts relevant references. Its implementation is in the `tools/extract_guidelines_urls_tool.py` file, along with other business logic functions in the `app/` folder. You can read the full file `mcp_server/src/routers/tools.py` to see all the 11 available MCP tools.\n",
        "\n",
        "Source: _mcp_server/src/routers/tools.py_\n",
        "\n",
        "```python\n",
        "@mcp.tool()\n",
        "async def extract_guidelines_urls(research_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract URLs and local file references from article guidelines.\n",
        "\n",
        "    Reads the ARTICLE_GUIDELINE_FILE file in the research directory and extracts:\n",
        "    - GitHub URLs\n",
        "    - Other HTTP/HTTPS URLs\n",
        "    - Local file references (files mentioned in quotes with extensions)\n",
        "\n",
        "    Results are saved to GUIDELINES_FILENAMES_FILE in the research directory.\n",
        "    \"\"\"\n",
        "    result = extract_guidelines_urls_tool(research_directory)\n",
        "    return result\n",
        "```\n",
        "\n",
        "This tool is the first step in the workflow. It reads the article guideline and writes a structured file containing URLs and local references. Notice how it requires a `research_directory` input, which is the path to the research directory containing a `article_guideline.md` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test it with a sample article guideline. In the research agent folder, there's a `data/sample_research_folder` folder with an `article_guideline.md` file. Let's use it as input for the `extract_guidelines_urls` tool.\n",
        "\n",
        "Here is how it is structured:\n",
        "\n",
        "```md\n",
        "## Global Context of the Lesson\n",
        "\n",
        "...\n",
        "\n",
        "## Lesson Outline\n",
        "\n",
        "## Section 1: Introduction\n",
        "\n",
        "...\n",
        "\n",
        "## Section 2: Understanding why agents need tools\n",
        "\n",
        "...\n",
        "\n",
        "## Section N: Conclusion\n",
        "\n",
        "...\n",
        "\n",
        "## Article code\n",
        "\n",
        "Links to code that will be used to support the article. Always prioritize this code over every other piece of code found in the sources: \n",
        "\n",
        "- [Notebook 1](https://github.com/path/to/notebook.ipynb)\n",
        "\n",
        "## Sources\n",
        "\n",
        "- [Function calling with the Gemini API](https://ai.google.dev/gemini-api/docs/function-calling)\n",
        "- [Function calling with OpenAI's API](https://platform.openai.com/docs/guides/function-calling)\n",
        "- [Tool Calling Agent From Scratch](https://www.youtube.com/watch?v=ApoDzZP8_ck)\n",
        "- [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/pdf/2401.17464v3)\n",
        "- [Building AI Agents from scratch - Part 1: Tool use](https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part)\n",
        "- [What is Tool Calling? Connecting LLMs to Your Data](https://www.youtube.com/watch?v=h8gMhXYAv1k)\n",
        "- [ReAct vs Plan-and-Execute: A Practical Comparison of LLM Agent Patterns](https://dev.to/jamesli/react-vs-plan-and-execute-a-practical-comparison-of-llm-agent-patterns-4gh9)\n",
        "- [Agentic Design Patterns Part 3, Tool Use](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/)\n",
        "```\n",
        "\n",
        "Normally, an `article_guideline.md` file would contain detailed information about the article to write, including the outline, the sections, the sources, and the code, as the research agent needs this information to look for the best content to include in the article. In this sample file, we have a simplified version of an article guideline.\n",
        "\n",
        "Now, run the next code cell to run the research agent MCP client again, and give it the following command. Make sure to replace the folder path with your actual absolute folder path, otherwise the tool will not find the file.\n",
        "- Command to give to the client: `Run the \"extract_guidelines_urls\" tool with the \"data/sample_research_folder\" directory as research folder and stop after the tool has finished running.`.\n",
        "\n",
        "In case you provide the wrong path, notice how the tool will return an error and how the agent will ask you to provide a valid path and how to proceed.\n",
        "\n",
        "*Important*: the agent will manage every message starting with the `/` as a command, so, if you want to provide the folder path in a message, you need to write something like this: `Here is the folder path: /absolute/path/to/the/folder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è  Available tools: 11\n",
            "üìö Available resources: 2\n",
            "üí¨ Available prompts: 1\n",
            "\n",
            "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Let's Get Those Guidelines**\n",
            "\n",
            "Okay, so I need to run that `extract_guidelines_urls` tool.  The data I'm working with is located in that directory ‚Äì it's `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder`.  Easy enough, I just call the tool with that path as the `research_directory` parameter and let it do its thing.  Time to get those guideline URLs extracted.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36müîß Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mextract_guidelines_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m‚ö° Executing tool 'extract_guidelines_urls' via MCP server...\u001b[0m\n",
            "\u001b[36m‚úÖ Tool execution successful!\u001b[0m\n",
            "\n",
            "\u001b[37müí¨ LLM Response: \u001b[0mThe `extract_guidelines_urls` tool has successfully extracted URLs from the article guidelines in the specified directory.\n",
            "Here's a summary of the findings:\n",
            "- **GitHub URLs:** 1\n",
            "- **YouTube video URLs:** 2\n",
            "- **Other web URLs:** 6\n",
            "- **Local file references:** 0\n",
            "\n",
            "The results have been saved to `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json`.\n",
            "\n",
            "What would you like to do next? You can choose to process the local files, scrape other URLs, process GitHub URLs, or transcribe YouTube URLs.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the MCP client in-kernel\n",
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "import sys\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "# Start client with in-memory server \n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the agent's thoughts. If everything ran correctly, you'll see the text \"Tool execution successful\". If so, notice that there is a new folder named `.nova` in the research directory, with a file `guidelines_filenames.json` inside. This file contains the URLs and local references extracted from the article guideline.\n",
        "\n",
        "Its content should be like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"github_urls\": [\n",
        "    \"https://github.com/path/to/notebook.ipynb\"\n",
        "  ],\n",
        "  \"youtube_videos_urls\": [\n",
        "    \"https://www.youtube.com/watch?v=ApoDzZP8_ck\",\n",
        "    \"https://www.youtube.com/watch?v=h8gMhXYAv1k\"\n",
        "  ],\n",
        "  \"other_urls\": [\n",
        "    \"https://ai.google.dev/gemini-api/docs/function-calling\",\n",
        "    \"https://platform.openai.com/docs/guides/function-calling\",\n",
        "    \"https://arxiv.org/pdf/2401.17464v3\",\n",
        "    \"https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part\",\n",
        "    \"https://dev.to/jamesli/react-vs-plan-and-execute-a-practical-comparison-of-llm-agent-patterns-4gh9\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/\"\n",
        "  ],\n",
        "  \"local_file_paths\": []\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, the tool has extracted those URLs from the `article_guideline.md` file and categorized them into the groups you see above.\n",
        "\n",
        "We can run the above tool also programmatically as follows. The output shows the result of running it from the local setup of the author of this notebook. To run it, update the path of the `research_folder` variable with your absolute path to the `sample_research_folder` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'success',\n",
              " 'github_sources_count': 1,\n",
              " 'youtube_sources_count': 2,\n",
              " 'web_sources_count': 6,\n",
              " 'local_files_count': 0,\n",
              " 'output_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json',\n",
              " 'message': \"Successfully extracted URLs from article guidelines in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Found 1 GitHub URLs, 2 YouTube videos URLs, 6 other URLs, and 0 local file references. Results saved to: /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json\"}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import extract_guidelines_urls_tool\n",
        "\n",
        "research_folder = \"/your/absolute/path/to/sample_research_folder\"\n",
        "extract_guidelines_urls_tool(research_folder=research_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll comment the output of this tool in the next lesson. In the next lessons, we'll run each tool one by one like in the above code cell, so you can see the output of each tool and understand how the research agent works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Registering MCP Resources\n",
        "\n",
        "Let's see now how to register an MCP resource endpoint using `fastmcp`.\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_server/src/routers/resources.py_\n",
        "\n",
        "```python\n",
        "@mcp.resource(\"system://memory\")\n",
        "async def memory_usage() -> Dict[str, Any]:\n",
        "    \"\"\"Monitor memory usage of the server.\"\"\"\n",
        "    return await get_memory_usage_resource()\n",
        "```\n",
        "\n",
        "It's very similar to how tools are registered, except that the `@mcp.resource()` decorator is used instead of the `@mcp.tool()` decorator.\n",
        "\n",
        "Let's now run the `get_memory_usage_resource` function to see the memory usage of the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'process_memory_mb': 297.359375,\n",
              " 'process_memory_percent': 1.8149375915527344,\n",
              " 'system_memory': {'total_gb': 16.0,\n",
              "  'available_gb': 6.070953369140625,\n",
              "  'used_percent': 62.1}}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.resources import get_memory_usage_resource\n",
        "\n",
        "await get_memory_usage_resource()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output is the same output that an MCP client would get if it uses this MCP resource.\n",
        "\n",
        "*Important*: in the research agent MCP client, we have only implemented the use of tools by the agent LLM, but we could have implemented the use of resources as well. Most MCP clients do not support resources yet, but their support is increasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Registering MCP Prompts\n",
        "\n",
        "This section shows how MCP prompts are implemented with `fastmcp`. This specific prompt defines the agentic workflow for the research agent.\n",
        "\n",
        "Source:\n",
        "_mcp_server/src/routers/prompts.py_\n",
        "\n",
        "```python\n",
        "@mcp.prompt()\n",
        "async def full_research_instructions_prompt() -> str:\n",
        "    \"\"\"Complete Nova research agent workflow instructions.\"\"\"\n",
        "    return await _get_research_instructions()\n",
        "```\n",
        "\n",
        "The prompt content encodes the full workflow orchestration the agent should follow when started via a prompt.\n",
        "\n",
        "In practice, MCP prompts are triggered by users from an MCP client, not by the agent LLM. When a user triggers an MCP prompt, the MCP client would retrieve that prompt and load it to instruct the LLM on how to run the available tools in sequence (and sometimes in parallel) according to the workflow described in it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For reference, here is the full prompt content of the only MCP prompt implemented in the research agent, which is the `full_research_instructions_prompt` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your job is to execute the workflow below.\n",
            "\n",
            "All the tools require a research directory as input.\n",
            "If the user doesn't provide a research directory, you should ask for it before executing any tool.\n",
            "\n",
            "**Workflow:**\n",
            "\n",
            "1. Setup:\n",
            "\n",
            "    1.1. Explain to the user the numbered steps of the workflow. Be concise. Keep them numbered so that the user\n",
            "    can easily refer to them later.\n",
            "    \n",
            "    1.2. Ask the user for the research directory, if not provided. Ask the user if any modification is needed for the\n",
            "    workflow (e.g. running from a specific step, or adding user feedback to specific steps).\n",
            "\n",
            "    1.3 Extract the URLs from the ARTICLE_GUIDELINE_FILE with the \"extract_guidelines_urls\" tool. This tool reads the\n",
            "    ARTICLE_GUIDELINE_FILE and extracts three groups of references from the guidelines:\n",
            "    ‚Ä¢ \"github_urls\" - all GitHub links;\n",
            "    ‚Ä¢ \"youtube_videos_urls\" - all YouTube video links;\n",
            "    ‚Ä¢ \"other_urls\" - all remaining HTTP/HTTPS links;\n",
            "    ‚Ä¢ \"local_files\" - relative paths to local files mentioned in the guidelines (e.g. \"code.py\", \"src/main.py\").\n",
            "    Only extensions allowed are: \".py\", \".ipynb\", and \".md\".\n",
            "    The extracted data is saved to the GUIDELINES_FILENAMES_FILE within the NOVA_FOLDER directory.\n",
            "\n",
            "2. Process the extracted resources in parallel:\n",
            "\n",
            "    You can run the following sub-steps (2.1 to 2.4) in parallel. In a single turn, you can call all the\n",
            "    necessary tools for these steps.\n",
            "\n",
            "    2.1 Local files - run the \"process_local_files\" tool to read every file path listed under \"local_files\" in the\n",
            "    GUIDELINES_FILENAMES_FILE and copy its content into the LOCAL_FILES_FROM_RESEARCH_FOLDER subfolder within\n",
            "    NOVA_FOLDER, giving each copy an appropriate filename (path separators are replaced with underscores).\n",
            "\n",
            "    2.2 Other URL links - run the \"scrape_and_clean_other_urls\" tool to read the `other_urls` list from the\n",
            "    GUIDELINES_FILENAMES_FILE and scrape/clean them. The tool writes the cleaned markdown files inside the\n",
            "    URLS_FROM_GUIDELINES_FOLDER subfolder within NOVA_FOLDER.\n",
            "\n",
            "    2.3 GitHub URLs - run the \"process_github_urls\" tool to process the `github_urls` list from the\n",
            "    GUIDELINES_FILENAMES_FILE with gitingest and save a Markdown summary for each URL inside the\n",
            "    URLS_FROM_GUIDELINES_CODE_FOLDER subfolder within NOVA_FOLDER.\n",
            "\n",
            "    2.4 YouTube URLs - run the \"transcribe_youtube_urls\" tool to process the `youtube_videos_urls` list from the\n",
            "    GUIDELINES_FILENAMES_FILE, transcribe each video, and save the transcript as a Markdown file inside the\n",
            "    URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder within NOVA_FOLDER.\n",
            "        Note: Please be aware that video transcription can be a time-consuming process. For reference,\n",
            "        transcribing a 39-minute video can take approximately 4.5 minutes.\n",
            "\n",
            "3. Repeat the following research loop for 3 rounds:\n",
            "\n",
            "    3.1. Run the \"generate_next_queries\" tool to analyze the ARTICLE_GUIDELINE_FILE, the already-scraped guideline\n",
            "    URLs, and the existing PERPLEXITY_RESULTS_FILE. The tool identifies knowledge gaps, proposes new web-search\n",
            "    questions, and writes them - together with a short justification for each - to the NEXT_QUERIES_FILE within\n",
            "    NOVA_FOLDER.\n",
            "\n",
            "    3.2. Run the \"run_perplexity_research\" tool with the new queries. This tool executes the queries with\n",
            "    Perplexity and appends the results to the PERPLEXITY_RESULTS_FILE within NOVA_FOLDER.\n",
            "\n",
            "4. Filter Perplexity results by quality:\n",
            "\n",
            "    4.1 Run the \"select_research_sources_to_keep\" tool. The tool reads the ARTICLE_GUIDELINE_FILE and the\n",
            "    PERPLEXITY_RESULTS_FILE, automatically evaluates each source for trustworthiness, authority and relevance,\n",
            "    writes the comma-separated IDs of the accepted sources to the PERPLEXITY_SOURCES_SELECTED_FILE **and** saves a\n",
            "    filtered markdown file PERPLEXITY_RESULTS_SELECTED_FILE that contains only the full content blocks of the accepted\n",
            "    sources. Both files are saved within NOVA_FOLDER.\n",
            "\n",
            "5. Identify which of the accepted sources deserve a *full* scrape:\n",
            "\n",
            "    5.1 Run the \"select_research_sources_to_scrape\" tool. It analyses the PERPLEXITY_RESULTS_SELECTED_FILE together\n",
            "    with the ARTICLE_GUIDELINE_FILE and the material already scraped from guideline URLs, then chooses up to 5 diverse,\n",
            "    authoritative sources whose full content will add most value. The chosen URLs are written (one per line) to the\n",
            "    URLS_TO_SCRAPE_FROM_RESEARCH_FILE within NOVA_FOLDER.\n",
            "\n",
            "    5.2 Run the \"scrape_research_urls\" tool. The tool reads the URLs from URLS_TO_SCRAPE_FROM_RESEARCH_FILE and\n",
            "    scrapes/cleans each URL's full content. The cleaned markdown files are saved to the\n",
            "    URLS_FROM_RESEARCH_FOLDER subfolder within NOVA_FOLDER with appropriate filenames.\n",
            "\n",
            "6. Write final research file:\n",
            "\n",
            "    6.1 Run the \"create_research_file\" tool. The tool combines all research data including filtered Perplexity results\n",
            "    from PERPLEXITY_RESULTS_SELECTED_FILE, scraped guideline sources from URLS_FROM_GUIDELINES_FOLDER,\n",
            "    URLS_FROM_GUIDELINES_CODE_FOLDER, and URLS_FROM_GUIDELINES_YOUTUBE_FOLDER, and full research sources from\n",
            "    URLS_FROM_RESEARCH_FOLDER into a comprehensive RESEARCH_MD_FILE organized into sections with collapsible blocks\n",
            "    for easy navigation. The final RESEARCH_MD_FILE is saved in the root of the research directory.\n",
            "\n",
            "Depending on the results of previous steps, you may want to skip running a tool if not necessary.\n",
            "\n",
            "**Critical Failure Policy:**\n",
            "\n",
            "If a tool reports a complete failure, you are required to halt the entire workflow immediately. A complete failure\n",
            "is defined as processing zero items successfully (e.g., scraped 0/7 URLs, processed 0 files).\n",
            "\n",
            "If this occurs, your immediate and only action is to:\n",
            "    1. State the exact tool that failed and quote the output message.\n",
            "    2. Announce that you are stopping the workflow as per your instructions.\n",
            "    3. Ask the user for guidance on how to proceed.\n",
            "\n",
            "**File and Folder Structure:**\n",
            "\n",
            "After running the complete workflow, the research directory will contain the following structure:\n",
            "\n",
            "```\n",
            "research_directory/\n",
            "‚îú‚îÄ‚îÄ ARTICLE_GUIDELINE_FILE                           # Input: Article guidelines and requirements\n",
            "‚îú‚îÄ‚îÄ NOVA_FOLDER/                                     # Hidden directory containing all research data\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ GUIDELINES_FILENAMES_FILE                    # Extracted URLs and local files from guidelines\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ LOCAL_FILES_FROM_RESEARCH_FOLDER/           # Copied local files referenced in guidelines\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [processed_local_files...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_FROM_GUIDELINES_FOLDER/               # Scraped content from other URLs in guidelines\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [scraped_web_pages...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_FROM_GUIDELINES_CODE_FOLDER/          # GitHub repository summaries and code analysis\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [github_repo_summaries...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_FROM_GUIDELINES_YOUTUBE_FOLDER/       # YouTube video transcripts\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [youtube_transcripts...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ NEXT_QUERIES_FILE                           # Generated web-search queries with justifications\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ PERPLEXITY_RESULTS_FILE                     # Complete results from all Perplexity research rounds\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ PERPLEXITY_SOURCES_SELECTED_FILE            # Comma-separated IDs of quality sources selected\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ PERPLEXITY_RESULTS_SELECTED_FILE            # Filtered Perplexity results (only selected sources)\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_TO_SCRAPE_FROM_RESEARCH_FILE          # URLs selected for full content scraping\n",
            "‚îÇ   ‚îî‚îÄ‚îÄ URLS_FROM_RESEARCH_FOLDER/                 # Fully scraped content from selected research URLs\n",
            "‚îÇ       ‚îî‚îÄ‚îÄ [full_research_sources...]\n",
            "‚îî‚îÄ‚îÄ RESEARCH_MD_FILE                                 # Final comprehensive research compilation\n",
            "```\n",
            "\n",
            "This organized structure ensures all research artifacts are systematically collected, processed, and made easily\n",
            "accessible for article writing and future reference.\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.prompts import full_research_instructions_prompt\n",
        "\n",
        "prompt = await full_research_instructions_prompt()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the instruction block that defines the agentic workflow for the research agent. In the next lessons, we'll go through each step defined in the workflow, learn how it is implemented, and run it in isolation.\n",
        "\n",
        "Let's now see how the MCP client works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. MCP Client Overview\n",
        "\n",
        "Here is the MCP client's layout. It is structured as follows:\n",
        "\n",
        "- `client.py`: CLI entry point. Parses `--transport`, creates the client (in‚Äëmemory or stdio), fetches capabilities, prints the startup banner, and runs the interactive loop.\n",
        "- `settings.py`: Centralized Pydantic settings for API keys, model selection, logging, transport, and server paths.\n",
        "- `utils/`: Helper modules used by `client.py`.\n",
        "\n",
        "The MCP client can run with two transports:\n",
        "\n",
        "- **in-memory**: The client imports the server factory (the `create_mcp_server` function from the `client.py` file) and instantiates the server inside the same Python process. This is fast, simple to debug, and is what we use in this notebook.\n",
        "- **stdio**: The client launches the server as a separate process and communicates using the MCP stdio transport. This mirrors how external MCP clients (e.g., editors) connect to servers and provides process isolation.\n",
        "\n",
        "Let's see how the code of the `client.py` file works.\n",
        "\n",
        "Source: _mcp_client/src/client.py_\n",
        "\n",
        "```python\n",
        "if args.transport == \"in-memory\":\n",
        "    ...\n",
        "    from mcp_server.src.server import create_mcp_server\n",
        "    mcp_server = create_mcp_server()\n",
        "    mcp_client = Client(mcp_server)\n",
        "\n",
        "elif args.transport == \"stdio\":\n",
        "    config = {\n",
        "        \"mcpServers\": {\n",
        "            \"research-agent\": {\n",
        "                \"transport\": \"stdio\",\n",
        "                \"command\": \"uv\",\n",
        "                \"args\": [\n",
        "                    \"--directory\", str(settings.server_main_path),\n",
        "                    \"run\", \"-m\", \"src.server\",\n",
        "                    \"--transport\", \"stdio\",\n",
        "                ],\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    mcp_client = Client(config)\n",
        "\n",
        "# At startup\n",
        "tools, resources, prompts = await get_capabilities_from_mcp_client(mcp_client)\n",
        "print_startup_info(tools, resources, prompts)\n",
        "\n",
        "async with mcp_client:\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"üë§ You: \").strip()\n",
        "        ...\n",
        "\n",
        "        # Parse input\n",
        "        parsed_input = parse_user_input(user_input)\n",
        "        ...\n",
        "\n",
        "        # Dispatch handling\n",
        "        await handle_user_message(parsed_input=parsed_input, ...)\n",
        "        ...\n",
        "```\n",
        "\n",
        "It does the following:\n",
        "1) Parse the `--transport` flag.\n",
        "2) If in-memory, build a `Client` with the FastMCP server object. If stdio, pass a config that tells FastMCP how to exec the server via `uv`.\n",
        "3) Query the MCP server for its capabilities (tools/resources/prompts) and print them.\n",
        "4) Enter the interactive loop: read input, parse it, and dispatch handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above is run when the MCP client is started. If you remember from previous cells, when the MCP client is started, it prints the following information:\n",
        "\n",
        "```\n",
        "üõ†Ô∏è Available tools: 11\n",
        "üìö Available resources: 2\n",
        "üí¨ Available prompts: 1\n",
        "\n",
        "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
        "```\n",
        "\n",
        "But, how does the MCP client know how many tools, resources, and prompts are available? Let's see how the `get_capabilities_from_mcp_client` function works.\n",
        "\n",
        "Source:\n",
        "_mcp_client/src/utils/mcp_startup_utils.py_\n",
        "\n",
        "```python\n",
        "async def get_capabilities_from_mcp_client(client: Client) -> tuple[List, List, List]:\n",
        "    \"\"\"Get available capabilities.\"\"\"\n",
        "    async with client:\n",
        "        tools = await client.list_tools()\n",
        "        resources = await client.list_resources()\n",
        "        prompts = await client.list_prompts()\n",
        "\n",
        "    return tools, resources, prompts\n",
        "```\n",
        "\n",
        "As you can see, the MCP client object has a `list_tools`, `list_resources`, and `list_prompts` method that returns the list of tools, resources, and prompts respectively. These lists contain information about their names, descriptions, parameters, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to learn how the MCP client parses the user input and how it handles the user messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Parsing Input and Commands\n",
        "\n",
        "The client supports a small command language. Input can be either a command (starting with `/`) or a freeform user message.\n",
        "\n",
        "Possible commands are:\n",
        "- `/tools`, `/resources`, `/prompts`\n",
        "- `/prompt/<name>` (e.g., `/prompt/full_research_instructions_prompt`)\n",
        "- `/resource/<uri>` (e.g., `/resource/system://status`)\n",
        "- `/model-thinking-switch`\n",
        "- `/quit`\n",
        "\n",
        "The `parse_user_input` function simply classifies the input (no side effects) and it returns a `ProcessedInput` with metadata. Here are some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "InputType.COMMAND_INFO_TOOLS\n",
            "InputType.COMMAND_INFO_RESOURCES\n",
            "InputType.COMMAND_PROMPT full_research_instructions_prompt\n",
            "InputType.NORMAL_MESSAGE\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_client.src.utils.parse_message_utils import parse_user_input\n",
        "\n",
        "processed_input = parse_user_input(\"/tools\")\n",
        "print(processed_input.input_type)\n",
        "\n",
        "processed_input = parse_user_input(\"/resources\")\n",
        "print(processed_input.input_type)\n",
        "\n",
        "processed_input = parse_user_input(\"/prompt/full_research_instructions_prompt\")\n",
        "print(processed_input.input_type, processed_input.prompt_name)\n",
        "\n",
        "processed_input = parse_user_input(\"Hello, how are you?\")\n",
        "print(processed_input.input_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These processed inputs are then used to dispatch the correct handling.\n",
        "\n",
        "The `handle_user_message` function orchestrates the conversation, calling the appropriate helper for the parsed command, or appending a normal message and running the agent loop.\n",
        "\n",
        "Here are some examples. Let's first create the MCP server and client, and get the server capabilities (available tools, resources, and prompts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"schema\" in \"FewShotExampleStructuredOutputCompliance\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n",
            "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/opik/error_tracking/shutdown_hooks.py:12: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  client = sentry_sdk.Hub.current.client\n",
            "\u001b[32m2025-09-17 11:29:42.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
            "\u001b[32m2025-09-17 11:29:42.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListResourcesRequest\n",
            "\u001b[32m2025-09-17 11:29:42.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListPromptsRequest\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-17 11:29:48.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
            "\u001b[32m2025-09-17 11:29:48.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_client.src.utils.mcp_startup_utils import get_capabilities_from_mcp_client\n",
        "from research_agent_part_2.mcp_client.src.utils.handle_message_utils import handle_user_message\n",
        "\n",
        "from research_agent_part_2.mcp_server.src.server import create_mcp_server\n",
        "from fastmcp import Client\n",
        "\n",
        "# Create the MCP server and client\n",
        "mcp_server = create_mcp_server()\n",
        "mcp_client = Client(mcp_server)\n",
        "\n",
        "# Get the MCP server capabilities\n",
        "tools, resources, prompts = await get_capabilities_from_mcp_client(mcp_client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's parse the user input and handle the user message with the `handle_user_message` function. Here is an example with commands (i.e. messages starting with `/`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[97m============================================================\u001b[0m\n",
            "\u001b[1m\u001b[96müìö Available Resources\u001b[0m\n",
            "\u001b[97m============================================================\u001b[0m\n",
            "\n",
            "\u001b[92m1. \u001b[0m\u001b[97msystem://status\u001b[0m\u001b[33m\n",
            "   Get system status and health information.\u001b[0m\n",
            "\n",
            "\u001b[92m2. \u001b[0m\u001b[97msystem://memory\u001b[0m\u001b[33m\n",
            "   Monitor memory usage of the server.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Parse the user input\n",
        "processed_input = parse_user_input(\"/resources\")\n",
        "conversation_history = []\n",
        "response = await handle_user_message(processed_input, tools, resources, prompts, conversation_history, mcp_client, thinking_enabled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `handle_user_message` function is basically a router that calls the appropriate helper for the parsed message. It is defined in the `handle_message_utils.py` file, you can read it to learn more about it.\n",
        "\n",
        "As previously explained, the `tools` object contains the list of tools registered in the MCP server, retrieved by the `list_tools` method. If the input is of type `COMMAND_INFO_TOOLS`, the `handle_command` function is called.\n",
        "\n",
        "Source:\n",
        "_mcp_client/src/utils/command_utils.py_\n",
        "\n",
        "```python\n",
        "def handle_command(processed_input: ProcessedInput, tools: List, resources: List, prompts: List):\n",
        "    \"\"\"Handle informational commands.\n",
        "\n",
        "    This function only handles informational commands (COMMAND_INFO_* types).\n",
        "    \"\"\"\n",
        "    if processed_input.input_type == InputType.COMMAND_INFO_TOOLS:\n",
        "        print_header(\"üõ†Ô∏è  Available Tools\")\n",
        "        for i, tool in enumerate(tools, 1):\n",
        "            print_item(tool.name, tool.description, i, Color.BRIGHT_WHITE, Color.YELLOW)\n",
        "    ...\n",
        "```\n",
        "\n",
        "This function retrieves, from each tool, the name and description, and prints them in a pretty format.\n",
        "\n",
        "All the tools are managed in a similar way.\n",
        "\n",
        "If the input message is of type `NORMAL_MESSAGE`, the `handle_agent_loop` function is called instead, which manages the agent loop for tool execution. Let's see how it works.\n",
        "\n",
        "Source:\n",
        "_mcp_client/src/utils/handle_agent_loop_utils.py_\n",
        "\n",
        "```python\n",
        "async def handle_agent_loop(\n",
        "    conversation_history: List[types.Content],\n",
        "    tools: List,\n",
        "    client: Client,\n",
        "    thinking_enabled: bool,\n",
        "):\n",
        "    \"\"\"Handle the agent loop for tool execution.\"\"\"\n",
        "    # Initialize LLM client\n",
        "    llm_config = build_llm_config_with_tools(tools, thinking_enabled)\n",
        "    llm_client = LLMClient(settings.model_id, llm_config)\n",
        "\n",
        "    while True:\n",
        "        print()\n",
        "        # Call LLM with current conversation history\n",
        "        response = await llm_client.generate_content(conversation_history)\n",
        "\n",
        "        # Extract and display thoughts as separate message (only if enabled)\n",
        "        if thinking_enabled:\n",
        "            thoughts = extract_thought_summary(response)\n",
        "            ...\n",
        "\n",
        "        # Check for function calls\n",
        "        function_call_info = extract_first_function_call(response)\n",
        "        if function_call_info:\n",
        "            name, args = function_call_info\n",
        "\n",
        "            # Check if this is a tool call\n",
        "            is_tool = any(tool.name == name for tool in tools)\n",
        "\n",
        "            if is_tool:\n",
        "                ...\n",
        "\n",
        "                # Execute the tool via MCP server\n",
        "                tool_result = await execute_tool(name, args, client)\n",
        "                # Add tool result to conversation history\n",
        "                tool_response = f\"Tool '{name}' executed successfully. Result: {tool_result}\"\n",
        "                conversation_history.append(types.Content(role=\"user\", parts=[types.Part(text=tool_response)]))\n",
        "                ...\n",
        "        else:\n",
        "            # Extract final text response - this ends the ReAct loop\n",
        "            final_text = extract_final_answer(response)\n",
        "            conversation_history.append(response.candidates[0].content)\n",
        "            ...\n",
        "            break  # Exit the agent loop\n",
        "```\n",
        "\n",
        "This function is the main loop that manages the agent loop for tool execution. It initializes the LLM client, builds the LLM configuration with the tools, and then enters the agent loop.\n",
        "\n",
        "The loop is structured as follows:\n",
        "\n",
        "1) Call the LLM with the current conversation history.\n",
        "2) Extract and display thoughts as separate message (only if enabled).\n",
        "3) Check for function calls.\n",
        "4) If there is a function call, check if it is a tool call.\n",
        "5) If it is a tool call, execute the tool via MCP server.\n",
        "6) Add the tool result to the conversation history.\n",
        "\n",
        "The `LLMClient` class is simply a wrapper class that allows to generate content (or a function call) with an LLM, independently from the specific LLM provider. Right now it only implements Google Gemini as model, but it can be easily extended to other models. It is defined in the `llm_utils.py` file.\n",
        "\n",
        "The `build_llm_config_with_tools` function builds the LLM configuration with the tools, it only works with Gemini for now. It is defined in the `llm_utils.py` file as well. Here's its code.\n",
        "\n",
        "```python\n",
        "def build_llm_config_with_tools(mcp_tools: List, thinking_enabled: bool = True) -> types.GenerateContentConfig:\n",
        "    \"\"\"Build Gemini config with all MCP tools converted to Gemini format.\"\"\"\n",
        "    gemini_tools = []\n",
        "\n",
        "    for tool in mcp_tools:\n",
        "        gemini_tool = types.Tool(\n",
        "            function_declarations=[\n",
        "                types.FunctionDeclaration(\n",
        "                    name=tool.name,\n",
        "                    description=tool.description,\n",
        "                    parameters=tool.inputSchema,\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        gemini_tools.append(gemini_tool)\n",
        "\n",
        "    # Create thinking config dynamically based on current state\n",
        "    thinking_config = types.ThinkingConfig(\n",
        "        include_thoughts=thinking_enabled,\n",
        "        thinking_budget=settings.thinking_budget,\n",
        "    )\n",
        "\n",
        "    return types.GenerateContentConfig(\n",
        "        tools=gemini_tools,\n",
        "        thinking_config=thinking_config,\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True),\n",
        "    )\n",
        "```\n",
        "\n",
        "The code above basically instructions the LLM to leverage thinking (if enabled) with the specificed thinking budget (i.e. the maximum number of tokens the LLM can use to think) and to use the available tools from the MCP server.\n",
        "\n",
        "The other functions from the `handle_agent_loop` function, like `extract_thought_summary` and `extract_final_answer`, are used to extract the thoughts and the final answer from the LLM response. It's boilerplate code that works for Gemini and can be copypasted for other projects.\n",
        "\n",
        "The `execute_tool` function is used to execute the tool via MCP server. It is defined in the `handle_agent_loop_utils.py` file. Here's its code.\n",
        "\n",
        "```python\n",
        "async def execute_tool(name: str, args: dict, client: Client):\n",
        "    \"\"\"Execute a tool and return the result.\"\"\"\n",
        "    ...\n",
        "    tool_result = await client.call_tool(name, args)\n",
        "    return tool_result\n",
        "```\n",
        "\n",
        "It uses the `call_tool` method of the `Client` object to execute the tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now test the MCP client with a user message that involves tool execution and see how the agent behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Processing Research Directory for Guideline URLs**\n",
            "\n",
            "Okay, so I need to get moving on this. The user wants me to grab some guideline URLs from a specific research directory.  My process here is pretty straightforward: I'll utilize the `extract_guidelines_urls` tool to do the heavy lifting.  I'll call the `default_api.extract_guidelines_urls` function and plug in the provided `research_directory` path. Simple enough.  I'm ready to handle potential errors that might pop up during the tool's execution. If something goes wrong, I'll need to interpret any error messages to offer a clear explanation to the user.  Hopefully, it'll run smoothly, but I'm prepared for anything.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36müîß Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mextract_guidelines_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m‚ö° Executing tool 'extract_guidelines_urls' via MCP server...\u001b[0m\n",
            "\u001b[36m‚úÖ Tool execution successful!\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Assessing Next Steps After URL Extraction**\n",
            "\n",
            "Okay, so I've successfully executed `extract_guidelines_urls` and have the results. Now, based on my understanding of the research process and the available tools, the natural next step would be to process the URLs that were extracted. Looking at the output, I see I have one GitHub URL, two YouTube URLs, and six web URLs, but no local files.\n",
            "\n",
            "Since there are no local files, I don't need to worry about `process_local_files` right now. However, I *do* have URLs that need processing, specifically from GitHub, YouTube, and the web. The tools for this are `process_github_urls`, `transcribe_youtube_urls`, and `scrape_and_clean_other_urls`, respectively.\n",
            "\n",
            "But here's the crucial point: I was explicitly instructed to *stop* after `extract_guidelines_urls`. No other tool was to be run automatically. So, even though I know how to move forward, I must adhere to the instructions. I cannot just jump in and start running those processing tools.\n",
            "\n",
            "Therefore, my next action is to inform the user about the results of the extraction and the available processing options. I need to ask them if they want to move forward with cleaning and processing the URLs, or if they have other instructions. I'm ready to follow their lead and ensure the research process continues in the desired direction.\u001b[0m\n",
            "\n",
            "\u001b[37müí¨ LLM Response: \u001b[0mI have successfully extracted the URLs from the article guidelines. Here's a summary of what was found:\n",
            "\n",
            "*   **GitHub URLs:** 1\n",
            "*   **YouTube Videos URLs:** 2\n",
            "*   **Other Web URLs:** 6\n",
            "*   **Local File References:** 0\n",
            "\n",
            "The results have been saved to `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json`.\n",
            "\n",
            "Would you like to proceed with processing these URLs (e.g., scraping the web URLs, transcribing YouTube videos, or processing GitHub repositories)?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Parse the user input\n",
        "path_to_research_folder = \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
        "message = (\n",
        "    f\"Call the 'extract_guidelines_urls' tool with the '{path_to_research_folder}' directory as research folder, and stop after the tool has finished running.\"\n",
        "    \"Don't run any other tool after the 'extract_guidelines_urls' tool has finished running.\"\n",
        "    \"If the tool fails, explain to me the error message.\"\n",
        ")\n",
        "processed_input = parse_user_input(message)\n",
        "conversation_history = []\n",
        "async with mcp_client:\n",
        "    response = await handle_user_message(processed_input, tools, resources, prompts, conversation_history, mcp_client, thinking_enabled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are good to go!\n",
        "\n",
        "In the next lesson, we'll learn more about how the MCP prompt is used by the MCP client to orchestrate the agentic workflow.\n",
        "Then, we'll go through each step of the research agent workflow, and we'll see how to run each tool in isolation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
