{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Factory íŠœí† ë¦¬ì–¼\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‹¤ì–‘í•œ LLM(Large Language Model) ì œê³µì—…ì²´ì˜ ëª¨ë¸ë“¤ì„ í†µí•©ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥\n",
    "- OpenAI, Anthropic, Naver ë“± ë‹¤ì–‘í•œ LLM ì œê³µì—…ì²´ ì§€ì›\n",
    "- íŒ©í† ë¦¬ íŒ¨í„´ì„ í†µí•œ ì¼ê´€ëœ ëª¨ë¸ ìƒì„± ì¸í„°í˜ì´ìŠ¤\n",
    "- ì¤‘ì•™ ì§‘ì¤‘ì‹ ëª¨ë¸ ì„¤ì • ê´€ë¦¬\n",
    "- ì»¤ìŠ¤í…€ ëª¨ë¸ ì„¤ì • ì§€ì›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "ë¨¼ì € í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ê° LLM ì œê³µì—…ì²´ì˜ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# API í‚¤ ì„¤ì •\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key\"\n",
    "# os.environ[\"NCP_APIGW_API_KEY\"] = \"your-naver-apigw-key\"\n",
    "# os.environ[\"NCP_CLOVASTUDIO_API_KEY\"] = \"your-naver-studio-key\"\n",
    "# os.environ[\"DEEPSEEK_API_KEY\"] = \"your-deepseek-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸°ë³¸ ì‚¬ìš©ë²•\n",
    "\n",
    "### 2.1 ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['gpt-4o', 'gpt-4o-mini', 'deepseek-v3', 'deepseek-reasoner', 'claude-3.7-sonnet', 'claude-3.5-sonnet', 'claude-3.5-haiku', 'naver', 'gemini-2.0-flash']\n"
     ]
    }
   ],
   "source": [
    "from langevaluate.llmfactory import LLMFactory\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸\n",
    "available_models = LLMFactory.get_model_list()\n",
    "print(\"Available models:\", available_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ê¸°ë³¸ ëª¨ë¸ ìƒì„± ë° ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 ëª¨ë¸ ìƒì„±\n",
    "gpt4_llm = LLMFactory.create_llm(\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Claude ëª¨ë¸ ìƒì„±\n",
    "# claude_llm = LLMFactory.create_llm(\"claude-3.5-sonnet\", temperature=0.5)\n",
    "\n",
    "# ë„¤ì´ë²„ ëª¨ë¸ ìƒì„±\n",
    "naver_llm = LLMFactory.create_llm(\"naver\", temperature=0.8)\n",
    "\n",
    "# deepseek ëª¨ë¸ ìƒì„±\n",
    "deepseek_llm = LLMFactory.create_llm('deepseek-v3', temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await deepseek_llm.ainvoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I assist you today? ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 4, 'total_tokens': 15, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 4}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4_prod0225', 'finish_reason': 'stop', 'logprobs': None} id='run-f5f53f47-0cb6-4bba-b5af-cf547f3479c4-0' usage_metadata={'input_tokens': 4, 'output_tokens': 11, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? ğŸ˜Š'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 11,\n",
       "  'prompt_tokens': 4,\n",
       "  'total_tokens': 15,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0},\n",
       "  'prompt_cache_hit_tokens': 0,\n",
       "  'prompt_cache_miss_tokens': 4},\n",
       " 'model_name': 'deepseek-chat',\n",
       " 'system_fingerprint': 'fp_3a5770e1b4_prod0225',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì»¤ìŠ¤í…€ ëª¨ë¸ ì„¤ì • ì‚¬ìš©\n",
    "\n",
    "get_config ë©”ì†Œë“œë¥¼ í™œìš©í•˜ë©´ í˜„ì¬ Modelì˜ Config ê°’ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(LLMFactory.get_config('deepseek-v3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê¸°ë³¸ ì œê³µë˜ëŠ” ëª¨ë¸ ì™¸ì—ë„ ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langevaluate.config import ModelConfig\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë¸ ì„¤ì • ìƒì„±\n",
    "custom_config = ModelConfig(\n",
    "    model_name=\"gpt-4-turbo-preview\",\n",
    "    api_base=\"https://api.openai.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    max_tokens=128000,\n",
    "    seed=66,\n",
    "    provider=\"openai\"\n",
    ")\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\n",
    "custom_llm = LLMFactory.create_llm(custom_config, temperature=0.7, rpm=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë¡œì»¬ ëª¨ë¸ ì„¤ì • ì‚¬ìš©\n",
    "\n",
    "CustomModelConfigë¥¼ ì‚¬ìš©í•˜ì—¬ LocalGPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langevaluate.config import LocalModelConfig\n",
    "\n",
    "local_config = LocalModelConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (Qwen2.5-7B-Instruct-1M)\n",
    "    port=30000,  # ì„œë²„ê°€ ì‹¤í–‰ë  í¬íŠ¸ ë²ˆí˜¸ (30000ë²ˆ í¬íŠ¸ì—ì„œ ëŒ€ê¸°)\n",
    "    max_tokens=4000,  # input í† í° ìˆ˜ + output í† í° ìˆ˜\n",
    "    gpus=\"0\",  # ì‚¬ìš©í•  GPU ID (GPU 1ë²ˆê³¼ 2ë²ˆ ì‚¬ìš©)\n",
    "    dp=1,  # ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ (Data Parallelism) í™œì„±í™” (2ê°œ GPU ì‚¬ìš©), token output speed ì¦ê°€\n",
    "    tp=1,   # í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ (Tensor Parallelism) í™œì„±í™” (2ê°œ GPU ì‚¬ìš©), GPU VRAM í¬ê¸°ê°€ ëª¨ìë¥´ë‹¤ë©´ ë” í‚¤ì›Œì•¼í•¨.\n",
    "    max_running_request=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting llm server boot\n",
      "WARNING 02-04 06:20:25 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "[2025-02-04 06:20:31] server_args=ServerArgs(model_path='Qwen/Qwen2.5-1.5B-Instruct', tokenizer_path='Qwen/Qwen2.5-1.5B-Instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2.5-1.5B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=30000, mem_fraction_static=0.9, max_running_requests=1024, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=66, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\n",
      "WARNING 02-04 06:20:36 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "WARNING 02-04 06:20:36 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "[2025-02-04 06:20:44 TP0] Init torch distributed begin.\n",
      "[W204 06:20:44.223751281 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:30299 (errno: 97 - Address family not supported by protocol).\n",
      "[2025-02-04 06:20:44 TP0] Load weight begin. avail mem=47.17 GB\n",
      "[2025-02-04 06:20:46 TP0] Using model weights format ['*.safetensors']\n",
      "[2025-02-04 06:20:46 TP0] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.84it/s]\n",
      "\n",
      "[2025-02-04 06:20:47 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=44.10 GB\n",
      "[2025-02-04 06:20:47 TP0] KV Cache is allocated. K size: 19.69 GB, V size: 19.69 GB.\n",
      "[2025-02-04 06:20:47 TP0] Memory pool end. avail mem=4.49 GB\n",
      "[2025-02-04 06:20:47 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:13<00:00,  1.72it/s]\n",
      "[2025-02-04 06:21:01 TP0] Capture cuda graph end. Time elapsed: 13.40 s\n",
      "[2025-02-04 06:21:01 TP0] max_total_num_tokens=1474797, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=1024, context_len=32768\n",
      "[2025-02-04 06:21:02] INFO:     Started server process [87426]\n",
      "[2025-02-04 06:21:02] INFO:     Waiting for application startup.\n",
      "[2025-02-04 06:21:02] INFO:     Application startup complete.\n",
      "[2025-02-04 06:21:02] INFO:     Uvicorn running on http://0.0.0.0:30000 (Press CTRL+C to quit)\n",
      "[2025-02-04 06:21:02] INFO:     127.0.0.1:52308 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-02-04 06:21:02] INFO:     127.0.0.1:52314 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-02-04 06:21:02 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-04 06:21:07] INFO:     127.0.0.1:52316 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-04 06:21:07] The server is fired up and ready to roll!\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "# localllmì€ ì„œë²„ë¥¼ localì—ì„œ ì‹¤í–‰ì‹œí‚¤ê¸° ë•Œë¬¸ì— ë¶€íŒ…ë˜ëŠ” ì‹œê°„ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "local_llm = LLMFactory.create_llm(local_config, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-04 06:21:07 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-04 06:21:08 TP0] Decode batch. #running-req: 1, #token: 68, token usage: 0.00, gen throughput (token/s): 5.87, #queue-req: 0\n",
      "[2025-02-04 06:21:08] INFO:     127.0.0.1:52324 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm Qwen, a large language model created by Alibaba Cloud. I'm here to help you with information and answer any questions you might have. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 35, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen2.5-1.5B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4a8d3093-8153-4cb3-8f0d-dcf6ed7eeaec-0', usage_metadata={'input_tokens': 35, 'output_tokens': 40, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await local_llm.ainvoke(\"hi. who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„œë²„ëŠ” shutdownì„ ì´ìš©í•˜ì—¬ ëŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "local_llm.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ì„œë²„ ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = ModelConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    api_base=\"http://exaone_3.5_32b:8000/v1\",\n",
    "    api_key='EMPTY',\n",
    "    max_tokens=20000,\n",
    "    seed=66,\n",
    "    provider=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# localllmì€ ì„œë²„ë¥¼ localì—ì„œ ì‹¤í–‰ì‹œí‚¤ê¸° ë•Œë¬¸ì— ë¶€íŒ…ë˜ëŠ” ì‹œê°„ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "custom_llm = LLMFactory.create_llm(local_config, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. request per minute ì œì–´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_llm = LLMFactory.create_llm('gpt-4o-mini', temperature=0.7, rpm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-16a9221d-ccdb-4537-a90b-0a3bc8f2f420-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 9, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_3267753c5d', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe241ad7-b207-4149-a3ee-9831671cefd0-0', usage_metadata={'input_tokens': 9, 'output_tokens': 11, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-c5113c8c-dc31-4ce9-8c2f-83b7eee9db29-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Hello! It seems like you might have typed something by mistake. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 8, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-98646bcc-6c9e-492d-9593-bae7c363513d-0', usage_metadata={'input_tokens': 8, 'output_tokens': 21, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await limit_llm.abatch(['hi', 'ì•ˆë…•', 'hello', 'jj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ\n",
    "\n",
    "ìƒì„±ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPT-4 Response ===\n",
      "ë§Œì„±ì½©íŒ¥ë³‘(Chronic Kidney Disease, CKD)ì€ ì½©íŒ¥(ì‹ ì¥)ì˜ ê¸°ëŠ¥ì´ ì ì§„ì ìœ¼ë¡œ ì €í•˜ë˜ëŠ” ì§ˆí™˜ì„ ë§í•©ë‹ˆë‹¤. ì´ ë³‘ì€ ì—¬ëŸ¬ ê°€ì§€ ì›ì¸ì— ì˜í•´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ ë‹¹ë‡¨ë³‘, ê³ í˜ˆì••, ê·¸ë¦¬ê³  ì‹¬í˜ˆê´€ ì§ˆí™˜ê³¼ ê°™ì€ ë§Œì„± ì§ˆí™˜ì´ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤. ë§Œì„±ì½©íŒ¥ë³‘ì€ ì¼ë°˜ì ìœ¼ë¡œ 3ê°œì›” ì´ìƒ ì§€ì†ë˜ë©°, ì‹ ì¥ì˜ ê¸°ëŠ¥ì´ ì•…í™”ë¨ì— ë”°ë¼ ì—¬ëŸ¬ ê°€ì§€ í•©ë³‘ì¦ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ë§Œì„±ì½©íŒ¥ë³‘ì˜ ë‹¨ê³„\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì€ ì‹ ì¥ ê¸°ëŠ¥ì˜ ì •ë„ì— ë”°ë¼ 5ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "1. **1ë‹¨ê³„**: ì‹ ì¥ ê¸°ëŠ¥ì´ ì •ìƒì´ë‚˜, ì‹ ì¥ ì†ìƒì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°.\n",
      "2. **2ë‹¨ê³„**: ê²½ë¯¸í•œ ì‹ ì¥ ê¸°ëŠ¥ ì €í•˜ (GFR 60-89 mL/min).\n",
      "3. **3ë‹¨ê³„**: ì¤‘ê°„ ì •ë„ì˜ ì‹ ì¥ ê¸°ëŠ¥ ì €í•˜ (GFR 30-59 mL/min). ì´ ë‹¨ê³„ëŠ” ë‹¤ì‹œ 3a(45-59)ì™€ 3b(30-44)ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "4. **4ë‹¨ê³„**: ì‹¬ê°í•œ ì‹ ì¥ ê¸°ëŠ¥ ì €í•˜ (GFR 15-29 mL/min).\n",
      "5. **5ë‹¨ê³„**: ì‹ ë¶€ì „ (GFR < 15 mL/min)ìœ¼ë¡œ, ì´ ë‹¨ê³„ì—ì„œëŠ” íˆ¬ì„ì´ë‚˜ ì‹ ì¥ ì´ì‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ì¦ìƒ\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì€ ì´ˆê¸°ì—ëŠ” ì¦ìƒì´ ê±°ì˜ ì—†ê±°ë‚˜ ê²½ë¯¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë³‘ì´ ì§„í–‰ë¨ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "- í”¼ë¡œê°ê³¼ ë¬´ê¸°ë ¥\n",
      "- ë¶€ì¢… (íŠ¹íˆ ë°œê³¼ ë°œëª©)\n",
      "- ê³ í˜ˆì••\n",
      "- ì†Œë³€ì˜ ë³€í™” (ì–‘, ìƒ‰, ëƒ„ìƒˆ)\n",
      "- ì‹ìš• ê°ì†Œì™€ ì²´ì¤‘ ê°ì†Œ\n",
      "- ë©”ìŠ¤êº¼ì›€ ë° êµ¬í† \n",
      "- ê°€ë ¤ì›€ì¦\n",
      "\n",
      "### ì§„ë‹¨\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì€ í˜ˆì•¡ ê²€ì‚¬, ì†Œë³€ ê²€ì‚¬, ê·¸ë¦¬ê³  ì˜ìƒ ê²€ì‚¬ ë“±ì„ í†µí•´ ì§„ë‹¨ë©ë‹ˆë‹¤. íŠ¹íˆ í˜ˆì•¡ì—ì„œ í¬ë ˆì•„í‹°ë‹Œ ìˆ˜ì¹˜ë¥¼ ì¸¡ì •í•˜ì—¬ ì‹ ì¥ ê¸°ëŠ¥ì„ í‰ê°€í•˜ë©°, GFR(ì‚¬êµ¬ì²´ ì—¬ê³¼ìœ¨)ì„ ê³„ì‚°í•˜ì—¬ ë³‘ì˜ ë‹¨ê³„ë¥¼ ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ì¹˜ë£Œ\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì˜ ì¹˜ë£ŒëŠ” ì£¼ë¡œ ì›ì¸ ì§ˆí™˜ì˜ ê´€ë¦¬ì™€ ì‹ ì¥ ê¸°ëŠ¥ ì €í•˜ì˜ ì§„í–‰ì„ ëŠ¦ì¶”ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì¹˜ë£Œë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "- í˜ˆì•• ì¡°ì ˆ (í•­ê³ í˜ˆì••ì œ ì‚¬ìš©)\n",
      "- ë‹¹ë‡¨ ê´€ë¦¬\n",
      "- ì‹ì´ìš”ë²• (ë‹¨ë°±ì§ˆ, ë‚˜íŠ¸ë¥¨, ì¹¼ë¥¨ ì„­ì·¨ ì¡°ì ˆ)\n",
      "- í•„ìš”í•œ ê²½ìš° ì•½ë¬¼ ì¹˜ë£Œ\n",
      "- ë³‘ì´ ì§„í–‰ëœ ê²½ìš° íˆ¬ì„ì´ë‚˜ ì‹ ì¥ ì´ì‹ ê³ ë ¤\n",
      "\n",
      "### ì˜ˆë°©\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì„ ì˜ˆë°©í•˜ê¸° ìœ„í•´ì„œëŠ” ê±´ê°•í•œ ìƒí™œ ìŠµê´€ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì¶©ë¶„í•œ ìˆ˜ë¶„ ì„­ì·¨, ê·œì¹™ì ì¸ ìš´ë™, ì ì ˆí•œ ì‹ì‚¬, ê·¸ë¦¬ê³  ì •ê¸°ì ì¸ ê±´ê°• ê²€ì§„ì´ ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì€ ì‹¬ê°í•œ í•©ë³‘ì¦ì„ ì´ˆë˜í•  ìˆ˜ ìˆëŠ” ì§ˆí™˜ì´ë¯€ë¡œ, ì¡°ê¸°ì— ë°œê²¬í•˜ê³  ì ì ˆí•œ ê´€ë¦¬ë¥¼ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "=== DeepSeek Response ===\n",
      "ë§Œì„±ì½©íŒ¥ë³‘(ë§Œì„± ì‹ ì¥ë³‘, Chronic Kidney Disease, CKD)ì€ ì‹ ì¥ ê¸°ëŠ¥ì´ ì ì§„ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì§ˆí™˜ì„ ë§í•©ë‹ˆë‹¤. ì‹ ì¥ì€ ìš°ë¦¬ ëª¸ì—ì„œ ë…¸íë¬¼ì„ ê±¸ëŸ¬ë‚´ê³ , ì²´ì•¡ê³¼ ì „í•´ì§ˆ ê· í˜•ì„ ìœ ì§€í•˜ë©°, í˜ˆì•• ì¡°ì ˆ, ì í˜ˆêµ¬ ìƒì„±, ë¼ˆ ê±´ê°• ìœ ì§€ ë“± ë‹¤ì–‘í•œ ì¤‘ìš”í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë§Œì„±ì½©íŒ¥ë³‘ì€ ì´ëŸ¬í•œ ì‹ ì¥ ê¸°ëŠ¥ì´ ì„œì„œíˆ ì €í•˜ë˜ì–´ ê²°êµ­ ì‹ ì¥ì´ ì œ ì—­í• ì„ í•˜ì§€ ëª»í•˜ê²Œ ë˜ëŠ” ìƒíƒœë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ì£¼ìš” ì›ì¸\n",
      "1. **ë‹¹ë‡¨ë³‘**: ê°€ì¥ í”í•œ ì›ì¸ìœ¼ë¡œ, ê³ í˜ˆë‹¹ì´ ì‹ ì¥ì˜ í˜ˆê´€ê³¼ í•„í„°ë¥¼ ì†ìƒì‹œí‚µë‹ˆë‹¤.\n",
      "2. **ê³ í˜ˆì••**: ê³ í˜ˆì••ì€ ì‹ ì¥ì˜ í˜ˆê´€ì— ë¶€ë‹´ì„ ì£¼ì–´ ì†ìƒì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "3. **ì‹ ì¥ì—¼**: ì‹ ì¥ì˜ ì—¼ì¦ì„± ì§ˆí™˜ìœ¼ë¡œ, ë©´ì—­ì²´ê³„ì˜ ì´ìƒì´ ì›ì¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "4. **ë‹¤ë‚­ì„± ì‹ ì¥ë³‘**: ìœ ì „ì  ìš”ì¸ìœ¼ë¡œ ì‹ ì¥ì— ì—¬ëŸ¬ ê°œì˜ ë‚­ì¢…ì´ ìƒê¸°ëŠ” ì§ˆí™˜.\n",
      "5. **ì•½ë¬¼ ë° ë…ì†Œ**: ì¥ê¸°ê°„ì˜ ì§„í†µì œ ì‚¬ìš©, ì¤‘ê¸ˆì† ë…¸ì¶œ ë“±ì´ ì‹ ì¥ ì†ìƒì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "6. **ê¸°íƒ€**: ë¹„ë§Œ, í¡ì—°, ê³ ë ¹, ì‹¬í˜ˆê´€ ì§ˆí™˜ ë“±ë„ ìœ„í—˜ ìš”ì¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "### ì¦ìƒ\n",
      "ì´ˆê¸°ì—ëŠ” íŠ¹ë³„í•œ ì¦ìƒì´ ì—†ì„ ìˆ˜ ìˆì§€ë§Œ, ì§ˆí™˜ì´ ì§„í–‰ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "- í”¼ë¡œê°\n",
      "- ì†Œë³€ëŸ‰ ë³€í™” (ì†Œë³€ëŸ‰ ê°ì†Œ ë˜ëŠ” ì¦ê°€)\n",
      "- ë¶€ê¸° (íŠ¹íˆ ë‹¤ë¦¬, ë°œ, ì–¼êµ´)\n",
      "- í˜¸í¡ ê³¤ë€\n",
      "- ì‹ìš• ê°ì†Œ, ë©”ìŠ¤êº¼ì›€, êµ¬í† \n",
      "- í”¼ë¶€ ê°€ë ¤ì›€ì¦\n",
      "- ê·¼ìœ¡ ê²½ë ¨ ë° ì‡ ì•½ê°\n",
      "- ìˆ˜ë©´ ì¥ì• \n",
      "\n",
      "### ì§„ë‹¨\n",
      "- **í˜ˆì•¡ ê²€ì‚¬**: í¬ë ˆì•„í‹°ë‹Œ ìˆ˜ì¹˜ë¥¼ ì¸¡ì •í•˜ì—¬ ì‹ ì¥ ê¸°ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
      "- **ì†Œë³€ ê²€ì‚¬**: ë‹¨ë°±ë‡¨ë‚˜ í˜ˆë‡¨ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
      "- **ì˜ìƒ ê²€ì‚¬**: ì´ˆìŒíŒŒ, CT, MRI ë“±ì„ í†µí•´ ì‹ ì¥ì˜ êµ¬ì¡°ì  ì´ìƒì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
      "- **ì‹ ì¥ ìƒê²€**: í•„ìš”í•œ ê²½ìš° ì¡°ì§ ê²€ì‚¬ë¥¼ í†µí•´ ì›ì¸ì„ íŒŒì•…í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ì¹˜ë£Œ\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì€ ì™„ì¹˜ê°€ ì–´ë µì§€ë§Œ, ì§„í–‰ì„ ëŠ¦ì¶”ê³  ì¦ìƒì„ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
      "1. **ì›ì¸ ì§ˆí™˜ ê´€ë¦¬**: ë‹¹ë‡¨ë³‘, ê³ í˜ˆì•• ë“±ì„ ì² ì €íˆ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
      "2. **ì•½ë¬¼ ì¹˜ë£Œ**: í˜ˆì•• ì¡°ì ˆ, ë‹¨ë°±ë‡¨ ê°ì†Œ, ë¹ˆí˜ˆ ì¹˜ë£Œ ë“±ì„ ìœ„í•œ ì•½ë¬¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "3. **ì‹ì´ ì¡°ì ˆ**: ë‹¨ë°±ì§ˆ, ë‚˜íŠ¸ë¥¨, ì¹¼ë¥¨, ì¸ ë“±ì˜ ì„­ì·¨ë¥¼ ì œí•œí•©ë‹ˆë‹¤.\n",
      "4. **ìƒí™œìŠµê´€ ê°œì„ **: ê¸ˆì—°, ì²´ì¤‘ ê´€ë¦¬, ê·œì¹™ì ì¸ ìš´ë™ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "5. **ì‹ ëŒ€ì²´ ìš”ë²•**: ë§ê¸° ì‹ ë¶€ì „ ë‹¨ê³„ì—ì„œëŠ” íˆ¬ì„(í˜ˆì•¡ íˆ¬ì„, ë³µë§‰ íˆ¬ì„)ì´ë‚˜ ì‹ ì¥ ì´ì‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ì˜ˆë°©\n",
      "- ê±´ê°•í•œ ì‹ìŠµê´€ ìœ ì§€\n",
      "- ì •ê¸°ì ì¸ ê±´ê°• ê²€ì§„\n",
      "- ë‹¹ë‡¨ë³‘ ë° ê³ í˜ˆì•• ê´€ë¦¬\n",
      "- ì•½ë¬¼ ë‚¨ìš© í”¼í•˜ê¸°\n",
      "- ì¶©ë¶„í•œ ìˆ˜ë¶„ ì„­ì·¨\n",
      "\n",
      "ë§Œì„±ì½©íŒ¥ë³‘ì€ ì¡°ê¸° ë°œê²¬ê³¼ ì ì ˆí•œ ê´€ë¦¬ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ, ìœ„í—˜ ìš”ì¸ì´ ìˆë‹¤ë©´ ì •ê¸°ì ìœ¼ë¡œ ê²€ì§„ì„ ë°›ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "=== Naver Response ===\n",
      "ë§Œì„± ì½©íŒ¥ë³‘ì€ 3ê°œì›” ì´ìƒ ì‹ ì¥ì´ ì†ìƒë˜ì–´ ìˆê±°ë‚˜, ì‹ ì¥ ê¸°ëŠ¥ ê°ì†Œê°€ ì§€ì†ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ì§ˆí™˜ì„ ë§í•©ë‹ˆë‹¤. ë§Œì„± ì½©íŒ¥ë³‘ì€ ë‹¤ì–‘í•œ ì›ì¸ì— ì˜í•´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°, ê·¸ ì¤‘ì—ì„œë„ ë‹¹ë‡¨ë³‘, ê³ í˜ˆì••, ì‚¬êµ¬ì²´ì‹ ì—¼ ë“±ì´ ì£¼ìš” ì›ì¸ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë§Œì„± ì½©íŒ¥ë³‘ì˜ ì¦ìƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. ì†Œë³€ëŸ‰ ê°ì†Œ: ì†Œë³€ëŸ‰ì´ ê°ì†Œí•˜ì—¬ ë…¸íë¬¼ì´ ì²´ë‚´ì— ìŒ“ì´ê²Œ ë©ë‹ˆë‹¤.\n",
      "2. ë¶€ì¢…: ëª¸ì´ ë¶“ê³  ì²´ì¤‘ì´ ì¦ê°€í•©ë‹ˆë‹¤.\n",
      "3. í˜ˆì•• ìƒìŠ¹: í˜ˆì••ì´ ìƒìŠ¹í•˜ì—¬ ê³ í˜ˆì••ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "4. í”¼ë¡œê°: í”¼ë¡œê°ì´ ì‹¬í•´ì§€ë©°, ì§‘ì¤‘ë ¥ì´ ì €í•˜ë©ë‹ˆë‹¤.\n",
      "5. ì‹ìš• ë¶€ì§„: ì‹ìš•ì´ ì €í•˜ë˜ë©°, êµ¬í† ë‚˜ ì„¤ì‚¬ ë“±ì˜ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "6. í”¼ë¶€ ê±´ì¡°: í”¼ë¶€ê°€ ê±´ì¡°í•´ì§€ê³  ê°€ë ¤ì›€ì¦ì´ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "7. ë¹ˆí˜ˆ: í˜ˆì•¡ ë‚´ ì í˜ˆêµ¬ ìˆ˜ì¹˜ê°€ ê°ì†Œí•˜ì—¬ ë¹ˆí˜ˆì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "8. ê°ê° ì´ìƒ: ì†ë°œì´ ì €ë¦¬ê³  ê°ê°ì´ ë‘”í•´ì§€ëŠ” ë“±ì˜ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë§Œì„± ì½©íŒ¥ë³‘ì€ ì¹˜ë£Œê°€ ì–´ë ¤ìš´ ì§ˆí™˜ ì¤‘ í•˜ë‚˜ ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¡°ê¸° ë°œê²¬ê³¼ ì ì ˆí•œ ì¹˜ë£Œë¥¼ í†µí•´ ë³‘ì˜ ì§„í–‰ì„ ëŠ¦ì¶”ê³ , í•©ë³‘ì¦ì„ ì˜ˆë°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì •ê¸°ì ì¸ ê±´ê°•ê²€ì§„ì„ í†µí•´ ë§Œì„± ì½©íŒ¥ë³‘ì„ ì¡°ê¸°ì— ë°œê²¬í•˜ê³ , ì ì ˆí•œ ì¹˜ë£Œë¥¼ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë˜, ìƒí™œìŠµê´€ ê°œì„ ë„ ì¤‘ìš”í•œë°, ì €ì—¼ì‹ ì‹ë‹¨ì„ ìœ ì§€í•˜ê³ , ë‹¨ë°±ì§ˆê³¼ ì¹¼ë¥¨ ì„­ì·¨ë¥¼ ì œí•œí•˜ë©°, ë‹´ë°°ì™€ ìˆ ì„ í”¼í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ìš´ë™ì„ í†µí•´ ì²´ì¤‘ì„ ì¡°ì ˆí•˜ê³ , í˜ˆì••ê³¼ í˜ˆë‹¹ì„ ê´€ë¦¬í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ\n",
    "messages = [\n",
    "    HumanMessage(content=\"ë§Œì„±ì½©íŒ¥ë³‘ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "]\n",
    "\n",
    "# ê° ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±\n",
    "gpt4_response = gpt4_llm.invoke(messages)\n",
    "deepseek_response = deepseek_llm.invoke(messages)\n",
    "naver_response = naver_llm.invoke(messages)\n",
    "\n",
    "print(\"=== GPT-4 Response ===\")\n",
    "print(gpt4_response.content)\n",
    "print(\"\\n=== DeepSeek Response ===\")\n",
    "print(deepseek_response.content)\n",
    "print(\"\\n=== Naver Response ===\")\n",
    "print(naver_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” LLMFactoryë¥¼ ì‚¬ìš©í•˜ì—¬:\n",
    "1. ë‹¤ì–‘í•œ LLM ì œê³µì—…ì²´ì˜ ëª¨ë¸ì„ ìƒì„±í•˜ê³ \n",
    "2. ì»¤ìŠ¤í…€ ëª¨ë¸ ì„¤ì •ì„ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ë©°\n",
    "3. ìƒì„±ëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ \n",
    "4. ì—ëŸ¬ ìƒí™©ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.\n",
    "\n",
    "LLMFactoryëŠ” ì—¬ëŸ¬ LLM ì œê³µì—…ì²´ì˜ ëª¨ë¸ì„ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°,\n",
    "ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬ë¥¼ í†µí•´ ëª¨ë¸ ì„¤ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
