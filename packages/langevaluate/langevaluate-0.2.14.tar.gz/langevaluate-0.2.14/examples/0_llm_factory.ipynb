{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Factory 튜토리얼\n",
    "\n",
    "이 튜토리얼에서는 다양한 LLM(Large Language Model) 제공업체의 모델들을 통합적으로 관리하고 사용하는 방법을 배워보겠습니다.\n",
    "\n",
    "## 주요 기능\n",
    "- OpenAI, Anthropic, Naver 등 다양한 LLM 제공업체 지원\n",
    "- 팩토리 패턴을 통한 일관된 모델 생성 인터페이스\n",
    "- 중앙 집중식 모델 설정 관리\n",
    "- 커스텀 모델 설정 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정\n",
    "\n",
    "먼저 필요한 환경변수를 설정합니다. 각 LLM 제공업체의 API 키가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# API 키 설정\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key\"\n",
    "# os.environ[\"NCP_APIGW_API_KEY\"] = \"your-naver-apigw-key\"\n",
    "# os.environ[\"NCP_CLOVASTUDIO_API_KEY\"] = \"your-naver-studio-key\"\n",
    "# os.environ[\"DEEPSEEK_API_KEY\"] = \"your-deepseek-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기본 사용법\n",
    "\n",
    "### 2.1 사용 가능한 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['gpt-4o', 'gpt-4o-mini', 'deepseek-v3', 'deepseek-reasoner', 'claude-3.7-sonnet', 'claude-3.5-sonnet', 'claude-3.5-haiku', 'naver', 'gemini-2.0-flash']\n"
     ]
    }
   ],
   "source": [
    "from langevaluate.llmfactory import LLMFactory\n",
    "\n",
    "# 사용 가능한 모델 목록 확인\n",
    "available_models = LLMFactory.get_model_list()\n",
    "print(\"Available models:\", available_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 기본 모델 생성 및 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 모델 생성\n",
    "gpt4_llm = LLMFactory.create_llm(\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Claude 모델 생성\n",
    "# claude_llm = LLMFactory.create_llm(\"claude-3.5-sonnet\", temperature=0.5)\n",
    "\n",
    "# 네이버 모델 생성\n",
    "naver_llm = LLMFactory.create_llm(\"naver\", temperature=0.8)\n",
    "\n",
    "# deepseek 모델 생성\n",
    "deepseek_llm = LLMFactory.create_llm('deepseek-v3', temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await deepseek_llm.ainvoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I assist you today? 😊' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 4, 'total_tokens': 15, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 4}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4_prod0225', 'finish_reason': 'stop', 'logprobs': None} id='run-f5f53f47-0cb6-4bba-b5af-cf547f3479c4-0' usage_metadata={'input_tokens': 4, 'output_tokens': 11, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? 😊'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 11,\n",
       "  'prompt_tokens': 4,\n",
       "  'total_tokens': 15,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0},\n",
       "  'prompt_cache_hit_tokens': 0,\n",
       "  'prompt_cache_miss_tokens': 4},\n",
       " 'model_name': 'deepseek-chat',\n",
       " 'system_fingerprint': 'fp_3a5770e1b4_prod0225',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 커스텀 모델 설정 사용\n",
    "\n",
    "get_config 메소드를 활용하면 현재 Model의 Config 값을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(LLMFactory.get_config('deepseek-v3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 제공되는 모델 외에도 커스텀 설정으로 새로운 모델을 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langevaluate.config import ModelConfig\n",
    "\n",
    "# 커스텀 모델 설정 생성\n",
    "custom_config = ModelConfig(\n",
    "    model_name=\"gpt-4-turbo-preview\",\n",
    "    api_base=\"https://api.openai.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    max_tokens=128000,\n",
    "    seed=66,\n",
    "    provider=\"openai\"\n",
    ")\n",
    "\n",
    "# 커스텀 설정으로 모델 생성\n",
    "custom_llm = LLMFactory.create_llm(custom_config, temperature=0.7, rpm=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 로컬 모델 설정 사용\n",
    "\n",
    "CustomModelConfig를 사용하여 LocalGPU를 사용하여 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langevaluate.config import LocalModelConfig\n",
    "\n",
    "local_config = LocalModelConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",  # 사용할 모델 이름 (Qwen2.5-7B-Instruct-1M)\n",
    "    port=30000,  # 서버가 실행될 포트 번호 (30000번 포트에서 대기)\n",
    "    max_tokens=4000,  # input 토큰 수 + output 토큰 수\n",
    "    gpus=\"0\",  # 사용할 GPU ID (GPU 1번과 2번 사용)\n",
    "    dp=1,  # 데이터 병렬 처리 (Data Parallelism) 활성화 (2개 GPU 사용), token output speed 증가\n",
    "    tp=1,   # 텐서 병렬 처리 (Tensor Parallelism) 활성화 (2개 GPU 사용), GPU VRAM 크기가 모자르다면 더 키워야함.\n",
    "    max_running_request=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting llm server boot\n",
      "WARNING 02-04 06:20:25 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "[2025-02-04 06:20:31] server_args=ServerArgs(model_path='Qwen/Qwen2.5-1.5B-Instruct', tokenizer_path='Qwen/Qwen2.5-1.5B-Instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2.5-1.5B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=30000, mem_fraction_static=0.9, max_running_requests=1024, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=66, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\n",
      "WARNING 02-04 06:20:36 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "WARNING 02-04 06:20:36 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "[2025-02-04 06:20:44 TP0] Init torch distributed begin.\n",
      "[W204 06:20:44.223751281 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:30299 (errno: 97 - Address family not supported by protocol).\n",
      "[2025-02-04 06:20:44 TP0] Load weight begin. avail mem=47.17 GB\n",
      "[2025-02-04 06:20:46 TP0] Using model weights format ['*.safetensors']\n",
      "[2025-02-04 06:20:46 TP0] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.84it/s]\n",
      "\n",
      "[2025-02-04 06:20:47 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=44.10 GB\n",
      "[2025-02-04 06:20:47 TP0] KV Cache is allocated. K size: 19.69 GB, V size: 19.69 GB.\n",
      "[2025-02-04 06:20:47 TP0] Memory pool end. avail mem=4.49 GB\n",
      "[2025-02-04 06:20:47 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:13<00:00,  1.72it/s]\n",
      "[2025-02-04 06:21:01 TP0] Capture cuda graph end. Time elapsed: 13.40 s\n",
      "[2025-02-04 06:21:01 TP0] max_total_num_tokens=1474797, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=1024, context_len=32768\n",
      "[2025-02-04 06:21:02] INFO:     Started server process [87426]\n",
      "[2025-02-04 06:21:02] INFO:     Waiting for application startup.\n",
      "[2025-02-04 06:21:02] INFO:     Application startup complete.\n",
      "[2025-02-04 06:21:02] INFO:     Uvicorn running on http://0.0.0.0:30000 (Press CTRL+C to quit)\n",
      "[2025-02-04 06:21:02] INFO:     127.0.0.1:52308 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-02-04 06:21:02] INFO:     127.0.0.1:52314 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-02-04 06:21:02 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-04 06:21:07] INFO:     127.0.0.1:52316 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-04 06:21:07] The server is fired up and ready to roll!\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "# localllm은 서버를 local에서 실행시키기 때문에 부팅되는 시간이 존재합니다.\n",
    "local_llm = LLMFactory.create_llm(local_config, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-04 06:21:07 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-04 06:21:08 TP0] Decode batch. #running-req: 1, #token: 68, token usage: 0.00, gen throughput (token/s): 5.87, #queue-req: 0\n",
      "[2025-02-04 06:21:08] INFO:     127.0.0.1:52324 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm Qwen, a large language model created by Alibaba Cloud. I'm here to help you with information and answer any questions you might have. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 35, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen2.5-1.5B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4a8d3093-8153-4cb3-8f0d-dcf6ed7eeaec-0', usage_metadata={'input_tokens': 35, 'output_tokens': 40, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await local_llm.ainvoke(\"hi. who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서버는 shutdown을 이용하여 끌 수 있습니다.\n",
    "local_llm.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 서버 커스텀 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = ModelConfig(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    api_base=\"http://exaone_3.5_32b:8000/v1\",\n",
    "    api_key='EMPTY',\n",
    "    max_tokens=20000,\n",
    "    seed=66,\n",
    "    provider=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# localllm은 서버를 local에서 실행시키기 때문에 부팅되는 시간이 존재합니다.\n",
    "custom_llm = LLMFactory.create_llm(local_config, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. request per minute 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_llm = LLMFactory.create_llm('gpt-4o-mini', temperature=0.7, rpm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-16a9221d-ccdb-4537-a90b-0a3bc8f2f420-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 9, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_3267753c5d', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe241ad7-b207-4149-a3ee-9831671cefd0-0', usage_metadata={'input_tokens': 9, 'output_tokens': 11, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-c5113c8c-dc31-4ce9-8c2f-83b7eee9db29-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Hello! It seems like you might have typed something by mistake. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 8, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-98646bcc-6c9e-492d-9593-bae7c363513d-0', usage_metadata={'input_tokens': 8, 'output_tokens': 21, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await limit_llm.abatch(['hi', '안녕', 'hello', 'jj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 사용 예시\n",
    "\n",
    "생성된 모델을 사용하여 텍스트를 생성하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPT-4 Response ===\n",
      "만성콩팥병(Chronic Kidney Disease, CKD)은 콩팥(신장)의 기능이 점진적으로 저하되는 질환을 말합니다. 이 병은 여러 가지 원인에 의해 발생할 수 있으며, 특히 당뇨병, 고혈압, 그리고 심혈관 질환과 같은 만성 질환이 주요 원인입니다. 만성콩팥병은 일반적으로 3개월 이상 지속되며, 신장의 기능이 악화됨에 따라 여러 가지 합병증이 발생할 수 있습니다.\n",
      "\n",
      "### 만성콩팥병의 단계\n",
      "만성콩팥병은 신장 기능의 정도에 따라 5단계로 나눌 수 있습니다:\n",
      "1. **1단계**: 신장 기능이 정상이나, 신장 손상이 존재하는 경우.\n",
      "2. **2단계**: 경미한 신장 기능 저하 (GFR 60-89 mL/min).\n",
      "3. **3단계**: 중간 정도의 신장 기능 저하 (GFR 30-59 mL/min). 이 단계는 다시 3a(45-59)와 3b(30-44)로 나눌 수 있습니다.\n",
      "4. **4단계**: 심각한 신장 기능 저하 (GFR 15-29 mL/min).\n",
      "5. **5단계**: 신부전 (GFR < 15 mL/min)으로, 이 단계에서는 투석이나 신장 이식이 필요할 수 있습니다.\n",
      "\n",
      "### 증상\n",
      "만성콩팥병은 초기에는 증상이 거의 없거나 경미할 수 있습니다. 하지만 병이 진행됨에 따라 다음과 같은 증상이 나타날 수 있습니다:\n",
      "- 피로감과 무기력\n",
      "- 부종 (특히 발과 발목)\n",
      "- 고혈압\n",
      "- 소변의 변화 (양, 색, 냄새)\n",
      "- 식욕 감소와 체중 감소\n",
      "- 메스꺼움 및 구토\n",
      "- 가려움증\n",
      "\n",
      "### 진단\n",
      "만성콩팥병은 혈액 검사, 소변 검사, 그리고 영상 검사 등을 통해 진단됩니다. 특히 혈액에서 크레아티닌 수치를 측정하여 신장 기능을 평가하며, GFR(사구체 여과율)을 계산하여 병의 단계를 정합니다.\n",
      "\n",
      "### 치료\n",
      "만성콩팥병의 치료는 주로 원인 질환의 관리와 신장 기능 저하의 진행을 늦추는 데 중점을 둡니다. 일반적인 치료법은 다음과 같습니다:\n",
      "- 혈압 조절 (항고혈압제 사용)\n",
      "- 당뇨 관리\n",
      "- 식이요법 (단백질, 나트륨, 칼륨 섭취 조절)\n",
      "- 필요한 경우 약물 치료\n",
      "- 병이 진행된 경우 투석이나 신장 이식 고려\n",
      "\n",
      "### 예방\n",
      "만성콩팥병을 예방하기 위해서는 건강한 생활 습관을 유지하는 것이 중요합니다. 충분한 수분 섭취, 규칙적인 운동, 적절한 식사, 그리고 정기적인 건강 검진이 도움이 됩니다.\n",
      "\n",
      "만성콩팥병은 심각한 합병증을 초래할 수 있는 질환이므로, 조기에 발견하고 적절한 관리를 하는 것이 중요합니다.\n",
      "\n",
      "=== DeepSeek Response ===\n",
      "만성콩팥병(만성 신장병, Chronic Kidney Disease, CKD)은 신장 기능이 점진적으로 감소하는 질환을 말합니다. 신장은 우리 몸에서 노폐물을 걸러내고, 체액과 전해질 균형을 유지하며, 혈압 조절, 적혈구 생성, 뼈 건강 유지 등 다양한 중요한 기능을 수행합니다. 만성콩팥병은 이러한 신장 기능이 서서히 저하되어 결국 신장이 제 역할을 하지 못하게 되는 상태를 의미합니다.\n",
      "\n",
      "### 주요 원인\n",
      "1. **당뇨병**: 가장 흔한 원인으로, 고혈당이 신장의 혈관과 필터를 손상시킵니다.\n",
      "2. **고혈압**: 고혈압은 신장의 혈관에 부담을 주어 손상을 초래할 수 있습니다.\n",
      "3. **신장염**: 신장의 염증성 질환으로, 면역체계의 이상이 원인일 수 있습니다.\n",
      "4. **다낭성 신장병**: 유전적 요인으로 신장에 여러 개의 낭종이 생기는 질환.\n",
      "5. **약물 및 독소**: 장기간의 진통제 사용, 중금속 노출 등이 신장 손상을 유발할 수 있습니다.\n",
      "6. **기타**: 비만, 흡연, 고령, 심혈관 질환 등도 위험 요인입니다.\n",
      "\n",
      "### 증상\n",
      "초기에는 특별한 증상이 없을 수 있지만, 질환이 진행되면 다음과 같은 증상이 나타날 수 있습니다:\n",
      "- 피로감\n",
      "- 소변량 변화 (소변량 감소 또는 증가)\n",
      "- 부기 (특히 다리, 발, 얼굴)\n",
      "- 호흡 곤란\n",
      "- 식욕 감소, 메스꺼움, 구토\n",
      "- 피부 가려움증\n",
      "- 근육 경련 및 쇠약감\n",
      "- 수면 장애\n",
      "\n",
      "### 진단\n",
      "- **혈액 검사**: 크레아티닌 수치를 측정하여 신장 기능을 평가합니다.\n",
      "- **소변 검사**: 단백뇨나 혈뇨 여부를 확인합니다.\n",
      "- **영상 검사**: 초음파, CT, MRI 등을 통해 신장의 구조적 이상을 확인합니다.\n",
      "- **신장 생검**: 필요한 경우 조직 검사를 통해 원인을 파악합니다.\n",
      "\n",
      "### 치료\n",
      "만성콩팥병은 완치가 어렵지만, 진행을 늦추고 증상을 관리하는 것이 목표입니다.\n",
      "1. **원인 질환 관리**: 당뇨병, 고혈압 등을 철저히 관리합니다.\n",
      "2. **약물 치료**: 혈압 조절, 단백뇨 감소, 빈혈 치료 등을 위한 약물을 사용합니다.\n",
      "3. **식이 조절**: 단백질, 나트륨, 칼륨, 인 등의 섭취를 제한합니다.\n",
      "4. **생활습관 개선**: 금연, 체중 관리, 규칙적인 운동이 중요합니다.\n",
      "5. **신대체 요법**: 말기 신부전 단계에서는 투석(혈액 투석, 복막 투석)이나 신장 이식이 필요할 수 있습니다.\n",
      "\n",
      "### 예방\n",
      "- 건강한 식습관 유지\n",
      "- 정기적인 건강 검진\n",
      "- 당뇨병 및 고혈압 관리\n",
      "- 약물 남용 피하기\n",
      "- 충분한 수분 섭취\n",
      "\n",
      "만성콩팥병은 조기 발견과 적절한 관리가 중요하므로, 위험 요인이 있다면 정기적으로 검진을 받는 것이 좋습니다.\n",
      "\n",
      "=== Naver Response ===\n",
      "만성 콩팥병은 3개월 이상 신장이 손상되어 있거나, 신장 기능 감소가 지속적으로 나타나는 질환을 말합니다. 만성 콩팥병은 다양한 원인에 의해 발생할 수 있으며, 그 중에서도 당뇨병, 고혈압, 사구체신염 등이 주요 원인으로 알려져 있습니다.\n",
      "\n",
      "만성 콩팥병의 증상은 다음과 같습니다.\n",
      "\n",
      "1. 소변량 감소: 소변량이 감소하여 노폐물이 체내에 쌓이게 됩니다.\n",
      "2. 부종: 몸이 붓고 체중이 증가합니다.\n",
      "3. 혈압 상승: 혈압이 상승하여 고혈압이 발생할 수 있습니다.\n",
      "4. 피로감: 피로감이 심해지며, 집중력이 저하됩니다.\n",
      "5. 식욕 부진: 식욕이 저하되며, 구토나 설사 등의 증상이 나타날 수 있습니다.\n",
      "6. 피부 건조: 피부가 건조해지고 가려움증이 생길 수 있습니다.\n",
      "7. 빈혈: 혈액 내 적혈구 수치가 감소하여 빈혈이 발생할 수 있습니다.\n",
      "8. 감각 이상: 손발이 저리고 감각이 둔해지는 등의 증상이 나타날 수 있습니다.\n",
      "\n",
      "만성 콩팥병은 치료가 어려운 질환 중 하나 입니다. 하지만 조기 발견과 적절한 치료를 통해 병의 진행을 늦추고, 합병증을 예방할 수 있습니다. 따라서 정기적인 건강검진을 통해 만성 콩팥병을 조기에 발견하고, 적절한 치료를 받는 것이 중요합니다. 또, 생활습관 개선도 중요한데, 저염식 식단을 유지하고, 단백질과 칼륨 섭취를 제한하며, 담배와 술을 피하는 것이 좋습니다. 그리고 운동을 통해 체중을 조절하고, 혈압과 혈당을 관리하는 것도 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# 텍스트 생성 예시\n",
    "messages = [\n",
    "    HumanMessage(content=\"만성콩팥병에 대해 설명해주세요.\")\n",
    "]\n",
    "\n",
    "# 각 모델로 응답 생성\n",
    "gpt4_response = gpt4_llm.invoke(messages)\n",
    "deepseek_response = deepseek_llm.invoke(messages)\n",
    "naver_response = naver_llm.invoke(messages)\n",
    "\n",
    "print(\"=== GPT-4 Response ===\")\n",
    "print(gpt4_response.content)\n",
    "print(\"\\n=== DeepSeek Response ===\")\n",
    "print(deepseek_response.content)\n",
    "print(\"\\n=== Naver Response ===\")\n",
    "print(naver_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이 튜토리얼에서는 LLMFactory를 사용하여:\n",
    "1. 다양한 LLM 제공업체의 모델을 생성하고\n",
    "2. 커스텀 모델 설정을 만들어 사용하며\n",
    "3. 생성된 모델로 텍스트를 생성하고\n",
    "4. 에러 상황을 처리하는 방법을 배웠습니다.\n",
    "\n",
    "LLMFactory는 여러 LLM 제공업체의 모델을 일관된 인터페이스로 사용할 수 있게 해주며,\n",
    "중앙 집중식 설정 관리를 통해 모델 설정을 효율적으로 관리할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
