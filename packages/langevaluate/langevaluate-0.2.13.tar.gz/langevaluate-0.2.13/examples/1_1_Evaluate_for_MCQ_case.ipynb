{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCQ (Multiple Choice Question) 평가 튜토리얼\n",
    "\n",
    "이 튜토리얼에서는 LLM을 사용하여 객관식 문제를 평가하는 방법을 배워보겠습니다.\n",
    "\n",
    "## 주요 기능\n",
    "- 객관식 문제의 테스트 케이스 생성\n",
    "- LLM을 사용한 문제 해결\n",
    "- 다국어 지원 (한국어/영어)\n",
    "- 비동기 실행 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langevaluate.llmfactory import LLMFactory\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCQ 테스트 케이스 정의\n",
    "\n",
    "## LLMTestCase 클래스 정의\n",
    "LLMTestCase 클래스는 다음과 같은 필드로 구성되어 있습니다. 필드의 의미는 다음과 같습니다:\n",
    "- input: LLM에 입력할 질문\n",
    "- choices: 선택 가능한 답변들의 리스트\n",
    "- expected_output: 정답 (인덱스 또는 문자열)\n",
    "- output: LLM이 실제로 출력한 답변 (선택사항)\n",
    "- reasoning: LLM의 답변 도출 과정 (선택사항)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 테스트 케이스 생성 예시\n",
    "간단한 퀴즈 문제를 만들어 MCQTestCase를 사용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일반상식 문제:\n",
      "질문: 프랑스의 수도는 어디입니까?\n",
      "선택지: ['런던', '파리', '베를린', '마드리드']\n",
      "정답 인덱스: B\n",
      "\n",
      "의료 문제:\n",
      "질문: 임신 22주차인 23세 임산부가 배뇨 시 통증을 호소합니다. 1일 전부터 시작되었으며 수분 섭취 증가와 크랜베리 추출물 복용에도 불구하고 악화되고 있습니다. 체온 36.5°C, 혈압 122/77mmHg, 맥박 80회/분, 호흡수 19회/분, 산소포화도 98%입니다. 신체검사상 척추각 압통은 없으며 임신한 자궁이 관찰됩니다. 가장 적절한 치료는 무엇입니까?\n",
      "선택지: ['Ampicillin', 'Ceftriaxone', 'Ciprofloxacin', 'Nitrofurantoin']\n",
      "정답: A\n",
      "추론: A\n"
     ]
    }
   ],
   "source": [
    "from langmetrics.llmtestcase import LLMTestCase\n",
    "# 테스트 케이스 예시 생성\n",
    "simple_testcase = LLMTestCase(\n",
    "    input=\"프랑스의 수도는 어디입니까?\",\n",
    "    choices=[\"런던\", \"파리\", \"베를린\", \"마드리드\"],\n",
    "    expected_output=\"B\"\n",
    ")\n",
    "\n",
    "medical_testcase = LLMTestCase(\n",
    "    input=\"임신 22주차인 23세 임산부가 배뇨 시 통증을 호소합니다. 1일 전부터 시작되었으며 수분 섭취 증가와 크랜베리 추출물 복용에도 불구하고 악화되고 있습니다. 체온 36.5°C, 혈압 122/77mmHg, 맥박 80회/분, 호흡수 19회/분, 산소포화도 98%입니다. 신체검사상 척추각 압통은 없으며 임신한 자궁이 관찰됩니다. 가장 적절한 치료는 무엇입니까?\",\n",
    "    choices=[\"Ampicillin\", \"Ceftriaxone\", \"Ciprofloxacin\", \"Nitrofurantoin\"],\n",
    "    expected_output=\"A\",\n",
    "    output=\"A\"\n",
    ")\n",
    "\n",
    "# 테스트 케이스 출력\n",
    "print(\"일반상식 문제:\")\n",
    "print(f\"질문: {simple_testcase.input}\")\n",
    "print(f\"선택지: {simple_testcase.choices}\")\n",
    "print(f\"정답 인덱스: {simple_testcase.expected_output}\")\n",
    "print(\"\\n의료 문제:\")\n",
    "print(f\"질문: {medical_testcase.input}\")\n",
    "print(f\"선택지: {medical_testcase.choices}\")\n",
    "print(f\"정답: {medical_testcase.expected_output}\")\n",
    "print(f\"추론: {medical_testcase.output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM을 이용하여 MCQ 평가\n",
    "\n",
    "LLM을 설정하고 MCQ 평가를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델 생성\n",
    "openai_llm = LLMFactory.create_llm('gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가를 위하여 MCQMetric class를 호출하도록 하겠습니다. MCQMetric은 LLM(Large Language Model)의 객관식 문제 답변을 평가하기 위한 메트릭 클래스입니다.\n",
    "\n",
    "MCQMetric을 사용하기 위해서는 먼저 인스턴스를 생성해야 합니다. 기본적인 설정 방법은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 매개변수 :\n",
    "\n",
    "`template_language`: 템플릿 언어 선택 ('ko' 또는 'en')\n",
    "\n",
    "`generate_template_type`: 답변 생성 방식 ('reasoning': 풀이 과정 포함, 'only_answer': 답만 생성)\n",
    "\n",
    "`verbose_mode`: 상세 로그 출력 여부 (기본값: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmetrics.metrics import MCQMetric\n",
    "metric = MCQMetric(\n",
    "    output_model=openai_llm,\n",
    "    template_language='en',  # 'ko' 또는 'en'\n",
    "    output_template_type='reasoning',  # 'reasoning' 또는 'only_answer'\n",
    "    verbose_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await metric.ameasure(simple_testcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input에 들어가 simple_testcase는 정답값과 함께 출력이 됨을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMTestCase(\n",
       "  input='프랑스의 수도는 어디입니까?',\n",
       "  output='{\\n    \"reasoning\": \"프랑스의 수도는 파리입니다. 런던은 영국의 수도이고, 베를린은 독일의 수도이며, 마드리드는 스페인의 수도입니다. 따라서 정답은 파리입니다. So the answer is B.\",\\n    \"answer\": \"B\"\\n}',\n",
       "  expected_output='B',\n",
       "  choices=['런던', '파리', '베를린', '마드리드']\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_testcase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMResult(\n",
      "  input='프랑스의 수도는 어디입니까?',\n",
      "  output='{\\n    \"reasoning\": \"프랑스의 수도는 파리입니다. 런던은 영국의 수도이고, 베를린은 독일의 수도이며, 마드리드는 스페인의 수도입니다. 따라서 정답은 파리입니다. So the answer is B.\",\\n    \"answer\": \"B\"\\n}',\n",
      "  expected_output='B',\n",
      "  choices=['런던', '파리', '베를린', '마드리드'],\n",
      "  score=1,\n",
      "  metadata={'output_model_name': 'gpt-4o-mini', 'token_usage': {'prompt_tokens': 151, 'completion_tokens': 70, 'total_tokens': 221}}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCQResult는 to_dict()와 from_dict() method를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = result.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '프랑스의 수도는 어디입니까?', 'output': '{\\n    \"reasoning\": \"프랑스의 수도는 파리입니다. 런던은 영국의 수도이고, 베를린은 독일의 수도이며, 마드리드는 스페인의 수도입니다. 따라서 정답은 파리입니다. So the answer is B.\",\\n    \"answer\": \"B\"\\n}', 'scoring_model_output': None, 'expected_output': 'B', 'context': None, 'retrieval_context': None, 'choices': ['런던', '파리', '베를린', '마드리드'], 'score': 1, 'metadata': {'output_model_name': 'gpt-4o-mini', 'token_usage': {'prompt_tokens': 151, 'completion_tokens': 70, 'total_tokens': 221}}}\n"
     ]
    }
   ],
   "source": [
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(\n",
       "  input='프랑스의 수도는 어디입니까?',\n",
       "  output='{\\n    \"reasoning\": \"프랑스의 수도는 파리입니다. 런던은 영국의 수도이고, 베를린은 독일의 수도이며, 마드리드는 스페인의 수도입니다. 따라서 정답은 파리입니다. So the answer is B.\",\\n    \"answer\": \"B\"\\n}',\n",
       "  expected_output='B',\n",
       "  choices=['런던', '파리', '베를린', '마드리드'],\n",
       "  score=1,\n",
       "  metadata={'output_model_name': 'gpt-4o-mini', 'token_usage': {'prompt_tokens': 151, 'completion_tokens': 70, 'total_tokens': 221}}\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.from_dict(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MCQ 템플릿 설정\n",
    "\n",
    "\n",
    "langmetrics의 Metric은 custom template을 이용할 수 있고, 이 template은 langchain의 Template을 이용하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langmetrics.metrics import MCQTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메시지를 설정하되, 반드시 JSON을 output을 출력으로 뱉는 template을 설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "    AIMessagePromptTemplate.from_template('당신은 의료 전문가입니다.'),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"다음의 객관식 문제를 풀어주세요.\n",
    "추론 과정을 설명하고 정답은 알파벳(A, B, C, D 등)으로만 답해주세요.\n",
    "\n",
    "**\n",
    "중요 : 반드시 JSON 형식으로만 답변해주세요. 'answer' 키에는 정답을 작성해주세요.\n",
    "JSON 예시:\n",
    "{{\n",
    "\"answer\": \"<정답>\"\n",
    "}}\n",
    "**\n",
    "\n",
    "문제:\n",
    "{question}\n",
    "\n",
    "보기:\n",
    "{choices}\n",
    "\n",
    "JSON:\"\"\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCQTemplate 클래스는 정의된 프롬프트를 사용하여 객관식 문제 템플릿을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_template = MCQTemplate(prompt_for_answer=evaluation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MCQMetric(\n",
    "    openai_llm, \n",
    "    verbose_mode=True, \n",
    "    template_language='en',\n",
    "    template=answer_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 testcase에 문제가 없습니다!\n",
      "LLMTestCase(\n",
      "  input='프랑스의 수도는 어디입니까?',\n",
      "  output='{\\n    \"reasoning\": \"프랑스의 수도는 파리입니다. 런던은 영국의 수도이고, 베를린은 독일의 수도이며, 마드리드는 스페인의 수도입니다. 따라서 정답은 파리입니다. So the answer is B.\",\\n    \"answer\": \"B\"\\n}',\n",
      "  expected_output='B',\n",
      "  choices=['런던', '파리', '베를린', '마드리드']\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(\n",
       "  input='프랑스의 수도는 어디입니까?',\n",
       "  output='{\\n    \"reasoning\": \"프랑스의 수도는 파리입니다. 런던은 영국의 수도이고, 베를린은 독일의 수도이며, 마드리드는 스페인의 수도입니다. 따라서 정답은 파리입니다. So the answer is B.\",\\n    \"answer\": \"B\"\\n}',\n",
       "  expected_output='B',\n",
       "  choices=['런던', '파리', '베를린', '마드리드'],\n",
       "  score=1,\n",
       "  metadata={'output_model_name': 'gpt-4o-mini', 'token_usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await metric.ameasure(simple_testcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
