Metadata-Version: 2.4
Name: ai_llama_module
Version: 0.1.6
Summary: Simple and lightweight Python wrapper for Llama AI API
Author-email: Isaac Gallienne <100033398@mvla.net>
License-Expression: MIT AND Apache-2.0
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.28.0
Requires-Dist: python-dotenv>=0.21.0

# AI Llama Module

ai_llama_module is a lightweight Python wrapper for the Llama AI API, allowing you to easily send messages and receive AI-generated responses.

⸻

Installation

Install via pip:

`` pip install ai_llama_module ``

Requirements:
	•	Python 3.9+
	•	requests
	•	python-dotenv

Make sure you have an environment variable llama_ai set with your Llama AI API key, e.g., in a .env file:

You can also pass an API key through with the variable api_key when calling the function, otherwise it will default to the enviornment variable.

llama_ai = your_api_key_here


⸻

Usage

```from ai_llama_module import call_ai```

# Single message
```
response = call_ai("Hello, Llama!")

print(response)
```
or you can use
```
response = call_ai("Hello World!", model, api_key = "YOUR KEY HERE")

print(response)
```
# Multiple messages
```
messages = [
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi there! How can I help?"}
]

response = call_ai(messages)

print(response)
```
⸻

Features:

	•	Supports single and multi-message inputs
	•	Automatically reads API key from environment variables
	•	Simple and easy-to-use interface
	•	You can specify the model used with the variable 'model' which defualts to Llama-4-Maverick-17B-128E-Instruct-FP8

⸻

Dependencies

	•	requests
	•	python-dotenv

⸻

License

MIT License – see LICENSE for details.
