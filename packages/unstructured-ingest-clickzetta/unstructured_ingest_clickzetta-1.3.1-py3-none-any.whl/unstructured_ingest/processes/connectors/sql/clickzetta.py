import json
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any, Generator, Optional, List, Dict, Tuple, Union

import numpy as np
import pandas as pd
from pydantic import Field, Secret

from unstructured_ingest.utils.data_prep import split_dataframe
from unstructured_ingest.utils.dep_check import requires_dependencies
from unstructured_ingest.data_types.file_data import FileData
from unstructured_ingest.logger import logger
from unstructured_ingest.processes.connector_registry import (
    DestinationRegistryEntry,
    SourceRegistryEntry,
)
from unstructured_ingest.processes.connectors.sql.sql import (
    _DATE_COLUMNS,
    SQLAccessConfig,
    SqlBatchFileData,
    SQLConnectionConfig,
    SQLDownloader,
    SQLDownloaderConfig,
    SQLIndexer,
    SQLIndexerConfig,
    SQLUploader,
    SQLUploaderConfig,
    SQLUploadStager,
    SQLUploadStagerConfig,
    parse_date_string,
)

if TYPE_CHECKING:
    from clickzetta.connector import connect
    
from clickzetta.zettapark.session import Session
from clickzetta.connector.sqlalchemy.datatype import VECTOR
from sqlalchemy.types import BIGINT
import clickzetta.zettapark.types as T

CONNECTOR_TYPE = "clickzetta"

_ARRAY_COLUMNS = (
    "embeddings",
    "languages",
    "link_urls",
    "link_texts",
    "sent_from",
    "sent_to",
    "emphasized_text_contents",
    "emphasized_text_tags",
)

# ClickZettaè¡¨çš„æ‰€æœ‰åˆ—å®šä¹‰ï¼ˆ33åˆ—ï¼‰
_REQUIRED_COLUMNS = [
    "id", "record_locator", "type", "record_id", "element_id", "filetype", "file_directory",
    "filename", "last_modified", "languages", "page_number", "text", "embeddings", "parent_id",
    "is_continuation", "orig_elements", "element_type", "coordinates", "link_texts", "link_urls",
    "email_message_id", "sent_from", "sent_to", "subject", "url", "version", "date_created",
    "date_modified", "date_processed", "text_as_html", "emphasized_text_contents", "emphasized_text_tags",
    "documents_original_source"
]
def _ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¡®ä¿DataFrameåŒ…å«æ‰€æœ‰å¿…éœ€çš„åˆ—ï¼Œå¹¶æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—

    Args:
        df: è¾“å…¥çš„DataFrame

    Returns:
        åŒ…å«æ‰€æœ‰å¿…éœ€åˆ—çš„DataFrame
    """
    # è¡¥é½ç¼ºå¤±åˆ—
    for col in _REQUIRED_COLUMNS:
        if col not in df.columns:
            df[col] = None

    # ä¿è¯åˆ—é¡ºåºä¸€è‡´å¹¶æ¸…ç†NaNå€¼
    return df[_REQUIRED_COLUMNS].copy().replace({np.nan: None})


def generate_df_schema(df: pd.DataFrame) -> T.StructType:
    """
    Generate a schema definition for a DataFrame in the format of T.StructType.

    Args:
        df (pd.DataFrame): The DataFrame for which to generate the schema.

    Returns:
        T.StructType: The schema definition.
    """
    type_mapping = {
        "int64": T.IntegerType(),
        "float64": T.FloatType(),
        "object": T.StringType(),
        "bool": T.BooleanType(),
        "datetime64[ns]": T.TimestampType(),
        "vector('float',512)": T.VectorType('float',512),
        "vector('float',768)": T.VectorType('float',768),
        "vector('float',1024)": T.VectorType('float',1024),
        "vector('float',1536)": T.VectorType('float',1536),
        "bigint": T.LongType(),
        "string": T.StringType(),
        "array": T.ArrayType()
    }

    fields = []
    for column_name, dtype in df.dtypes.items():
        field_type = type_mapping.get(str(dtype), T.StringType())  # Default to StringType if type is unknown
        fields.append(T.StructField(column_name, field_type))

    return T.StructType(fields)

class ClickzettaAccessConfig(SQLAccessConfig):
    password: Optional[str] = Field(default=None, description="DB password")


class ClickzettaConnectionConfig(SQLConnectionConfig):
    schema: str = Field(default=None, description="Schema name for Clickzetta.")
    access_config: Secret[ClickzettaAccessConfig] = Field(
        default=ClickzettaAccessConfig(), validate_default=True
    )
    service: str = Field(
        default=None,
        description="Your service url. "
        "Your service url.",
    )
    username: Optional[str] = Field(default=None, description="username")
    instance: Optional[str] = Field(default=None, description="instance id")
    workspace: Optional[str] = Field(default=None, description="workspace/database name")
    vcluster: str = Field(
        default=None,
        description="vcluster name.",
    )
    connector_type: str = Field(default=CONNECTOR_TYPE, init=False)
    # Pydantic v2: ä½¿ç”¨ model_config æ›¿ä»£ class Config ä»¥æ¶ˆé™¤å¼ƒç”¨è­¦å‘Š
    model_config = {
        "populate_by_name": True
    }

    @contextmanager
    @requires_dependencies(["clickzetta"], extras="clickzetta")
    def get_session(self) -> Generator["Session", None, None]:
        from clickzetta.zettapark.session import Session

        connect_kwargs = {
            "service": self.service,
            "username": self.username,
            "instance": self.instance,
            "workspace": self.workspace,
            "vcluster": self.vcluster,
            "schema": self.schema,
            "password": self.access_config.get_secret_value().password,
        }
        active_kwargs = {k: v for k, v in connect_kwargs.items() if v is not None}
        session = None  # é˜²æ­¢finallyæŠ¥é”™
        try:
            session = Session.builder.configs(active_kwargs).create()
            session.sql("select 'Initialize session to the Clickzetta by unstructured ingest Tool';").collect()
            yield session
        finally:
            if session:
                session.close()

    @contextmanager
    @requires_dependencies(["clickzetta"], extras="clickzetta")
    def get_connection(self) -> Generator[Any, None, None]:
        from clickzetta.connector import connect
        # æ˜ç¡®æ˜ å°„å­—æ®µï¼Œé¿å… model_dump å¯èƒ½çš„åµŒå¥—æˆ–å¤§å°å†™é—®é¢˜
        connect_kwargs = {
            "service": self.service,
            "username": self.username,
            "instance": self.instance,
            "workspace": self.workspace,
            "vcluster": self.vcluster,
            "schema": self.schema,
            "password": self.access_config.get_secret_value().password,
            "paramstyle": "qmark",
        }
        # è‡ªåŠ¨ strip æ‰€æœ‰å­—ç¬¦ä¸²å‚æ•°ï¼Œå¹¶æ‰“å°ç±»å‹å’Œå€¼
        for k, v in connect_kwargs.items():
            if isinstance(v, str):
                connect_kwargs[k] = v.strip()
        active_kwargs = {k: v for k, v in connect_kwargs.items() if v is not None}
        connection = connect(**active_kwargs)
        try:
            yield connection
        finally:
            connection.commit()
            connection.close()

    @contextmanager
    def get_cursor(self) -> Generator[Any, None, None]:
        with self.get_connection() as connection:
            cursor = connection.cursor()
            try:
                yield cursor
            finally:
                cursor.close()


class ClickzettaIndexerConfig(SQLIndexerConfig):
    pass


@dataclass
class ClickzettaIndexer(SQLIndexer):
    connection_config: ClickzettaConnectionConfig
    index_config: ClickzettaIndexerConfig
    connector_type: str = CONNECTOR_TYPE


class ClickzettaDownloaderConfig(SQLDownloaderConfig):
    pass


@dataclass
class ClickzettaDownloader(SQLDownloader):
    connection_config: ClickzettaConnectionConfig
    download_config: ClickzettaDownloaderConfig
    connector_type: str = CONNECTOR_TYPE
    # ä¿®æ­£ï¼šClickZetta ä¸æ”¯æŒ IN (?, ?, ?) å ä½ç¬¦ï¼Œéœ€ç›´æ¥æ‹¼æ¥ id åˆ—è¡¨
    @requires_dependencies(["clickzetta"], extras="clickzetta")
    def query_db(self, file_data: SqlBatchFileData) -> tuple[list[dict], list[str]]:
        table_name = file_data.additional_metadata.table_name
        id_column = file_data.additional_metadata.id_column
        ids = [item.identifier for item in file_data.batch_items]

        # ç›´æ¥æ‹¼æ¥ id åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ SQL è¯­æ³•é”™è¯¯
        id_list_str = ",".join([f"'{str(i)}'" for i in ids]) if ids else "''"
        query = f"SELECT {','.join(self.download_config.fields) if self.download_config.fields else '*'} FROM {table_name} WHERE {id_column} IN ({id_list_str})"
        with self.connection_config.get_session() as session:
            result = session.sql(query).to_pandas()
            rows = result.to_dict(orient="records")
            columns = list(rows[0].keys()) if rows else []
            return rows, columns


class ClickzettaUploadStagerConfig(SQLUploadStagerConfig):
    pass


class ClickzettaUploadStager(SQLUploadStager):
    upload_stager_config: ClickzettaUploadStagerConfig


class ClickzettaUploaderConfig(SQLUploaderConfig):
    documents_original_source: str = Field(default="unknown", description="Source of the documents")



@dataclass
class ClickzettaUploader(SQLUploader):
    upload_config: ClickzettaUploaderConfig = field(default_factory=ClickzettaUploaderConfig)
    connection_config: ClickzettaConnectionConfig
    connector_type: str = CONNECTOR_TYPE
    values_delimiter: str = "?"

    def prepare_data(
        self, columns: List[str], data: Tuple[Tuple[Any, ...], ...]
    ) -> List[Tuple[Any, ...]]:
        output = []
        for row in data:
            parsed = []
            for column_name, value in zip(columns, row):
                if column_name in _DATE_COLUMNS:
                    if value is None or pd.isna(value):
                        parsed.append(None)
                    else:
                        parsed.append(parse_date_string(value))
                elif column_name in _ARRAY_COLUMNS:
                    if not isinstance(value, list) and (value is None or pd.isna(value)):
                        parsed.append(None)
                    else:
                        parsed.append(json.dumps(value))
                else:
                    parsed.append(value)
            output.append(tuple(parsed))
        return output

    def __post_init__(self):
        # å¦‚æœæ²¡æœ‰è®¾ç½®batch_sizeï¼Œä½¿ç”¨é»˜è®¤å€¼1000
        if not hasattr(self.upload_config, 'batch_size') or self.upload_config.batch_size is None:
            self.upload_config.batch_size = 1000
        self._batch_buffer = []  # æ‰¹é‡ç¼“å†²åŒº
        self._buffer_size = 0

    def is_batch(self) -> bool:
        """å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼"""
        return True

    def run_batch(self, contents: list, **kwargs) -> None:
        """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶çš„æ•°æ®"""
        
        all_data = []
        all_file_data = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®
        for content in contents:
            try:
                from unstructured_ingest.utils.data_prep import get_json_data
                data = get_json_data(path=content.path)
                if data:
                    all_data.extend(data)
                    # ä¸ºæ¯æ¡è®°å½•ä¿å­˜æ–‡ä»¶æ•°æ®å¼•ç”¨
                    all_file_data.extend([content.file_data] * len(data))
            except Exception as e:
                logger.warning(f"Failed to load data from {content.path}: {e}")
                continue
        
        if all_data:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ file_data ä½œä¸ºä»£è¡¨ï¼ˆå› ä¸ºæ‰¹é‡ä¸Šä¼ éœ€è¦ä¸€ä¸ª file_dataï¼‰
            representative_file_data = contents[0].file_data if contents else None

            # ç›´æ¥è°ƒç”¨ä¼˜åŒ–çš„æ‰¹é‡ä¸Šä¼ æ–¹æ³•
            self._upload_data_batch(data=all_data, file_data=representative_file_data)

    def _upload_data_batch(self, data: list[dict], file_data: FileData) -> None:
        """æ‰¹é‡ä¸Šä¼ ï¼Œåªä¿ç•™ç›®æ ‡è¡¨å­—æ®µï¼ˆid/textï¼‰ï¼Œå…¼å®¹ stager è¾“å‡º"""
        import pandas as pd
        import numpy as np
        # åªä¿ç•™ id/text ä¸¤åˆ—ï¼Œå…¼å®¹ stager è¾“å‡º
        filtered_data = []
        for item in data:
            if isinstance(item, dict):
                filtered_data.append({
                    "id": item.get("id"),
                    "text": item.get("text")
                })
        df = pd.DataFrame(filtered_data)
        # è¡¥é½ç¼ºå¤±åˆ—
        for col in ["id", "text"]:
            if col not in df.columns:
                df[col] = None
        df = df[["id", "text"]].copy()
        df = df.replace({np.nan: None})
        df_schema = generate_df_schema(df)
        columns = list(df.columns)
        with self.connection_config.get_session() as session:
            batch_count = 0
            for rows in split_dataframe(df=df, chunk_size=self.upload_config.batch_size):
                batch_count += 1
                batch_size = len(rows)
                values = self.prepare_data(columns, tuple(rows.itertuples(index=False, name=None)))
                values_df = pd.DataFrame(values, columns=columns)
                zetta_df = session.create_dataframe(values_df, schema=df_schema)
                zetta_df.write.mode("append").save_as_table(self.upload_config.table_name)
    
    def _parse_values(self, columns: List[str]) -> str:
        return ",".join([self.values_delimiter for _ in columns])

    def upload_dataframe(self, df: pd.DataFrame, file_data: FileData) -> None:
        """ğŸ”§ æ™ºèƒ½ç¼“å†²ï¼šå°æ‰¹é‡ç«‹å³åˆ·æ–°ï¼Œå¤§æ‰¹é‡ä½¿ç”¨ç¼“å†²"""
        # å°†æ•°æ®æ·»åŠ åˆ°ç¼“å†²åŒº
        self._batch_buffer.append(df)
        self._buffer_size += len(df)
        
        # ğŸ”§ å¼ºåˆ¶ç«‹å³åˆ·æ–°ç­–ç•¥ï¼šç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
        # å¯¹äºä»»ä½•æ•°æ®éƒ½ç«‹å³åˆ·æ–°ï¼Œé¿å…ç¼“å†²åŒºå¯¼è‡´çš„æ•°æ®ä¸¢å¤±é—®é¢˜
        self._flush_buffer()
        
        # æ³¨å†Œatexitå›è°ƒï¼Œç¡®ä¿ç¨‹åºé€€å‡ºæ—¶åˆ·æ–°ç¼“å†²åŒº
        if not hasattr(self, '_atexit_registered'):
            import atexit
            atexit.register(self._safe_flush)
            self._atexit_registered = True
    
    def _flush_buffer(self):
        """åˆ·æ–°ç¼“å†²åŒºï¼Œæ‰¹é‡ä¸Šä¼ æ‰€æœ‰æ•°æ®"""
        if not self._batch_buffer:
            return
            
        try:
            # åˆå¹¶æ‰€æœ‰DataFrame
            combined_df = pd.concat(self._batch_buffer, ignore_index=True)
        except Exception as e:
            logger.error(f"åˆå¹¶DataFrameå¤±è´¥: {e}")
            # å¦‚æœconcatå¤±è´¥ï¼Œå°è¯•é€ä¸ªå¤„ç†
            try:
                for df in self._batch_buffer:
                    self._write_single_df(df)
                self._batch_buffer = []
                self._buffer_size = 0
                return
            except Exception as e2:
                logger.error(f"é€ä¸ªå¤„ç†DataFrameä¹Ÿå¤±è´¥: {e2}")
                return
        
        # è¡¥é½ç¼ºå¤±åˆ—å¹¶ä¿è¯åˆ—é¡ºåºä¸€è‡´
        combined_df = _ensure_required_columns(combined_df)

        df_schema = generate_df_schema(combined_df)
        columns = list(combined_df.columns)

        # ä½¿ç”¨å•ä¸ªsessionå¤„ç†æ‰€æœ‰æ•°æ®
        with self.connection_config.get_session() as session:

            for rows in split_dataframe(df=combined_df, chunk_size=self.upload_config.batch_size):
                values = self.prepare_data(columns, tuple(rows.itertuples(index=False, name=None)))
                values_df = pd.DataFrame(values, columns=columns)

                # åº”ç”¨åˆ—è¡¥é½é€»è¾‘ï¼Œç¡®ä¿æ‰€æœ‰33åˆ—éƒ½å­˜åœ¨
                values_df = _ensure_required_columns(values_df)

                # å°† embeddings åˆ—è½¬æ¢ä¸º vector ç±»å‹
                if "embeddings" in values_df.columns:
                    def to_vector(val):
                        if val is None:
                            return None
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè½¬ä¸ºlist
                        if isinstance(val, str):
                            try:
                                val = json.loads(val)
                            except Exception:
                                return None
                        # è½¬ä¸ºfloatæ•°ç»„
                        return [float(x) for x in val]
                    values_df["embeddings"] = values_df["embeddings"].apply(to_vector)

                # è®¾ç½® documents_original_source åˆ—çš„å€¼
                if "documents_original_source" in values_df.columns:
                    values_df["documents_original_source"] = self.upload_config.documents_original_source

                try:
                    zetta_df = session.create_dataframe(values_df, schema=df_schema)
                    zetta_df.write.mode("append").save_as_table(self.upload_config.table_name)
                except Exception as e:
                    logger.error(f"å†™å…¥å¤±è´¥ - {e}")
                    raise

            # æ¸…ç©ºç¼“å†²åŒº
            self._batch_buffer = []
            self._buffer_size = 0

    def run_data(self, data: list[dict], file_data: FileData, **kwargs) -> None:
        """é‡å†™run_dataæ–¹æ³•ï¼Œç¡®ä¿æœ€ååˆ·æ–°ç¼“å†²åŒº"""
        df = pd.DataFrame(data)
        self.upload_dataframe(df=df, file_data=file_data)
        
        # ç¡®ä¿åœ¨å¤„ç†å®Œæˆååˆ·æ–°ç¼“å†²åŒº
        # è¿™ä¸ªæ–¹æ³•é€šå¸¸æ˜¯å¤„ç†å•ä¸ªæ–‡ä»¶çš„æœ€åæ­¥éª¤
        # æ³¨æ„ï¼šè¿™å¯èƒ½å¯¼è‡´å°æ‰¹æ¬¡ä¸Šä¼ ï¼Œä½†èƒ½ç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
        if hasattr(self, '_batch_buffer') and self._batch_buffer and self._buffer_size < self.upload_config.batch_size:
            # å¦‚æœæ˜¯æœ€åä¸€æ‰¹æ•°æ®ï¼ˆç¼“å†²åŒºæœªæ»¡ï¼‰ï¼Œç­‰å¾…æ›´å¤šæ•°æ®
            # ä½†å¦‚æœç­‰å¾…æ—¶é—´è¿‡é•¿ï¼Œåº”è¯¥åˆ·æ–°
            pass
    
    def finish(self):
        """å®Œæˆä¸Šä¼ ï¼Œåˆ·æ–°æ‰€æœ‰å‰©ä½™çš„ç¼“å†²åŒºæ•°æ®"""
        if hasattr(self, '_batch_buffer') and self._batch_buffer:
            self._flush_buffer()
    
    def _safe_flush(self):
        """å®‰å…¨çš„åˆ·æ–°æ–¹æ³•ï¼Œç”¨äºatexitå›è°ƒ"""
        try:
            if hasattr(self, '_batch_buffer') and self._batch_buffer:
                self._flush_buffer()
        except Exception as e:
            logger.error(f"å®‰å…¨åˆ·æ–°å¤±è´¥: {e}")
    
    def _write_single_df(self, df):
        """å†™å…¥å•ä¸ªDataFrameåˆ°æ•°æ®åº“"""
        
        # è¡¥é½ç¼ºå¤±åˆ—å¹¶ä¿è¯åˆ—é¡ºåºä¸€è‡´
        df = _ensure_required_columns(df)

        df_schema = generate_df_schema(df)
        table_full_name = f"{self.workspace}.{self.db_schema}.{self.table_name}"
        try:
            self.connection.write_pandas(
                df=df,
                table_name=table_full_name,
                if_exists="append",
                df_schema=df_schema
            )
        except Exception as e:
            logger.error(f"å†™å…¥DataFrameå¤±è´¥: {e}")
            raise

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿åˆ·æ–°å‰©ä½™çš„ç¼“å†²åŒºæ•°æ®"""
        if hasattr(self, '_batch_buffer') and self._batch_buffer:
            try:
                self._flush_buffer()
            except Exception as e:
                logger.error(f"ææ„å‡½æ•°ä¸­åˆ·æ–°ç¼“å†²åŒºå¤±è´¥: {e}")



clickzetta_source_entry = SourceRegistryEntry(
    connection_config=ClickzettaConnectionConfig,
    indexer_config=ClickzettaIndexerConfig,
    indexer=ClickzettaIndexer,
    downloader_config=ClickzettaDownloaderConfig,
    downloader=ClickzettaDownloader,
)

clickzetta_destination_entry = DestinationRegistryEntry(
    connection_config=ClickzettaConnectionConfig,
    uploader=ClickzettaUploader,
    uploader_config=ClickzettaUploaderConfig,
    upload_stager=ClickzettaUploadStager,
    upload_stager_config=ClickzettaUploadStagerConfig,
)
