{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Dispytch is a lightweight, async Python framework for event-handling. It\u2019s designed to streamline the development of clean and testable event-driven services.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\ud83e\udde0 Async core \u2013 built for modern Python I/O</li> <li>\ud83d\udd0c FastAPI-style dependency injection \u2013 clean, decoupled handlers</li> <li>\ud83d\udcec Backend-flexible \u2013 with Kafka, RabbitMQ and Redis PubSub out-of-the-box</li> <li>\ud83e\uddfe Pydantic-based validation \u2013 event schemas are validated using pydantic</li> <li>\ud83d\udd01 Built-in retry logic \u2013 configurable, resilient, no boilerplate</li> <li>\u2705 Automatic acknowledgement \u2013 events are acknowledged automatically after successful processing</li> </ul>"},{"location":"#example-emitting-events","title":"\u2728 Example: Emitting Events","text":"<pre><code>import uuid\nfrom datetime import datetime\nfrom pydantic import BaseModel\nfrom dispytch import EventBase\n\n\nclass User(BaseModel):\n    id: str\n    email: str\n    name: str\n\n\nclass UserRegistered(EventBase):\n    __topic__ = \"user_events\"\n    __event_type__ = \"user_registered\"\n\n    user: User\n    timestamp: int\n\n\nasync def example_emit(emitter):\n    await emitter.emit(\n        UserRegistered(\n            user=User(\n                id=str(uuid.uuid4()),\n                email=\"example@mail.com\",\n                name=\"John Doe\",\n            ),\n            timestamp=int(datetime.now().timestamp()),\n        )\n    )\n</code></pre>"},{"location":"#example-handling-events","title":"\u2728 Example: Handling Events","text":"<pre><code>from typing import Annotated\nfrom pydantic import BaseModel\nfrom dispytch import Event, Dependency, HandlerGroup\nfrom service import UserService, get_user_service\n\n\nclass User(BaseModel):\n    id: str\n    email: str\n    name: str\n\n\n# Define event body schema\nclass UserCreatedEvent(BaseModel):\n    user: User\n    timestamp: int\n\n\nuser_events = HandlerGroup()\n\n\n@user_events.handler(topic='user_events', event='user_registered')\nasync def handle_user_registered(\n        event: Event[UserCreatedEvent],\n        user_service: Annotated[UserService, Dependency(get_user_service)]\n):\n    user = event.body.user\n    timestamp = event.body.timestamp\n    print(f\"[User Registered] {user.id} - {user.email} at {timestamp}\")\n    await user_service.do_smth_with_the_user(user)\n</code></pre>"},{"location":"concepts/backend_specific_event_configs/","title":"\u2699\ufe0f Backend-Specific Event Settings","text":"<p>Events often need fine-grained control over how they\u2019re published\u2014things like partitioning, headers, priorities, timestamps, etc. Dispytch supports this via the optional <code>__backend_config__</code> class attribute on any <code>EventBase</code> subclass.</p> <p>This lets you define backend-specific settings inside your event instance in a clean, declarative way.</p>"},{"location":"concepts/backend_specific_event_configs/#what-is-__backend_config__","title":"\ud83e\udde9 What Is <code>__backend_config__</code>?","text":"<p><code>__backend_config__</code> is an optional <code>BaseModel</code> that lets you pass custom options to your producer. Each backend (Kafka, RabbitMQ, Redis, etc.) can define its own config schema.</p>"},{"location":"concepts/backend_specific_event_configs/#example","title":"\ud83d\udd0e Example","text":"<pre><code>class UserCreated(EventBase):\n    __topic__ = \"user_events\"\n    __event_type__ = \"user_created\"\n\n    user: User\n    timestamp: int\n\n\nasync def example_emit(emitter: EventEmitter, user: User):\n    await emitter.emit(\n        UserCreated(\n            user=user,\n            timestamp=int(datetime.now().timestamp()),\n            __backend_config__=KafkaEventConfig(\n                partition_key=user.id,\n            )\n        )\n\n    )\n</code></pre>"},{"location":"concepts/backend_specific_event_configs/#kafkaeventconfig","title":"\ud83e\udeb5 KafkaEventConfig","text":"<p>Use this config to control how events are sent to Kafka.</p> <pre><code>class KafkaEventConfig(BaseModel):\n    partition_key: Optional[Any] = None\n    partition: Optional[int] = None\n    timestamp_ms: Optional[int] = None\n    headers: Optional[dict] = None\n</code></pre>"},{"location":"concepts/backend_specific_event_configs/#rabbitmqeventconfig","title":"\ud83d\udc07 RabbitMQEventConfig","text":"<p>RabbitMQ gives you full control over message delivery via its rich AMQP options.</p> <pre><code>class RabbitMQEventConfig(BaseModel):\n    delivery_mode: int | None = None\n    priority: int | None = None\n    expiration: int | datetime | float | timedelta | None = None\n    headers: dict | None = None\n    content_type: str | None = None\n    ...\n</code></pre> <p>Using this config, you can set AMQP-specific things</p>"},{"location":"concepts/backend_specific_event_configs/#implementing-custom-configs","title":"\ud83d\udd28 Implementing Custom Configs","text":"<p>If you're writing a custom producer (see Writing Custom Producers &amp; Consumers), you can define your own config schema:</p> <pre><code>class MyCustomConfig(BaseModel):\n    foo: str\n    retries: int = 3\n</code></pre> <p>Then inspect and apply it inside your <code>send()</code> method:</p> <pre><code>if isinstance(config, MyCustomConfig):\n    do_something_with(config.foo)\n</code></pre>"},{"location":"concepts/di/","title":"\ud83e\uddea Dependency Injection (DI)","text":"<p>Dispytch supports a FastAPI-style Dependency Injection system to cleanly manage your handler dependencies\u2014keeping your logic modular, testable, and DRY.</p>"},{"location":"concepts/di/#why-use-di","title":"\ud83d\udca1 Why Use DI?","text":"<p>\u267b\ufe0f Decoupling \u2013 Separate business logic from infrastructure concerns</p> <p>\u2705 Testability \u2013 Easily mock or override dependencies in tests</p> <p>\ud83d\udd04 Reusability \u2013 Centralize shared resources and logic</p>"},{"location":"concepts/di/#how-it-works","title":"How It Works","text":"<p>Handlers declare dependencies using Python\u2019s type annotations combined with Dispytch\u2019s <code>Dependency</code> wrapper and <code>Event</code> generic class.</p> <p>Example:</p> <pre><code>class EventBody(BaseModel):\n    value: int\n    name: str\n\n\nasync def get_service() -&gt; Service:\n    service = Service()\n    yield service\n    await service.cleanup()\n\n\n@handler_group.handler(topic=\"test_events\", event=\"event_type\")\nasync def handle_event(\n        # Validates the event payload using EventBody model\n        event: Event[EventBody],\n\n        # Injects the result of get_service(); supports automatic cleanup when using context managers\n        service: Annotated[Service, Dependency(get_service)]\n):\n    print(f\"Name = {event.body.name} | Value = {event.body.value}\")\n    await service.do_smth(event.body.value)\n</code></pre> <p>At runtime, Dispytch:</p> <ol> <li> <p>Parses the handler signature.</p> </li> <li> <p>Resolves dependencies using the provided factory functions (sync or async).</p> </li> <li> <p>Injects results directly into the handler.</p> </li> <li> <p>Handles cleanup automatically for context-managed dependencies.</p> </li> </ol>"},{"location":"concepts/di/#nested-dependencies","title":"\ud83e\udde9 Nested Dependencies","text":"<p>Dispytch supports dependency chains\u2014where one dependency depends on another. Each layer is resolved automatically, with full lifecycle management.</p>"},{"location":"concepts/di/#example","title":"\u270d\ufe0f Example","text":"<pre><code>class Config(BaseModel):\n    threshold: int = 5\n\n\nasync def get_config() -&gt; Config:\n    return Config()\n\n\nasync def get_service(config: Annotated[Config, Dependency(get_config)]):\n    service = Service(config.threshold)\n    yield service\n    await service.cleanup()\n\n\n@handler_group.handler(topic=\"nested\", event=\"example\")\nasync def handle_nested(\n        event: Event[Any],\n        service: Annotated[Service, Dependency(get_service)]\n):\n    await service.do_smth()\n</code></pre>"},{"location":"concepts/di/#whats-happening","title":"\ud83d\udd0d What's happening","text":"<ul> <li><code>get_service</code> declares a dependency on <code>get_config</code></li> <li>Dispytch resolves <code>get_config</code>, then passes the result to <code>get_service</code></li> <li>The final resolved <code>Service</code> is injected into the handler</li> </ul>"},{"location":"concepts/di/#context-aware-dependencies","title":"\ud83c\udf10 Context-Aware Dependencies","text":"<p>Dependency functions can receive contextual information about the current event\u2014such as its topic, type, or payload\u2014by accepting a typed <code>Event[T]</code> as an argument.</p> <p>This enables runtime-aware logic, such as per-event logging, dynamic configuration, or tenant resolution.</p>"},{"location":"concepts/di/#example_1","title":"\u270d\ufe0f Example","text":"<pre><code>class Payload(BaseModel):\n    user_id: str\n    action: str\n\n\nasync def get_logger(event: Event[Payload]) -&gt; Logger:\n    # Logger initialized with event metadata\n    return Logger(context={\n        \"topic\": event.__topic__,\n        \"type\": event.__event_type__,\n        \"user_id\": event.body.user_id\n    })\n\n\n@handler_group.handler(topic=\"loggable\", event=\"ctx_aware\")\nasync def handle_event_with_logger(\n        event: Event[Payload],\n        logger: Annotated[Logger, Dependency(get_logger)]\n):\n    logger.info(f\"User {event.body.user_id} performed action: {event.body.action}\")\n</code></pre>"},{"location":"concepts/di/#alternative-syntax","title":"\ud83d\udd01 Alternative Syntax","text":"<p>As an alternative for the <code>Annotated[T, Dependency(...)]</code> style, Dispytch lets you inject dependencies by assigning a <code>Dependency</code> instance directly as a default value for a handler parameter. Functionally the same\u2014just a different flavor.</p> <p>\ud83d\udccb Note: This injection method does not work for the <code>Event</code> parameter. You must use explicit type hints for <code>Event</code> to enable proper injection.</p>"},{"location":"concepts/di/#example_2","title":"\u270d\ufe0f Example","text":"<pre><code>async def get_service() -&gt; Service:\n    return Service()\n\n\n@handler_group.handler(topic=\"alt_usage\", event=\"example\")\nasync def handler_two(\n        event: Event,\n        service=Dependency(get_service)  # Injected via default argument\n):\n    await service.do_smth()\n</code></pre>"},{"location":"concepts/dynamic_topics/","title":"\ud83e\udde0 Dynamic Topics","text":"<p>Dispytch makes event routing flexible and expressive through dynamic topics. This allows you to define parameterized topic structures like <code>user.{user_id}.notification</code> and bind them directly to your handler function arguments. Whether you're building per-tenant pipelines, user-specific notifications, or fine-grained subscriptions, dynamic topics are a useful tool in Dispytch.</p>"},{"location":"concepts/dynamic_topics/#what-are-dynamic-topics","title":"\ud83d\udd0d What Are Dynamic Topics?","text":"<p>Dynamic topics are topic patterns that contain segments identified by curly braces, e.g.:</p> <pre><code>\"user.{user_id}.notification\"\n</code></pre>"},{"location":"concepts/dynamic_topics/#use-cases","title":"\ud83d\udee0\ufe0f Use Cases","text":"<p>Some common use cases include:</p> <ul> <li>User-specific channels \u2013 <code>user.{user_id}.notification</code></li> <li>Tenant or organization scoping \u2013 <code>tenant.{tenant_id}.events</code></li> <li>Versioned event streams \u2013 <code>service.{version}.log</code></li> </ul>"},{"location":"concepts/dynamic_topics/#broker-compatibility","title":"\ud83e\uddef Broker Compatibility","text":"<p>Dynamic topics are supported with all brokers in Dispytch. However, keep in mind:</p> <ul> <li>Redis (with <code>psubscribe</code>) and AMQP (with topic exchange routing) are well-suited due to native support for   wildcards.</li> <li> <p>Kafka is technically compatible but not ideal for dynamic topic models due to:</p> <ul> <li>Static topic creation (topics must exist upfront)</li> <li>No wildcard subscription</li> <li>Poor scalability with high topic cardinality</li> </ul> </li> </ul> <p>If you're using Kafka, prefer fewer topics and use event payloads for context and partitions for scalability. But if your use case truly needs dynamic topics (e.g., for multi-tenancy separation), you can use dynamic topics carefully.</p>"},{"location":"concepts/dynamic_topics/#defining-dynamic-segments-with-topicsegment","title":"\ud83e\udde9 Defining Dynamic Segments with <code>TopicSegment()</code>","text":"<p>You can bind dynamic parts of the topic to function parameters using <code>TopicSegment</code>.</p> <p>Here are three ways to use it:</p> Annotated Parameter (Recommended)Class-style AnnotationDefault Value <pre><code>def handler(user_id: Annotated[int, TopicSegment()]):\n    ...\n</code></pre> <pre><code>def handler(user_id: Annotated[int, TopicSegment]):\n    ...\n</code></pre> <p>Equivalent to the first form, but useful if you forget the parentheses.</p> <pre><code>def handler(user_id: int = TopicSegment()):\n    ...\n</code></pre> <p>You're not allowed to forget the parentheses using this one xD</p>"},{"location":"concepts/dynamic_topics/#aliases-in-dynamic-segments","title":"\ud83e\ude9e Aliases in Dynamic Segments","text":"<p>By default, Dispytch binds the segment name in the topic (e.g., <code>user_id</code>) to the parameter name in your function. You can override this using aliases.</p>"},{"location":"concepts/dynamic_topics/#alias","title":"\ud83c\udff7\ufe0f <code>alias</code>","text":"<p>Sets the name that Dispytch should expect in the topic string.</p> <pre><code>def handler(uid: Annotated[int, TopicSegment(alias=\"user_id\")]):\n    ...\n</code></pre> <p>In this case, the topic should be:</p> <pre><code>\"user.{user_id}.notification\"\n</code></pre> <p>Even though your handler uses <code>uid</code>, Dispytch will map <code>user_id</code> from the topic to it.</p>"},{"location":"concepts/dynamic_topics/#validation_alias","title":"\ud83e\uddea <code>validation_alias</code>","text":"<p>You can also use <code>validation_alias</code></p> <pre><code>def handler(uid: int = TopicSegment(validation_alias=\"user_id\")):\n    ...\n</code></pre>"},{"location":"concepts/dynamic_topics/#both-alias-and-validation_alias","title":"\ud83e\udd77 Both <code>alias</code> and <code>validation_alias</code>","text":"<p>What if you use both? <code>validation_alias</code> takes precedence for parsing values from the topic.</p> <pre><code>def handler(user: int = TopicSegment(alias=\"ignored\", validation_alias=\"user_id\")):\n    ...\n</code></pre> <ul> <li>Topic segment: <code>user.{user_id}</code></li> <li>Handler parameter: <code>user</code></li> <li>Dispytch looks for <code>user_id</code> in the topic (not <code>ignored</code>)</li> </ul>"},{"location":"concepts/dynamic_topics/#example-user-specific-notifications-with-redis","title":"\u270d\ufe0f Example: User-Specific Notifications with Redis","text":"<pre><code>from typing import Annotated\nfrom dispytch import Event, TopicSegment\n\n\n@listener.handler(topic=\"user.{user_id}.notification\", event=\"user_notification\")\nasync def handle_notification(event: Event, user_id: Annotated[int, TopicSegment()]):\n    print(f\"\ud83d\udd14 Notification for user {user_id}: {event.body}\")\n</code></pre> <p>Given Topic: <code>\"user.42.notification\"</code></p> <p>Dispytch will extract <code>user_id=42</code> and pass it to the handler.</p>"},{"location":"concepts/dynamic_topics/#example-using-aliases","title":"\u270d\ufe0f Example: Using Aliases","text":"<pre><code>@listener.handler(topic=\"user.{uid}.notification\", event=\"user_notification\")\nasync def handler(user_id: Annotated[int, TopicSegment(alias=\"uid\")]):\n    print(f\"User ID: {user_id}\")\n</code></pre> <p>Here, Dispytch maps <code>{uid}</code> in the topic to the <code>user_id</code> parameter.</p>"},{"location":"concepts/dynamic_topics/#event-definition","title":"\ud83d\ude80 Event Definition","text":"<p>On the producer side, this looks like:</p> <pre><code>from dispytch import EventBase\n\n\nclass UserNotification(EventBase):\n    __topic__ = \"user.{user_id}.notification\"\n    __event_type__ = \"user_notification\"\n\n    user_id: int\n    message: str\n</code></pre> <p>This allows you to emit events with dynamic topics.</p>"},{"location":"concepts/dynamic_topics/#emitting-dynamic-events","title":"\ud83d\udce4 Emitting Dynamic Events","text":"<pre><code>event = UserNotification(user_id=42, message=\"Hey there!\")\nawait emitter.emit(event)\n</code></pre> <p>Dispytch will automatically interpolate the topic:</p> <pre><code>\"user.42.notification\"\n</code></pre>"},{"location":"concepts/dynamic_topics/#validating-topic-parameters","title":"\ud83e\uddea Validating Topic Parameters","text":"<p>Dynamic topic segments in Dispytch aren't just dumb markers. Under the hood, <code>TopicSegment()</code> has the properties of a Pydantic <code>Field</code>, which means you can apply validation constraints directly to values extracted from the topic.</p> <p>This is useful when:</p> <ul> <li>You want to restrict allowed values (e.g., whitelisting with <code>Literal</code>)</li> <li>You want to apply numeric bounds or type checks (e.g., <code>le=100</code>, <code>gt=0</code>)</li> <li>You want to fail early if a topic segment is invalid</li> </ul>"},{"location":"concepts/dynamic_topics/#example-whitelisting-with-literal","title":"\u270d\ufe0f Example: Whitelisting with <code>Literal</code>","text":"<p>You can define a handler that only accepts certain literal values:</p> <pre><code>from typing import Annotated, Literal\nfrom dispytch import TopicSegment\n\n\ndef handler(value: Annotated[Literal[\"test\", \"example\"], TopicSegment()]):\n    ...\n</code></pre> <p>If the incoming topic contains anything else \u2014 like <code>weirdvalue</code> \u2014 validation fails.</p>"},{"location":"concepts/dynamic_topics/#example-range-constraints","title":"\u270d\ufe0f Example: Range Constraints","text":"<p>You can also enforce constraints like <code>le</code> (less than or equal):</p> <pre><code>def handler(value: Annotated[int, TopicSegment(le=125)]):\n    ...\n</code></pre> <p>If the topic resolves to <code>value=130</code>, validation fails and Dispytch raises an error before calling the handler.</p>"},{"location":"concepts/dynamic_topics/#topic-delimiters-in-eventlistener","title":"\ud83d\udd17 Topic Delimiters in <code>EventListener</code>","text":"<p>When using dynamic topics, Dispytch needs a way to split topic strings into segments \u2014 this is done using the <code>topic_delimiter</code> argument in the <code>EventListener</code>.</p> <pre><code>listener = EventListener(consumer, topic_delimiter='.')\n</code></pre> <p>This tells Dispytch to treat topic segments as dot-separated:</p> <pre><code>\"user.123.notification\"  \u2192  [\"user\", \"123\", \"notification\"]\n</code></pre> <p>The <code>topic_delimiter</code> is used both for matching incoming topics and for extracting values from dynamic segments.</p>"},{"location":"concepts/dynamic_topics/#important-caveat-avoid-using-the-delimiter-in-substituted-values","title":"\u26a0\ufe0f Important Caveat: Avoid Using the Delimiter in Substituted Values","text":"<p>When you emit or receive an event with a dynamic topic, substituted values must not contain the delimiter. For example:</p> <pre><code>\"user.{value}.notification\"\n</code></pre> <p>With <code>topic_delimiter='.'</code>, using <code>value=7.45</code> would result in:</p> <pre><code>\"user.7.45.notification\"\n</code></pre> <p>This will break matching \u2014 because Dispytch will incorrectly split it into:</p> <pre><code>[\"user\", \"7\", \"45\", \"notification\"]\n</code></pre>"},{"location":"concepts/dynamic_topics/#do","title":"\u2705 DO:","text":"<p>Use values like <code>745</code>, <code>user_45</code>, or strings that don\u2019t include the delimiter.</p>"},{"location":"concepts/dynamic_topics/#dont","title":"\u274c DON'T:","text":"<p>Use values that contain the delimiter, like <code>7.45</code> with <code>'.'</code> or <code>\"foo/bar\"</code> with <code>'/'</code>.</p>"},{"location":"concepts/dynamic_topics/#broker-specific-delimiter-constraints","title":"\ud83d\udd12 Broker-Specific Delimiter Constraints","text":"<p>Some brokers enforce a specific topic delimiter that cannot be changed:</p> <ul> <li>RabbitMQ: Uses <code>.</code> (dot) as the hard-coded separator for topic exchanges.</li> <li>Kafka: Does not split topics by delimiter; full topic names are atomic.</li> <li>Redis (pubsub): Allows <code>psubscribe</code> with glob patterns, so any delimiter works, but be consistent.</li> </ul> <p>Make sure to align your <code>topic_delimiter</code> choice with your broker's behavior.</p>"},{"location":"concepts/emitter/","title":"\ud83d\udce4 <code>EventEmitter</code>","text":"<p>The <code>EventEmitter</code> is a core component of Dispytch used to emit (publish) events to an underlying message broker such as RabbitMQ or Kafka. It abstracts away the details of the producer backend and allows you to send events with minimal boilerplate.</p>"},{"location":"concepts/emitter/#why-do-i-need-it","title":"\u2705 Why do I need it?","text":"<ul> <li> <p>Separation of concerns: Your app\u2019s business logic shouldn\u2019t wrestle with raw message brokers. <code>EventEmitter</code>   abstracts away the gritty details of RabbitMQ, Kafka, or whatever is under the hood, so you can focus on events\u2014not   infrastructure.</p> </li> <li> <p>Consistency &amp; safety: Typed events with <code>EventBase</code> ensure your payloads are validated and predictable. No more   guessing the shape of your data or fumbling with manual serialization.</p> </li> <li> <p>Plug &amp; play with multiple backends: Whether you\u2019re team Kafka or RabbitMQ (or both), <code>EventEmitter</code> lets you   switch between or postpone backend decisions with less overhead.</p> </li> <li> <p>Testability: Emitting an event is just calling a method on an object you can mock or swap out\u2014making your code   easier to test and reason about.</p> </li> </ul> <p>Bottom line: <code>EventEmitter</code> turns noisy, complex event publishing into a streamlined, reliable, and developer-friendly interface. Without it, you\u2019re stuck juggling broker APIs, serialization, and error-prone glue code.</p>"},{"location":"concepts/emitter/#basic-structure","title":"\ud83e\uddf1 Basic Structure","text":"<pre><code>event_emitter = EventEmitter(producer)\nawait event_emitter.emit(MyEvent(...))\n</code></pre> <p><code>EventEmitter</code> expects a <code>Producer</code> instance (such as <code>RabbitMQProducer</code> or <code>KafkaProducer</code>) that handles the actual transport layer.</p>"},{"location":"concepts/emitter/#event-definition","title":"\ud83e\uddfe Event Definition","text":"<ul> <li> <p><code>MyEvent</code> inherits from <code>EventBase</code> and defines:</p> <ul> <li><code>__topic__</code>: Target topic for the event.</li> <li><code>__event_type__</code>: Identifier for the type of event.</li> <li>Event payload fields using standard <code>pydantic</code> model syntax.</li> </ul> </li> </ul> <p>Example:</p> <pre><code>from dispytch import EventBase\n\n\nclass MyEvent(EventBase):\n    __topic__ = \"my_topic\"\n    __event_type__ = \"something_happened\"\n\n    user_id: str\n    value: int\n</code></pre>"},{"location":"concepts/emitter/#example-setting-up-event-emitter","title":"\u270d\ufe0f Example: Setting Up Event Emitter","text":"RabbitMQKafkaRedis Pub/Sub <pre><code>import aio_pika\nfrom dispytch import EventEmitter, EventBase\nfrom dispytch.rabbitmq import RabbitMQProducer\n\n\nclass MyEvent(EventBase):\n    __topic__ = 'notifications'\n    __event_type__ = 'user_registered'\n\n    user_id: str\n    email: str\n\n\nasync def main():\n    connection = await aio_pika.connect('amqp://guest:guest@localhost:5672')\n    channel = await connection.channel()\n    exchange = await channel.declare_exchange('notifications', aio_pika.ExchangeType.DIRECT)\n\n    producer = RabbitMQProducer(exchange)\n    emitter = EventEmitter(producer)\n\n    await emitter.emit(\n        MyEvent(user_id=\"abc123\",\n                email=\"user@example.com\")\n    )\n    print(\"Event sent!\")\n</code></pre> <p>\ud83d\udca1 Note: <code>__topic__</code> will be used as a routing key when published to exchange</p> <pre><code>from aiokafka import AIOKafkaProducer\nfrom dispytch import EventEmitter, EventBase\nfrom dispytch.kafka import KafkaProducer\n\n\nclass MyEvent(EventBase):\n    __topic__ = 'user_events'\n    __event_type__ = 'user_logged_in'\n\n    user_id: str\n    timestamp: str\n\n\nasync def main():\n    kafka_raw_producer = AIOKafkaProducer(bootstrap_servers=\"localhost:19092\")\n    # The next line is essential. \n    await kafka_raw_producer.start()  # DO NOT FORGET \n\n    producer = KafkaProducer(kafka_raw_producer)\n    emitter = EventEmitter(producer)\n\n    await emitter.emit(\n        MyEvent(user_id=\"abc123\",\n                timestamp=\"2025-07-07T12:00:00Z\")\n    )\n    print(\"Event emitted!\")\n</code></pre> <p>\u26a0\ufe0f Important:</p> <p>When using Kafka with EventEmitter, you must manually start the underlying AIOKafkaProducer. Dispytch does not start it for you.</p> <p>If you forget to call:</p> <pre><code>await kafka_raw_producer.start()\n</code></pre> <p>events will not be published, and you won\u2019t get any errors\u2014they\u2019ll just silently vanish into the void.</p> <p>So don\u2019t skip it. Don\u2019t forget it. Your future self will thank you.</p> <pre><code># !!! Important: Use the asyncio-compatible Redis client from redis.asyncio\nfrom redis.asyncio import Redis\nfrom dispytch import EventEmitter, EventBase\nfrom dispytch.redis import RedisProducer\n\n\nclass SystemAlert(EventBase):\n    __topic__ = \"system.alerts\"\n    __event_type__ = \"system_alert\"\n\n    level: str\n    message: str\n\n\nasync def main():\n    redis = Redis()\n\n    producer = RedisProducer(redis)\n    emitter = EventEmitter(producer)\n\n    await emitter.emit(\n        SystemAlert(level=\"critical\",\n                    message=\"CPU temperature high\")\n    )\n    print(\"Alert sent!\")\n</code></pre> <p>\u26a0\ufe0f Important:</p> <p>When using RedisProducer with EventEmitter, you should pass the asyncio-compatible Redis client (from redis.asyncio) to the producer.</p>"},{"location":"concepts/emitter/#handling-timeouts-with-on_timeout","title":"\u23f1\ufe0f Handling Timeouts with <code>on_timeout</code>","text":"<p>By default, if an event fails to emit due to a timeout, Dispytch logs a warning. If you want custom behavior (e.g., metrics, retries, alerts), you can register a callback using <code>on_timeout()</code>:</p> <pre><code>@emitter.on_timeout\ndef handle_timeout(event):\n    print(f\"Event {event.id} failed to emit!\")\n</code></pre> <p>The callback can be sync or async, and receives the original <code>EventBase</code> instance that timed out.</p>"},{"location":"concepts/emitter/#notes","title":"\ud83d\udccc Notes","text":"<ul> <li>Dispytch automatically serializes the payload as JSON by default. To change the default serializer you can   pass included <code>MessagePackSerializer</code> to the EventEmitter or write one on your own</li> <li>Event ordering and delivery guarantees \u2014 depend on the underlying broker (Kafka/RabbitMQ/Redis), not Dispytch.</li> </ul>"},{"location":"concepts/listener/","title":"\ud83d\udce5 <code>EventListener</code>","text":"<p><code>EventListener</code> is Dispytch\u2019s high-level abstraction for consuming events from a message broker (like Kafka or RabbitMQ) and dispatching them to appropriate async handler functions. It\u2019s where your event-driven logic lives \u2014 clean, decoupled, and dependency-injected.</p>"},{"location":"concepts/listener/#why-do-i-need-it","title":"\u2705 Why do I need it?","text":"<ul> <li> <p>Decoupled event handling: <code>EventListener</code> connects your logic to the outside world without mixing transport   concerns into your business code. It listens, routes, and invokes handlers\u2014you just write the logic.</p> </li> <li> <p>Strong typing, minimal guesswork: With <code>Event[...]</code> and Pydantic models, your handlers receive structured,   validated data. No manual parsing, no ambiguous payloads.</p> </li> <li> <p>Built-in dependency injection: Handlers don\u2019t need to do everything. With first-class support for DI, you can   split logic into small, testable, reusable functions that plug into the handler automatically.</p> </li> <li> <p>Async-native execution: Handlers are fully async. Whether you're processing 10 events or 10,000, <code>EventListener</code>   doesn\u2019t block or get in your way.</p> </li> <li> <p>Flexible retry logic: Failures happen. With per-handler retry controls, you decide what\u2019s worth retrying and how   persistent to be\u2014without bloating your handler code.</p> </li> <li> <p>Organized and scalable: With <code>HandlerGroup</code>, you can group related handlers by topic or concern, making   codebases more maintainable and modular.</p> </li> <li> <p>Backend agnostic: Whether you're consuming from Kafka, RabbitMQ, or anything else behind a compatible consumer,   the interface stays the same\u2014clean and stable.</p> </li> </ul> <p>Bottom line: <code>EventListener</code> gives you a clean, scalable, and testable way to react to events across your system. Without it, you're hand-wiring consumers, parsing raw payloads, and stuffing all your logic into bloated callback functions.</p>"},{"location":"concepts/listener/#basic-structure","title":"\ud83e\uddf1 Basic Structure","text":"<pre><code>event_listener = EventListener(consumer)\n\n\n@event_listener.handler(topic=\"...\", event=\"...\")\nasync def handle_event(event: Event[T]):\n    ...\n</code></pre> <p>Where:</p> <ul> <li><code>consumer</code> is an instance of a compatible consumer (e.g. <code>KafkaConsumer</code>, <code>RabbitMQConsumer</code> or your own)</li> <li><code>T</code> is your <code>pydantic</code> model for the event body.</li> <li>Decorated handler is auto-wired to the event type via <code>topic</code> + <code>event</code>.</li> </ul>"},{"location":"concepts/listener/#example-setting-up-event-listener","title":"\u270d\ufe0f Example: Setting Up Event Listener","text":"RabbitMQKafkaRedis Pub/Sub <pre><code>import aio_pika\nfrom typing import Annotated\nfrom pydantic import BaseModel\n\nfrom dispytch import EventListener, Event, Dependency\nfrom dispytch.rabbitmq import RabbitMQConsumer\n\n\nclass MyEventBody(BaseModel):\n    user_id: str\n\n\nasync def get_user(event: Event[MyEventBody]):\n    yield event.body.user_id\n\n\nasync def main():\n    connection = await aio_pika.connect(\"amqp://guest:guest@localhost:5672\")\n    channel = await connection.channel()\n    queue = await channel.declare_queue(\"notifications\")\n    exchange = await channel.declare_exchange(\"notifications\", aio_pika.ExchangeType.DIRECT)\n    await queue.bind(exchange, routing_key=\"notifications\")\n\n    consumer = RabbitMQConsumer(queue)\n    listener = EventListener(consumer)\n\n    @listener.handler(topic=\"notifications\", event=\"user_registered\")\n    async def handle_user_reg(\n            user_id: Annotated[str, Dependency(get_user)]\n    ):\n        print(f\"Received registration for user {user_id}\")\n\n    await listener.listen()\n</code></pre> <pre><code>from aiokafka import AIOKafkaConsumer\nfrom typing import Annotated\nfrom pydantic import BaseModel\n\nfrom dispytch import EventListener, Event, Dependency\nfrom dispytch.kafka import KafkaConsumer\n\n\nclass MyEventBody(BaseModel):\n    action: str\n    value: int\n\n\nasync def parse_value(event: Event[MyEventBody]):\n    yield event.body.value\n\n\nasync def main():\n    raw_consumer = AIOKafkaConsumer(\n        \"user_events\",\n        bootstrap_servers=\"localhost:19092\",\n        enable_auto_commit=False,\n        group_id=\"listener_group\"\n    )\n    # the next line is essential\n    await raw_consumer.start()  # DO NOT FORGET\n    consumer = KafkaConsumer(raw_consumer)\n    listener = EventListener(consumer)\n\n    @listener.handler(topic=\"user_events\", event=\"user_logged_in\")\n    async def handle_login(\n            value: Annotated[int, Dependency(parse_value)]\n    ):\n        print(f\"Login action with value: {value}\")\n\n    await listener.listen()\n</code></pre> <p>\u26a0\ufe0f Important:</p> <p>When using Kafka with EventListener, you must manually start the underlying AIOKafkaConsumer. Dispytch does not start it for you.</p> <p>If you forget to call:</p> <pre><code>await raw_consumer.start()\n</code></pre> <p>events will not be consumed, and you won\u2019t get any errors\u2014they\u2019ll just silently vanish into the void.</p> <p>So don\u2019t skip it. Don\u2019t forget it. Your future self will thank you.</p> <pre><code># !!! Important: Use the asyncio-compatible Redis client from redis.asyncio\nfrom redis.asyncio import Redis\nfrom pydantic import BaseModel\n\nfrom dispytch import EventListener, Event\nfrom dispytch.redis import RedisConsumer\n\n\nclass SystemAlert(BaseModel):\n    level: str\n    message: str\n\n\nasync def main():\n    redis = Redis()\n    pubsub = redis.pubsub()\n\n    await pubsub.subscribe(\"system.alerts\")\n\n    consumer = RedisConsumer(pubsub)\n    listener = EventListener(consumer)\n\n    @listener.handler(topic=\"system.alerts\", event=\"system_alert\")\n    async def handle_alert(event: Event[SystemAlert]):\n        print(f\"\ud83d\udea8 [{event.body.level.upper()}] {event.body.message}\")\n\n    print(\"\ud83d\udee1\ufe0f Listening for system alerts...\")\n    await listener.listen()\n</code></pre> <p>\u26a0\ufe0f Important:</p> <p>When using RedisConsumer with EventListener, you should pass the asyncio-compatible Redis client (from redis.asyncio) to the consumer.</p>"},{"location":"concepts/listener/#notes-gotchas","title":"\u26a0\ufe0f Notes &amp; Gotchas","text":"<ul> <li>Handlers receive the full <code>Event[T]</code> with your typed payload (Pydantic model) under <code>.body</code>:</li> <li>The event payload must match the Pydantic schema \u2014 or decoding will fail.</li> <li>The <code>event</code> string must match <code>__event_type__</code> of the published event.</li> <li>Event handling is fully async, and multiple handlers can run concurrently.</li> <li>You can attach multiple handlers to the same topic and event type.</li> </ul>"},{"location":"concepts/listener/#retries","title":"\ud83d\udd01 Retries","text":"<p>Handlers can be configured to automatically retry on failure using <code>retries</code>, <code>retry_on</code>, and <code>retry_interval</code>.</p>"},{"location":"concepts/listener/#parameters","title":"\ud83c\udfaf Parameters:","text":"<ul> <li><code>retries</code>: Number of retry attempts (default: <code>0</code>)</li> <li><code>retry_on</code>: list of exception types to trigger retry. If <code>None</code>, all exceptions will trigger retry.</li> <li><code>retry_interval</code>: Delay (in seconds) between retries (default: <code>1.25</code>)</li> </ul>"},{"location":"concepts/listener/#example","title":"\u2705 Example:","text":"<pre><code>@listener.handler(\n    topic=\"critical_events\",\n    event=\"do_or_die\",\n    retries=3,\n    retry_on=[RuntimeError],\n    retry_interval=2.0\n)\nasync def handle_critical(event: Event[MyEventBody]):\n    print(\"Processing...\")\n    raise RuntimeError(\"Temporary failure\")\n</code></pre> <p>In this example, the handler will retry up to 3 times only if a <code>RuntimeError</code> is raised, waiting 2 seconds between attempts.</p> <p>\ud83d\udca1 If you don't set <code>retry_on</code>, all exceptions will trigger a retry \u2014 use with caution.</p>"},{"location":"concepts/listener/#handlergroup","title":"\ud83e\udde9 <code>HandlerGroup</code>","text":"<p><code>HandlerGroup</code> allows you to organize and register handlers modularly, useful for grouping handlers by topic or event type.</p>"},{"location":"concepts/listener/#use-cases","title":"\u2705 Use Cases","text":"<ul> <li>Defining a group of related handlers</li> <li>Splitting handlers into modules</li> <li>Avoiding repetition of <code>topic</code>/<code>event</code> in every decorator</li> </ul>"},{"location":"concepts/listener/#setup","title":"\ud83d\udd27 Setup","text":"<pre><code>from dispytch import HandlerGroup\n\ngroup = HandlerGroup(default_topic=\"my_topic\", default_event=\"default_type\")\n\n\n@group.handler(event=\"user_created\")\nasync def handle_user_created(event: Event[...]):\n    ...\n\n\n@group.handler(event=\"user_deleted\", retries=2)\nasync def handle_user_deleted(event: Event[...]):\n    ...\n</code></pre> <p>You can register this <code>HandlerGroup</code> with an <code>EventListener</code>:</p> <pre><code>listener.add_handler_group(group)\n</code></pre> <p>Behind the scenes, Dispytch will collect all handlers in the group and attach them to the listener.</p>"},{"location":"concepts/listener/#group-config-behavior","title":"\ud83c\udfaf Group Config Behavior","text":"<ul> <li><code>topic</code> and <code>event</code> in the decorator override group defaults.</li> <li>If neither is set (decorator or default), you get a <code>TypeError</code>.</li> </ul>"},{"location":"concepts/own_consumers_and_producers/","title":"\u2699\ufe0f Writing Custom Producers &amp; Consumers","text":"<p>Dispytch doesn\u2019t lock you into any specific messaging backend. If you want to connect to something like Redis Streams, SQS, or whatever queue you want\u2014you can do that by implementing your own <code>Producer</code> and <code>Consumer</code>.</p> <p>Here\u2019s how.</p>"},{"location":"concepts/own_consumers_and_producers/#custom-producer","title":"\ud83e\uddea Custom Producer","text":"<p>To build your own event emitter backend, implement the <code>Producer</code> interface.</p>"},{"location":"concepts/own_consumers_and_producers/#interface","title":"\u270d\ufe0f Interface","text":"<pre><code>class Producer(ABC):\n    @abstractmethod\n    async def send(self, topic: str, payload: bytes, config: BaseModel | None = None):\n        ...\n</code></pre>"},{"location":"concepts/own_consumers_and_producers/#notes","title":"\ud83d\udca1 Notes","text":"<ul> <li><code>topic</code>: where the event goes</li> <li><code>payload</code>: bytes containing the event payload</li> <li><code>config</code>: optional backend-specific config, usually declared in the event as <code>__backend_config__</code></li> <li>If your send logic times out raise <code>ProducerTimeout</code></li> </ul>"},{"location":"concepts/own_consumers_and_producers/#example-pseudocode","title":"\u2705 Example (Pseudocode!!!)","text":"<pre><code>from dispytch.emitter.producer import ProducerTimeout, Producer\n\n\nclass RedisProducer(Producer):\n    async def send(self, topic: str, payload: bytes, config: BaseModel | None = None):\n        result = await redis_client.xadd(topic, payload)\n        if not result:\n            raise ProducerTimeout(\"Redis XADD failed\")\n</code></pre>"},{"location":"concepts/own_consumers_and_producers/#custom-consumer","title":"\ud83e\uddc3 Custom Consumer","text":"<p>To receive and handle events from your own backend, implement the <code>Consumer</code> interface.</p>"},{"location":"concepts/own_consumers_and_producers/#interface_1","title":"\u270d\ufe0f Interface","text":"<pre><code>class Consumer(ABC):\n    @abstractmethod\n    def listen(self) -&gt; AsyncIterator[Message]:\n        ...\n\n    @abstractmethod\n    def ack(self, msg: Message):\n        ...\n</code></pre>"},{"location":"concepts/own_consumers_and_producers/#notes_1","title":"\ud83d\udca1 Notes","text":"<ul> <li> <p><code>listen()</code> must yield <code>Message</code> objects. This is an async generator.</p> </li> <li> <p><code>ack()</code> is called when Dispytch successfully processes an event. Use it to mark the event as handled (e.g., ack a   Kafka offset or delete a message from a queue).</p> </li> </ul>"},{"location":"concepts/own_consumers_and_producers/#example-pseudocode_1","title":"\u2705 Example (Pseudocode!!!)","text":"<pre><code>from dispytch.listener.consumer import Consumer, Message\n\n\nclass RedisConsumer(Consumer):\n    async def listen(self) -&gt; AsyncIterator[Message]:\n        while True:\n            raw = await redis_client.xread(...)\n            yield Message(\n                topic=raw[\"stream\"],\n                payload=raw[\"payload\"]\n            )\n\n    def ack(self, msg: Message):\n        # Redis streams might not need manual ack, or you could XDEL here\n        pass\n</code></pre>"},{"location":"concepts/own_consumers_and_producers/#use-your-custom-classes","title":"\ud83d\udee0\ufe0f Use Your Custom Classes","text":"<p>Once implemented, you can use your custom producer and consumer classes directly in <code>EventEmitter</code> and <code>EventListener</code></p>"},{"location":"concepts/serialization/","title":"\ud83e\uddf1 Serialization &amp; Deserialization","text":"<p>By default, Dispytch uses JSON for serializing and deserializing events. This keeps things simple and readable\u2014but you're not stuck with it. If you're sending binary data, need better performance, or just enjoy making things more complicated than they need to be, you can plug in a custom serializer or deserializer.</p>"},{"location":"concepts/serialization/#setting-a-serializer-producer-side","title":"\u270d\ufe0f Setting a Serializer (Producer Side)","text":"<p>To override the default JSON serializer, pass a serializer instance to your <code>EventEmitter</code>:</p> <pre><code>from dispytch.serialization.msgpack import MessagePackSerializer\n\nemitter = EventEmitter(producer, MessagePackSerializer())\n</code></pre> <p>If you don\u2019t explicitly pass serializer, <code>JSONSerializer()</code> is used under the hood.</p>"},{"location":"concepts/serialization/#setting-a-deserializer-consumer-side","title":"\ud83e\udde9 Setting a Deserializer (Consumer Side)","text":"<p>Same deal for consumers. You can pick how incoming messages are decoded (should be consistent with sending side, though):</p> <pre><code>from dispytch.serialization.msgpack import MessagePackDeserializer\n\nlistener = EventListener(consumer, MessagePackDeserializer())\n</code></pre> <p>Again, if you don\u2019t set it, Dispytch will default to <code>JSONDeserializer()</code>.</p>"},{"location":"concepts/serialization/#writing-your-own","title":"\u2728 Writing Your Own","text":"<p>Custom serialization is as simple as implementing a method.</p> <pre><code>from dispytch.serialization.serializer import Serializer\n\n\nclass MyCoolSerializer(Serializer):\n    def serialize(self, payload: dict) -&gt; bytes:\n        ...\n</code></pre> <pre><code>from dispytch.serialization.deserializer import Deserializer, MessagePayload\n\n\nclass MyCoolDeserializer(Deserializer):\n    def deserialize(self, data: bytes) -&gt; MessagePayload:\n        ...\n</code></pre>"},{"location":"getting-started/installation/","title":"\ud83d\udce6 Installation","text":"<p>Dispytch is backend-agnostic by default. This means it won't install Kafka or RabbitMQ dependencies unless explicitly requested.</p> <p>Install with <code>uv</code>, <code>poetry</code> or <code>pip</code>, including extras for your preferred message broker:</p>"},{"location":"getting-started/installation/#with-kafka-support","title":"With Kafka support","text":"<pre><code>uv add dispytch[kafka]\n# or\npoetry add dispytch[kafka]\n# or\npip install dispytch[kafka]\n</code></pre> <p>Includes: <code>aiokafka</code></p>"},{"location":"getting-started/installation/#with-rabbitmq-support","title":"With RabbitMQ support","text":"<pre><code>uv add dispytch[rabbitmq]\n# or\npoetry add dispytch[rabbitmq]\n# or\npip install dispytch[rabbitmq]\n</code></pre> <p>Includes: <code>aio-pika</code></p>"},{"location":"getting-started/installation/#with-redis-support","title":"With Redis support","text":"<pre><code>uv add dispytch[redis]\n# or\npoetry add dispytch[redis]\n# or\npip install dispytch[redis]\n</code></pre> <p>Includes: <code>redis</code></p>"},{"location":"getting-started/installation/#no-backend-by-default","title":"\u26a0\ufe0f No Backend by Default","text":"<p>If you install Dispytch without any extras:</p> <pre><code>uv add dispytch\n# or\npoetry add dispytch\n# or\npip install dispytch\n</code></pre> <p>then no producer or consumer backends will be available. You'll need to install at least one extra or install the dependencies separately to use built-in event producers and consumers.</p>"},{"location":"getting-started/installation/#custom-backends","title":"\ud83d\udd27 Custom Backends","text":"<p>If you're building your own backend implementation (e.g., for Redis, NATS, SQS, etc.), installing Dispytch without extras is exactly what you want.</p>"},{"location":"getting-started/quickstart/","title":"\ud83d\ude80 Quickstart","text":"<p>Get your event-driven flow running with Dispytch in four simple steps.</p>"},{"location":"getting-started/quickstart/#1-define-your-event","title":"1. Define Your Event","text":"<p>Subclass <code>EventBase</code> to declare your event\u2019s topic and type metadata, along with its payload:</p> <pre><code>from dispytch import EventBase\n\n\nclass UserRegistered(EventBase):\n    __topic__ = \"user_events\"\n    __event_type__ = \"user_registered\"\n\n    user_id: str\n    email: str\n</code></pre>"},{"location":"getting-started/quickstart/#2-emit-events","title":"2. Emit Events","text":"<p>Create an <code>EventEmitter</code> with your configured backend producer, then emit events asynchronously:</p> <pre><code>from dispytch.emitter import EventEmitter\nfrom events import UserRegistered\n\nproducer = ...  # your backend producer setup\nemitter = EventEmitter(producer)\n</code></pre> <pre><code>async def emit_user_registered(emitter):\n    await emitter.emit(UserRegistered(user_id=\"123\", email=\"user@example.com\"))\n</code></pre>"},{"location":"getting-started/quickstart/#3-register-event-handlers","title":"3. Register Event Handlers","text":"<p>Organize handlers with <code>HandlerGroup</code>. Define the event schema (usually a Pydantic model), then decorate your async handler:</p> <pre><code>from pydantic import BaseModel\nfrom dispytch import HandlerGroup, Event\n\n\nclass UserRegistered(BaseModel):\n    user_id: str\n    email: str\n\n\nuser_events = HandlerGroup(default_topic=\"user_events\")\n\n\n@user_events.handler(event=\"user_registered\")\nasync def handle_user_registered(event: Event[UserRegistered]):\n    print(f\"User {event.body.user_id} registered with email {event.body.email}\")\n</code></pre>"},{"location":"getting-started/quickstart/#4-start-the-listener","title":"4. Start the Listener","text":"<p>Connect your backend consumer to an <code>EventListener</code>, register your handler group(s), then listen for incoming events:</p> <pre><code>import asyncio\nfrom dispytch.listener import EventListener\nfrom handlers import user_events\n\nconsumer = ...  # your backend consumer setup\nlistener = EventListener(consumer)\nlistener.add_handler_group(user_events)\n\nif __name__ == \"__main__\":\n    asyncio.run(listener.listen())\n</code></pre>"},{"location":"getting-started/quickstart/#thats-it","title":"That\u2019s It!","text":"<p>Define events, emit them, handle them asynchronously \u2014 all wired up and ready to roll.</p>"}]}