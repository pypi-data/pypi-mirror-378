Metadata-Version: 2.4
Name: crawlo
Version: 1.3.3
Summary: Crawlo æ˜¯ä¸€æ¬¾åŸºäºå¼‚æ­¥IOçš„é«˜æ€§èƒ½Pythonçˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒåˆ†å¸ƒå¼æŠ“å–ã€‚
Home-page: https://github.com/crawl-coder/Crawlo.git
Author: crawl-coder
Author-email: crawlo@qq.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: aiohttp>=3.12.14
Requires-Dist: aiomysql>=0.2.0
Requires-Dist: aioredis>=2.0.1
Requires-Dist: asyncmy>=0.2.10
Requires-Dist: cssselect>=1.2.0
Requires-Dist: dateparser>=1.2.2
Requires-Dist: httpx[http2]>=0.27.0
Requires-Dist: curl-cffi>=0.13.0
Requires-Dist: lxml>=5.2.1
Requires-Dist: motor>=3.7.0
Requires-Dist: parsel>=1.9.1
Requires-Dist: pydantic>=2.11.7
Requires-Dist: pymongo>=4.11
Requires-Dist: PyMySQL>=1.1.1
Requires-Dist: python-dateutil>=2.9.0.post0
Requires-Dist: redis>=6.2.0
Requires-Dist: requests>=2.32.4
Requires-Dist: six>=1.17.0
Requires-Dist: ujson>=5.9.0
Requires-Dist: urllib3>=2.5.0
Requires-Dist: w3lib>=2.1.2
Requires-Dist: rich>=14.1.0
Requires-Dist: astor>=0.8.1
Requires-Dist: watchdog>=6.0.0
Provides-Extra: render
Requires-Dist: webdriver-manager>=4.0.0; extra == "render"
Requires-Dist: playwright; extra == "render"
Requires-Dist: selenium>=3.141.0; extra == "render"
Provides-Extra: all
Requires-Dist: bitarray>=1.5.3; extra == "all"
Requires-Dist: PyExecJS>=1.5.1; extra == "all"
Requires-Dist: pymongo>=3.10.1; extra == "all"
Requires-Dist: redis-py-cluster>=2.1.0; extra == "all"
Requires-Dist: webdriver-manager>=4.0.0; extra == "all"
Requires-Dist: playwright; extra == "all"
Requires-Dist: selenium>=3.141.0; extra == "all"

<!-- markdownlint-disable MD033 MD041 -->
<div align="center">
  <h1 align="center">Crawlo</h1>
  <p align="center">å¼‚æ­¥åˆ†å¸ƒå¼çˆ¬è™«æ¡†æ¶</p>
  <p align="center"><strong>åŸºäº asyncio çš„é«˜æ€§èƒ½å¼‚æ­¥åˆ†å¸ƒå¼çˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼éƒ¨ç½²</strong></p>
  
  <p align="center">
    <a href="https://www.python.org/downloads/">
      <img src="https://img.shields.io/badge/python-%3C%3D3.12-blue" alt="Python Version">
    </a>
    <a href="LICENSE">
      <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
    </a>
    <a href="https://crawlo.readthedocs.io/">
      <img src="https://img.shields.io/badge/docs-latest-brightgreen" alt="Documentation">
    </a>
    <a href="https://github.com/crawlo/crawlo/actions">
      <img src="https://github.com/crawlo/crawlo/workflows/CI/badge.svg" alt="CI Status">
    </a>
  </p>
  
  <p align="center">
    <a href="#-ç‰¹æ€§">ç‰¹æ€§</a> â€¢
    <a href="#-å¿«é€Ÿå¼€å§‹">å¿«é€Ÿå¼€å§‹</a> â€¢
    <a href="#-å‘½ä»¤è¡Œå·¥å…·">å‘½ä»¤è¡Œå·¥å…·</a> â€¢
    <a href="#-ç¤ºä¾‹é¡¹ç›®">ç¤ºä¾‹é¡¹ç›®</a>
  </p>
</div>

<br />

<!-- ç‰¹æ€§ section -->
<div align="center">
  <h2>ğŸŒŸ ç‰¹æ€§</h2>

  <table>
    <thead>
      <tr>
        <th>ç‰¹æ€§</th>
        <th>æè¿°</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>âš¡ <strong>å¼‚æ­¥é«˜æ€§èƒ½</strong></td>
        <td>åŸºäº asyncio å®ç°ï¼Œå……åˆ†åˆ©ç”¨ç°ä»£ CPU å¤¯æ€§èƒ½</td>
      </tr>
      <tr>
        <td>ğŸŒ <strong>åˆ†å¸ƒå¼æ”¯æŒ</strong></td>
        <td>å†…ç½® Redis é˜Ÿåˆ—ï¼Œè½»æ¾å®ç°åˆ†å¸ƒå¼éƒ¨ç½²</td>
      </tr>
      <tr>
        <td>ğŸ”§ <strong>æ¨¡å—åŒ–è®¾è®¡</strong></td>
        <td>ä¸­é—´ä»¶ã€ç®¡é“ã€æ‰©å±•ç»„ä»¶ç³»ç»Ÿï¼Œæ˜“äºå®šåˆ¶å’Œæ‰©å±•</td>
      </tr>
      <tr>
        <td>ğŸ”„ <strong>æ™ºèƒ½å»é‡</strong></td>
        <td>å¤šç§å»é‡ç­–ç•¥ï¼ˆå†…å­˜ã€Redisã€Bloom Filterï¼‰</td>
      </tr>
      <tr>
        <td>âš™ï¸ <strong>çµæ´»é…ç½®</strong></td>
        <td>æ”¯æŒå¤šç§é…ç½®æ–¹å¼ï¼Œé€‚åº”ä¸åŒåœºæ™¯éœ€æ±‚</td>
      </tr>
      <tr>
        <td>ğŸ“‹ <strong>é«˜çº§æ—¥å¿—</strong></td>
        <td>æ”¯æŒæ—¥å¿—è½®è½¬ã€ç»“æ„åŒ–æ—¥å¿—ã€JSONæ ¼å¼ç­‰é«˜çº§åŠŸèƒ½</td>
      </tr>
      <tr>
        <td>ğŸ“š <strong>ä¸°å¯Œæ–‡æ¡£</strong></td>
        <td>å®Œæ•´çš„ä¸­è‹±æ–‡åŒè¯­æ–‡æ¡£å’Œç¤ºä¾‹é¡¹ç›®</td>
      </tr>
    </tbody>
  </table>
</div>

<br />

---

<!-- å¿«é€Ÿå¼€å§‹ section -->
<h2 align="center">ğŸš€ å¿«é€Ÿå¼€å§‹</h2>

### å®‰è£…

```bash
pip install crawlo
```

### åˆ›å»ºé¡¹ç›®

```bash
# åˆ›å»ºé»˜è®¤é¡¹ç›®
crawlo startproject myproject

# åˆ›å»ºåˆ†å¸ƒå¼æ¨¡æ¿é¡¹ç›®
crawlo startproject myproject distributed

# åˆ›å»ºé¡¹ç›®å¹¶é€‰æ‹©ç‰¹å®šæ¨¡å—
crawlo startproject myproject --modules mysql,redis,proxy

cd myproject
```

### ç”Ÿæˆçˆ¬è™«

```bash
# åœ¨é¡¹ç›®ç›®å½•ä¸­ç”Ÿæˆçˆ¬è™«
crawlo genspider news_spider news.example.com
```

### ç¼–å†™çˆ¬è™«

``python
from crawlo import Spider, Request, Item

class MyItem(Item):
    title = ''
    url = ''

class MySpider(Spider):
    name = 'myspider'
    
    async def start_requests(self):
        yield Request('https://httpbin.org/get', callback=self.parse)
    
    async def parse(self, response):
        yield MyItem(
            title='Example Title',
            url=response.url
        )
```

### è¿è¡Œçˆ¬è™«

```bash
# ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è¿è¡Œçˆ¬è™«ï¼ˆæ¨èï¼‰
crawlo run myspider

# ä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„ run.py è„šæœ¬è¿è¡Œ
python run.py

# è¿è¡Œæ‰€æœ‰çˆ¬è™«
crawlo run all

# åœ¨é¡¹ç›®å­ç›®å½•ä¸­ä¹Ÿèƒ½æ­£ç¡®è¿è¡Œ
cd subdirectory
crawlo run myspider
```

---

<!-- å‘½ä»¤è¡Œå·¥å…· section -->
<h2 align="center">ğŸ”§ å‘½ä»¤è¡Œå·¥å…·</h2>

Crawlo æä¾›äº†ä¸°å¯Œçš„å‘½ä»¤è¡Œå·¥å…·ï¼Œç®€åŒ–é¡¹ç›®åˆ›å»ºå’Œç®¡ç†ã€‚

### crawlo startproject

åˆ›å»ºæ–°çš„çˆ¬è™«é¡¹ç›®ã€‚

```bash
# åˆ›å»ºé»˜è®¤é¡¹ç›®
crawlo startproject myproject

# åˆ›å»ºæŒ‡å®šæ¨¡æ¿çš„é¡¹ç›®
crawlo startproject myproject simple
crawlo startproject myproject distributed
```

### crawlo genspider

åœ¨ç°æœ‰é¡¹ç›®ä¸­ç”Ÿæˆæ–°çš„çˆ¬è™«ã€‚

```bash
# åœ¨å½“å‰ç›®å½•ç”Ÿæˆçˆ¬è™«
crawlo genspider myspider http://example.com

# æŒ‡å®šæ¨¡æ¿ç”Ÿæˆçˆ¬è™«
crawlo genspider myspider http://example.com --template basic
```

### crawlo run

è¿è¡ŒæŒ‡å®šçš„çˆ¬è™«ã€‚

```bash
# è¿è¡Œå•ä¸ªçˆ¬è™«
crawlo run myspider

# è¿è¡Œæ‰€æœ‰çˆ¬è™«
crawlo run all

# ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ
crawlo run myspider --json

# ç¦ç”¨ç»Ÿè®¡ä¿¡æ¯
crawlo run myspider --no-stats
```

### crawlo list

åˆ—å‡ºé¡¹ç›®ä¸­çš„æ‰€æœ‰çˆ¬è™«ã€‚

```bash
crawlo list
```

### crawlo check

æ£€æŸ¥é¡¹ç›®é…ç½®å’Œçˆ¬è™«å®ç°ã€‚

```bash
# æ£€æŸ¥æ‰€æœ‰çˆ¬è™«
crawlo check

# æ£€æŸ¥ç‰¹å®šçˆ¬è™«
crawlo check myspider
```

### crawlo stats

æŸ¥çœ‹çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯ã€‚

```bash
# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
crawlo stats
```

---

<!-- é…ç½®æ–¹å¼ section -->
<h2 align="center">âš™ï¸ é…ç½®æ–¹å¼</h2>

Crawlo æä¾›äº†å¤šç§çµæ´»çš„é…ç½®æ–¹å¼ï¼Œä»¥é€‚åº”ä¸åŒçš„ä½¿ç”¨åœºæ™¯å’Œå¼€å‘éœ€æ±‚ã€‚

### ä¸‰ç§é…ç½®æ–¹å¼è¯¦è§£

#### 1. é…ç½®å·¥å‚æ–¹å¼ï¼ˆæ¨èï¼‰

ä½¿ç”¨ `CrawloConfig` é…ç½®å·¥å‚æ˜¯æ¨èçš„é…ç½®æ–¹å¼ï¼Œå®ƒæä¾›äº†ç±»å‹å®‰å…¨å’Œæ™ºèƒ½æç¤ºã€‚

```python
from crawlo.config import CrawloConfig
from crawlo.crawler import CrawlerProcess

# å•æœºæ¨¡å¼é…ç½®
config = CrawloConfig.standalone(
    concurrency=8,
    download_delay=1.0
)

# åˆ†å¸ƒå¼æ¨¡å¼é…ç½®
config = CrawloConfig.distributed(
    redis_host='127.0.0.1',
    redis_port=6379,
    project_name='myproject',
    concurrency=16
)

# è‡ªåŠ¨æ£€æµ‹æ¨¡å¼é…ç½®
config = CrawloConfig.auto(concurrency=12)

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
config = CrawloConfig.from_env()

# åˆ›å»ºçˆ¬è™«è¿›ç¨‹
process = CrawlerProcess(settings=config.to_dict())
```

#### 2. ç›´æ¥é…ç½®æ–¹å¼

ç›´æ¥åœ¨ `settings.py` æ–‡ä»¶ä¸­é…ç½®å„é¡¹å‚æ•°ï¼Œé€‚åˆéœ€è¦ç²¾ç»†æ§åˆ¶çš„åœºæ™¯ã€‚

```
# settings.py
PROJECT_NAME = 'myproject'
RUN_MODE = 'standalone'  # æˆ– 'distributed' æˆ– 'auto'
CONCURRENCY = 8
DOWNLOAD_DELAY = 1.0

# åˆ†å¸ƒå¼æ¨¡å¼ä¸‹éœ€è¦é…ç½®Redis
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
REDIS_PASSWORD = ''

# å…¶ä»–é…ç½®...
```

#### 3. ç¯å¢ƒå˜é‡æ–¹å¼

é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼Œé€‚åˆéƒ¨ç½²å’ŒCI/CDåœºæ™¯ã€‚

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export CRAWLO_MODE=standalone
export CONCURRENCY=8
export DOWNLOAD_DELAY=1.0
export REDIS_HOST=127.0.0.1
export REDIS_PORT=6379
```

```python
# åœ¨ä»£ç ä¸­è¯»å–ç¯å¢ƒå˜é‡
from crawlo.config import CrawloConfig
config = CrawloConfig.from_env()
process = CrawlerProcess(settings=config.to_dict())
```

### ä¸åŒè¿è¡Œæ¨¡å¼ä¸‹çš„æœ€ä½³é…ç½®æ–¹å¼

#### å•æœºæ¨¡å¼ (standalone)

é€‚ç”¨äºå¼€å‘è°ƒè¯•ã€å°è§„æ¨¡æ•°æ®é‡‡é›†ã€ä¸ªäººé¡¹ç›®ã€‚

**æ¨èé…ç½®æ–¹å¼ï¼š**
```python
from crawlo.config import CrawloConfig
config = CrawloConfig.standalone(concurrency=4, download_delay=1.0)
process = CrawlerProcess(settings=config.to_dict())
```

**ç‰¹ç‚¹ï¼š**
- ç®€å•æ˜“ç”¨ï¼Œèµ„æºå ç”¨å°‘
- æ— éœ€é¢å¤–ä¾èµ–ï¼ˆå¦‚Redisï¼‰
- é€‚åˆä¸ªäººå¼€å‘ç¯å¢ƒ

#### åˆ†å¸ƒå¼æ¨¡å¼ (distributed)

é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é‡‡é›†ã€å¤šèŠ‚ç‚¹ååŒå·¥ä½œã€é«˜å¹¶å‘éœ€æ±‚ã€‚

**æ¨èé…ç½®æ–¹å¼ï¼š**
```python
from crawlo.config import CrawloConfig
config = CrawloConfig.distributed(
    redis_host='your_redis_host',
    redis_port=6379,
    project_name='myproject',
    concurrency=16
)
process = CrawlerProcess(settings=config.to_dict())
```

**ç‰¹ç‚¹ï¼š**
- æ”¯æŒå¤šèŠ‚ç‚¹æ‰©å±•
- é«˜å¹¶å‘å¤„ç†èƒ½åŠ›
- éœ€è¦Redisæ”¯æŒ

#### è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ (auto)

é€‚ç”¨äºå¸Œæœ›æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€ä½³è¿è¡Œæ–¹å¼ã€‚

**æ¨èé…ç½®æ–¹å¼ï¼š**
```python
from crawlo.config import CrawloConfig
config = CrawloConfig.auto(concurrency=12)
process = CrawlerProcess(settings=config.to_dict())
```

**ç‰¹ç‚¹ï¼š**
- æ™ºèƒ½æ£€æµ‹ç¯å¢ƒé…ç½®
- è‡ªåŠ¨é€‰æ‹©è¿è¡Œæ¨¡å¼
- é€‚åˆåœ¨ä¸åŒç¯å¢ƒä¸­ä½¿ç”¨åŒä¸€å¥—é…ç½®

### ç»„ä»¶é…ç½®è¯´æ˜

Crawloæ¡†æ¶çš„ä¸­é—´ä»¶ã€ç®¡é“å’Œæ‰©å±•ç»„ä»¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åŠ è½½é»˜è®¤ç»„ä»¶ï¼Œç”¨æˆ·åªéœ€é…ç½®è‡ªå®šä¹‰ç»„ä»¶ã€‚

#### ä¸­é—´ä»¶é…ç½®

æ¡†æ¶é»˜è®¤åŠ è½½ä»¥ä¸‹ä¸­é—´ä»¶ï¼š
- RequestIgnoreMiddlewareï¼šå¿½ç•¥æ— æ•ˆè¯·æ±‚
- DownloadDelayMiddlewareï¼šæ§åˆ¶è¯·æ±‚é¢‘ç‡
- DefaultHeaderMiddlewareï¼šæ·»åŠ é»˜è®¤è¯·æ±‚å¤´
- ProxyMiddlewareï¼šè®¾ç½®ä»£ç†
- OffsiteMiddlewareï¼šç«™å¤–è¯·æ±‚è¿‡æ»¤
- RetryMiddlewareï¼šå¤±è´¥è¯·æ±‚é‡è¯•
- ResponseCodeMiddlewareï¼šå¤„ç†ç‰¹æ®ŠçŠ¶æ€ç 
- ResponseFilterMiddlewareï¼šå“åº”å†…å®¹è¿‡æ»¤

ç”¨æˆ·å¯ä»¥é€šè¿‡`CUSTOM_MIDDLEWARES`é…ç½®è‡ªå®šä¹‰ä¸­é—´ä»¶ï¼š

``python
# settings.py
CUSTOM_MIDDLEWARES = [
    'myproject.middlewares.CustomMiddleware',
]
```

> **æ³¨æ„**ï¼šDefaultHeaderMiddleware å’Œ OffsiteMiddleware éœ€è¦ç›¸åº”çš„é…ç½®æ‰èƒ½å¯ç”¨ï¼š
> - DefaultHeaderMiddleware éœ€è¦é…ç½® `DEFAULT_REQUEST_HEADERS` æˆ– `USER_AGENT` å‚æ•°
> - OffsiteMiddleware éœ€è¦é…ç½® `ALLOWED_DOMAINS` å‚æ•°
> 
> å¦‚æœæœªé…ç½®ç›¸åº”å‚æ•°ï¼Œè¿™äº›ä¸­é—´ä»¶ä¼šå› ä¸º NotConfiguredError è€Œè¢«ç¦ç”¨ã€‚

> **æ³¨æ„**ï¼šä¸­é—´ä»¶çš„é¡ºåºå¾ˆé‡è¦ã€‚SimpleProxyMiddleware é€šå¸¸æ”¾åœ¨åˆ—è¡¨æœ«å°¾ï¼Œ
> è¿™æ ·å¯ä»¥åœ¨æ‰€æœ‰é»˜è®¤ä¸­é—´ä»¶å¤„ç†åå†åº”ç”¨ä»£ç†è®¾ç½®ã€‚

#### ç®¡é“é…ç½®

æ¡†æ¶é»˜è®¤åŠ è½½ä»¥ä¸‹ç®¡é“ï¼š
- ConsolePipelineï¼šæ§åˆ¶å°è¾“å‡º
- é»˜è®¤å»é‡ç®¡é“ï¼ˆæ ¹æ®è¿è¡Œæ¨¡å¼è‡ªåŠ¨é€‰æ‹©ï¼‰

ç”¨æˆ·å¯ä»¥é€šè¿‡`CUSTOM_PIPELINES`é…ç½®è‡ªå®šä¹‰ç®¡é“ï¼š

```python
# settings.py
CUSTOM_PIPELINES = [
    'crawlo.pipelines.json_pipeline.JsonPipeline',
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',
]
```

#### æ‰©å±•é…ç½®

æ¡†æ¶é»˜è®¤åŠ è½½ä»¥ä¸‹æ‰©å±•ï¼š
- LogIntervalExtensionï¼šå®šæ—¶æ—¥å¿—
- LogStatsï¼šç»Ÿè®¡ä¿¡æ¯
- CustomLoggerExtensionï¼šè‡ªå®šä¹‰æ—¥å¿—

ç”¨æˆ·å¯ä»¥é€šè¿‡`CUSTOM_EXTENSIONS`é…ç½®è‡ªå®šä¹‰æ‰©å±•ï¼š

```python
# settings.py
CUSTOM_EXTENSIONS = [
    'crawlo.extension.memory_monitor.MemoryMonitorExtension',
]
```

<!-- æ¶æ„è®¾è®¡ section -->
<h2 align="center">ğŸ—ï¸ æ¶æ„è®¾è®¡</h2>

### æ ¸å¿ƒç»„ä»¶è¯´æ˜

Crawlo æ¡†æ¶ç”±ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶æ„æˆï¼š

<table>
  <thead>
    <tr>
      <th>ç»„ä»¶</th>
      <th>åŠŸèƒ½æè¿°</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Crawler</strong></td>
      <td>çˆ¬è™«è¿è¡Œå®ä¾‹ï¼Œç®¡ç†Spiderä¸å¼•æ“çš„ç”Ÿå‘½å‘¨æœŸ</td>
    </tr>
    <tr>
      <td><strong>Engine</strong></td>
      <td>å¼•æ“ç»„ä»¶ï¼Œåè°ƒSchedulerã€Downloaderã€Processor</td>
    </tr>
    <tr>
      <td><strong>Scheduler</strong></td>
      <td>è°ƒåº¦å™¨ï¼Œç®¡ç†è¯·æ±‚é˜Ÿåˆ—å’Œå»é‡è¿‡æ»¤</td>
    </tr>
    <tr>
      <td><strong>Downloader</strong></td>
      <td>ä¸‹è½½å™¨ï¼Œè´Ÿè´£ç½‘ç»œè¯·æ±‚ï¼Œæ”¯æŒå¤šç§å®ç°(aiohttp, httpx, curl-cffi)</td>
    </tr>
    <tr>
      <td><strong>Processor</strong></td>
      <td>å¤„ç†å™¨ï¼Œå¤„ç†å“åº”æ•°æ®å’Œç®¡é“</td>
    </tr>
    <tr>
      <td><strong>QueueManager</strong></td>
      <td>ç»Ÿä¸€çš„é˜Ÿåˆ—ç®¡ç†å™¨ï¼Œæ”¯æŒå†…å­˜é˜Ÿåˆ—å’ŒRedisé˜Ÿåˆ—çš„è‡ªåŠ¨åˆ‡æ¢</td>
    </tr>
    <tr>
      <td><strong>Filter</strong></td>
      <td>è¯·æ±‚å»é‡è¿‡æ»¤å™¨ï¼Œæ”¯æŒå†…å­˜å’ŒRedisä¸¤ç§å®ç°</td>
    </tr>
    <tr>
      <td><strong>Middleware</strong></td>
      <td>ä¸­é—´ä»¶ç³»ç»Ÿï¼Œå¤„ç†è¯·æ±‚/å“åº”çš„é¢„å¤„ç†å’Œåå¤„ç†</td>
    </tr>
    <tr>
      <td><strong>Pipeline</strong></td>
      <td>æ•°æ®å¤„ç†ç®¡é“ï¼Œæ”¯æŒå¤šç§å­˜å‚¨æ–¹å¼(æ§åˆ¶å°ã€æ•°æ®åº“ç­‰)å’Œå»é‡åŠŸèƒ½</td>
    </tr>
    <tr>
      <td><strong>Spider</strong></td>
      <td>çˆ¬è™«åŸºç±»ï¼Œå®šä¹‰çˆ¬å–é€»è¾‘</td>
    </tr>
  </tbody>
</table>

### è¿è¡Œæ¨¡å¼

Crawloæ”¯æŒä¸‰ç§è¿è¡Œæ¨¡å¼ï¼š

<table>
  <thead>
    <tr>
      <th>æ¨¡å¼</th>
      <th>æè¿°</th>
      <th>é˜Ÿåˆ—ç±»å‹</th>
      <th>è¿‡æ»¤å™¨ç±»å‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>standalone</strong></td>
      <td>å•æœºæ¨¡å¼</td>
      <td>å†…å­˜é˜Ÿåˆ—</td>
      <td>å†…å­˜è¿‡æ»¤å™¨</td>
    </tr>
    <tr>
      <td><strong>distributed</strong></td>
      <td>åˆ†å¸ƒå¼æ¨¡å¼</td>
      <td>Redisé˜Ÿåˆ—</td>
      <td>Redisè¿‡æ»¤å™¨</td>
    </tr>
    <tr>
      <td><strong>auto</strong></td>
      <td>è‡ªåŠ¨æ£€æµ‹æ¨¡å¼</td>
      <td>æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€ä½³è¿è¡Œæ–¹å¼</td>
      <td>æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©</td>
    </tr>
  </tbody>
</table>

> **è¿è¡Œæ¨¡å¼è¯´æ˜**: distributedæ¨¡å¼ä¸ºå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®¾è®¡ï¼Œå¼ºåˆ¶ä½¿ç”¨Redisé˜Ÿåˆ—å’Œå»é‡ï¼›standalone+autoä¸ºå•æœºæ™ºèƒ½æ¨¡å¼ï¼Œæ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©å†…å­˜æˆ–Redisé˜Ÿåˆ—ä¸å»é‡ç­–ç•¥ï¼Œé›¶é…ç½®å¯åŠ¨ã€‚

#### è¿è¡Œæ¨¡å¼é€‰æ‹©æŒ‡å—

##### 1. å•æœºæ¨¡å¼ (standalone)
- **é€‚ç”¨åœºæ™¯**ï¼š
  - å¼€å‘å’Œæµ‹è¯•é˜¶æ®µ
  - å°è§„æ¨¡æ•°æ®é‡‡é›†ï¼ˆå‡ åƒåˆ°å‡ ä¸‡æ¡æ•°æ®ï¼‰
  - å­¦ä¹ å’Œæ¼”ç¤ºç”¨é€”
  - å¯¹ç›®æ ‡ç½‘ç«™è´Ÿè½½è¦æ±‚ä¸é«˜çš„åœºæ™¯
- **ä¼˜åŠ¿**ï¼š
  - é…ç½®ç®€å•ï¼Œæ— éœ€é¢å¤–ä¾èµ–
  - èµ„æºæ¶ˆè€—ä½
  - å¯åŠ¨å¿«é€Ÿ
  - é€‚åˆæœ¬åœ°å¼€å‘è°ƒè¯•
- **é™åˆ¶**ï¼š
  - æ— æ³•è·¨ä¼šè¯å»é‡
  - æ— æ³•åˆ†å¸ƒå¼éƒ¨ç½²
  - å†…å­˜å ç”¨éšæ•°æ®é‡å¢é•¿

##### 2. åˆ†å¸ƒå¼æ¨¡å¼ (distributed)
- **é€‚ç”¨åœºæ™¯**ï¼š
  - å¤§è§„æ¨¡æ•°æ®é‡‡é›†ï¼ˆç™¾ä¸‡çº§ä»¥ä¸Šï¼‰
  - éœ€è¦å¤šèŠ‚ç‚¹ååŒå·¥ä½œ
  - è¦æ±‚è·¨ä¼šè¯ã€è·¨èŠ‚ç‚¹å»é‡
  - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- **ä¼˜åŠ¿**ï¼š
  - æ”¯æŒæ°´å¹³æ‰©å±•
  - è·¨èŠ‚ç‚¹ä»»åŠ¡åè°ƒ
  - æŒä¹…åŒ–å»é‡è¿‡æ»¤
  - é«˜å¯ç”¨æ€§
- **è¦æ±‚**ï¼š
  - éœ€è¦RedisæœåŠ¡å™¨
  - ç½‘ç»œç¯å¢ƒç¨³å®š
  - æ›´å¤æ‚çš„é…ç½®ç®¡ç†

##### 3. è‡ªåŠ¨æ¨¡å¼ (auto)
- **é€‚ç”¨åœºæ™¯**ï¼š
  - å¸Œæœ›æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
  - å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒä½¿ç”¨åŒä¸€å¥—ä»£ç 
  - åŠ¨æ€é€‚åº”è¿è¡Œç¯å¢ƒ
- **å·¥ä½œæœºåˆ¶**ï¼š
  - æ£€æµ‹Rediså¯ç”¨æ€§
  - Rediså¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°åˆ†å¸ƒå¼æ¨¡å¼
  - Redisä¸å¯ç”¨æ—¶å›é€€åˆ°å•æœºæ¨¡å¼
- **ä¼˜åŠ¿**ï¼š
  - ç¯å¢ƒé€‚åº”æ€§å¼º
  - éƒ¨ç½²çµæ´»
  - å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒé…ç½®ç»Ÿä¸€

#### é˜Ÿåˆ—ç±»å‹é€‰æ‹©æŒ‡å—

Crawloæ”¯æŒä¸‰ç§é˜Ÿåˆ—ç±»å‹ï¼Œå¯é€šè¿‡`QUEUE_TYPE`é…ç½®é¡¹è®¾ç½®ï¼š

- **memory**ï¼šä½¿ç”¨å†…å­˜é˜Ÿåˆ—ï¼Œé€‚ç”¨äºå•æœºæ¨¡å¼
- **redis**ï¼šä½¿ç”¨Redisé˜Ÿåˆ—ï¼Œé€‚ç”¨äºåˆ†å¸ƒå¼æ¨¡å¼
- **auto**ï¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼Œæ ¹æ®Rediså¯ç”¨æ€§è‡ªåŠ¨é€‰æ‹©

æ¨èä½¿ç”¨`auto`æ¨¡å¼ï¼Œè®©æ¡†æ¶æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„é˜Ÿåˆ—ç±»å‹ã€‚

<!-- é…ç½®ç³»ç»Ÿ section -->
<h2 align="center">ğŸ›ï¸ é…ç½®ç³»ç»Ÿ</h2>

### ä¼ ç»Ÿé…ç½®æ–¹å¼

```
# settings.py
PROJECT_NAME = 'myproject'
CONCURRENCY = 16
DOWNLOAD_DELAY = 1.0
QUEUE_TYPE = 'memory'  # å•æœºæ¨¡å¼
# QUEUE_TYPE = 'redis'   # åˆ†å¸ƒå¼æ¨¡å¼

# Redis é…ç½® (åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ä½¿ç”¨)
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB = 0
REDIS_PASSWORD = ''

# æ•°æ®ç®¡é“é…ç½®
# æ³¨æ„ï¼šæ¡†æ¶é»˜è®¤ç®¡é“å·²è‡ªåŠ¨åŠ è½½ï¼Œæ­¤å¤„ä»…ç”¨äºæ·»åŠ è‡ªå®šä¹‰ç®¡é“
CUSTOM_PIPELINES = [
    'crawlo.pipelines.json_pipeline.JsonPipeline',
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',      # MySQLå­˜å‚¨ç®¡é“
]

# é«˜çº§æ—¥å¿—é…ç½®
LOG_FILE = 'logs/spider.log'
LOG_LEVEL = 'INFO'
LOG_MAX_BYTES = 10 * 1024 * 1024  # 10MB
LOG_BACKUP_COUNT = 5
LOG_JSON_FORMAT = False  # è®¾ç½®ä¸ºTrueå¯ç”¨JSONæ ¼å¼

# å¯ç”¨é«˜çº§æ—¥å¿—æ‰©å±•
ADVANCED_LOGGING_ENABLED = True

# å¯ç”¨æ—¥å¿—ç›‘æ§
LOG_MONITOR_ENABLED = True
LOG_MONITOR_INTERVAL = 30
LOG_MONITOR_DETAILED_STATS = True

# æ·»åŠ æ‰©å±•ï¼ˆæ³¨æ„ï¼šæ¡†æ¶é»˜è®¤æ‰©å±•å·²è‡ªåŠ¨åŠ è½½ï¼Œæ­¤å¤„ä»…ç”¨äºæ·»åŠ è‡ªå®šä¹‰æ‰©å±•ï¼‰
CUSTOM_EXTENSIONS = [
    'crawlo.extension.memory_monitor.MemoryMonitorExtension',
]
```

### MySQL ç®¡é“é…ç½®

Crawlo æä¾›äº†ç°æˆçš„ MySQL ç®¡é“å®ç°ï¼Œå¯ä»¥è½»æ¾å°†çˆ¬å–çš„æ•°æ®å­˜å‚¨åˆ° MySQL æ•°æ®åº“ä¸­ï¼š

```
# åœ¨ settings.py ä¸­å¯ç”¨ MySQL ç®¡é“
CUSTOM_PIPELINES = [
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',
]

# MySQL æ•°æ®åº“é…ç½®
MYSQL_HOST = 'localhost'
MYSQL_PORT = 3306
MYSQL_USER = 'your_username'
MYSQL_PASSWORD = 'your_password'
MYSQL_DB = 'your_database'
MYSQL_TABLE = 'your_table_name'

# å¯é€‰çš„æ‰¹é‡æ’å…¥é…ç½®
MYSQL_BATCH_SIZE = 100
MYSQL_USE_BATCH = True
```

MySQL ç®¡é“ç‰¹æ€§ï¼š
- **å¼‚æ­¥æ“ä½œ**ï¼šåŸºäº asyncmy é©±åŠ¨ï¼Œæä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥æ•°æ®åº“æ“ä½œ
- **è¿æ¥æ± **ï¼šè‡ªåŠ¨ç®¡ç†æ•°æ®åº“è¿æ¥ï¼Œæé«˜æ•ˆç‡
- **æ‰¹é‡æ’å…¥**ï¼šæ”¯æŒæ‰¹é‡æ’å…¥ä»¥æé«˜æ€§èƒ½
- **äº‹åŠ¡æ”¯æŒ**ï¼šç¡®ä¿æ•°æ®ä¸€è‡´æ€§
- **çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªå®šä¹‰è¡¨åã€æ‰¹é‡å¤§å°ç­‰å‚æ•°

### å‘½ä»¤è¡Œé…ç½®

```
# è¿è¡Œå•ä¸ªçˆ¬è™«
crawlo run myspider

# è¿è¡Œæ‰€æœ‰çˆ¬è™«
crawlo run all

# åœ¨é¡¹ç›®å­ç›®å½•ä¸­ä¹Ÿèƒ½æ­£ç¡®è¿è¡Œ
cd subdirectory
crawlo run myspider
```

---

<!-- æ ¸å¿ƒç»„ä»¶ section -->
<h2 align="center">ğŸ§© æ ¸å¿ƒç»„ä»¶</h2>

### Requestç±»

Requestç±»æ˜¯Crawloæ¡†æ¶ä¸­ç”¨äºå°è£…HTTPè¯·æ±‚çš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›äº†ä¸°å¯Œçš„åŠŸèƒ½æ¥å¤„ç†å„ç§ç±»å‹çš„HTTPè¯·æ±‚ã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from crawlo import Request

# åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„GETè¯·æ±‚
request = Request('https://example.com')

# åˆ›å»ºå¸¦å›è°ƒå‡½æ•°çš„è¯·æ±‚
request = Request('https://example.com', callback=self.parse)
```

#### paramså‚æ•°ï¼ˆGETè¯·æ±‚å‚æ•°ï¼‰

ä½¿ç”¨`params`å‚æ•°æ¥æ·»åŠ GETè¯·æ±‚çš„æŸ¥è¯¢å‚æ•°ï¼Œè¿™äº›å‚æ•°ä¼šè‡ªåŠ¨é™„åŠ åˆ°URLä¸Šï¼š

```python
# GETè¯·æ±‚å¸¦å‚æ•°
request = Request(
    url='https://httpbin.org/get',
    params={'key1': 'value1', 'key2': 'value2', 'page': 1},
    callback=self.parse
)
# å®é™…è¯·æ±‚URLä¼šå˜æˆ: https://httpbin.org/get?key1=value1&key2=value2&page=1

# å¤æ‚å‚æ•°ç¤ºä¾‹
request = Request(
    url='https://api.example.com/search',
    params={
        'q': 'pythonçˆ¬è™«',
        'sort': 'date',
        'order': 'desc',
        'limit': 20
    },
    callback=self.parse_results
)
```

å¯¹äºGETè¯·æ±‚ï¼Œå¦‚æœåŒæ—¶æŒ‡å®šäº†[params](file:///Users/oscar/projects/Crawlo/crawlo/network/request.py#L55-L55)å’Œ[form_data](file:///Users/oscar/projects/Crawlo/crawlo/network/request.py#L53-L53)å‚æ•°ï¼Œå®ƒä»¬éƒ½ä¼šè¢«ä½œä¸ºæŸ¥è¯¢å‚æ•°é™„åŠ åˆ°URLä¸Šã€‚

#### form_dataå‚æ•°ï¼ˆPOSTè¡¨å•æ•°æ®ï¼‰

ä½¿ç”¨`form_data`å‚æ•°å‘é€è¡¨å•æ•°æ®ï¼Œä¼šæ ¹æ®è¯·æ±‚æ–¹æ³•è‡ªåŠ¨å¤„ç†ï¼š

```python
# POSTè¯·æ±‚å‘é€è¡¨å•æ•°æ®
request = Request(
    url='https://httpbin.org/post',
    method='POST',
    form_data={
        'username': 'crawlo_user',
        'password': 'secret_password',
        'remember_me': 'true'
    },
    callback=self.parse_login
)

# GETè¯·æ±‚ä½¿ç”¨form_dataï¼ˆä¼šè‡ªåŠ¨è½¬æ¢ä¸ºæŸ¥è¯¢å‚æ•°ï¼‰
request = Request(
    url='https://httpbin.org/get',
    method='GET',
    form_data={
        'search': 'crawlo framework',
        'category': 'documentation'
    },
    callback=self.parse_search
)
```

å¯¹äºPOSTè¯·æ±‚ï¼Œ[form_data](file:///Users/oscar/projects/Crawlo/crawlo/network/request.py#L53-L53)ä¼šè¢«ç¼–ç ä¸º`application/x-www-form-urlencoded`æ ¼å¼å¹¶ä½œä¸ºè¯·æ±‚ä½“å‘é€ã€‚

#### json_bodyå‚æ•°ï¼ˆJSONè¯·æ±‚ä½“ï¼‰

ä½¿ç”¨`json_body`å‚æ•°å‘é€JSONæ•°æ®ï¼š

```python
# å‘é€JSONæ•°æ®
request = Request(
    url='https://api.example.com/users',
    method='POST',
    json_body={
        'name': 'Crawlo User',
        'email': 'user@example.com',
        'preferences': {
            'theme': 'dark',
            'notifications': True
        }
    },
    callback=self.parse_response
)

# PUTè¯·æ±‚æ›´æ–°èµ„æº
request = Request(
    url='https://api.example.com/users/123',
    method='PUT',
    json_body={
        'name': 'Updated Name',
        'email': 'updated@example.com'
    },
    callback=self.parse_update
)
```

ä½¿ç”¨[json_body](file:///Users/oscar/projects/Crawlo/crawlo/network/request.py#L54-L54)æ—¶ï¼Œä¼šè‡ªåŠ¨è®¾ç½®`Content-Type: application/json`è¯·æ±‚å¤´ï¼Œå¹¶å°†æ•°æ®åºåˆ—åŒ–ä¸ºJSONæ ¼å¼ã€‚

#### æ··åˆä½¿ç”¨å‚æ•°

å¯ä»¥åŒæ—¶ä½¿ç”¨å¤šç§å‚æ•°ç±»å‹ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨å¤„ç†ï¼š

```python
# GETè¯·æ±‚åŒæ—¶ä½¿ç”¨paramså’Œform_dataï¼ˆéƒ½ä¼šä½œä¸ºæŸ¥è¯¢å‚æ•°ï¼‰
request = Request(
    url='https://api.example.com/search',
    params={'category': 'books'},           # ä½œä¸ºæŸ¥è¯¢å‚æ•°
    form_data={'q': 'python', 'limit': 10}, # ä¹Ÿä½œä¸ºæŸ¥è¯¢å‚æ•°
    callback=self.parse_search
)

# POSTè¯·æ±‚ä½¿ç”¨form_dataå’Œheaders
request = Request(
    url='https://api.example.com/upload',
    method='POST',
    form_data={'title': 'My Document'},
    headers={'Authorization': 'Bearer token123'},
    callback=self.parse_upload
)
```

#### è¯·æ±‚é…ç½®

Requestç±»æ”¯æŒä¸°å¯Œçš„é…ç½®é€‰é¡¹ï¼š

```python
request = Request(
    url='https://example.com',
    method='GET',
    headers={'User-Agent': 'Crawlo Bot'},
    cookies={'session_id': 'abc123'},
    priority=RequestPriority.HIGH,
    timeout=30,
    proxy='http://proxy.example.com:8080',
    dont_filter=True,  # è·³è¿‡å»é‡æ£€æŸ¥
    meta={'custom_key': 'custom_value'},  # ä¼ é€’è‡ªå®šä¹‰å…ƒæ•°æ®
    callback=self.parse
)
```

#### é“¾å¼è°ƒç”¨

Requestç±»æ”¯æŒé“¾å¼è°ƒç”¨æ¥ç®€åŒ–é…ç½®ï¼š

```python
request = Request('https://example.com')\
    .add_header('User-Agent', 'Crawlo Bot')\
    .set_proxy('http://proxy.example.com:8080')\
    .set_timeout(30)\
    .add_flag('important')\
    .set_meta('custom_key', 'custom_value')
```

#### ä¼˜å…ˆçº§è®¾ç½®

Crawloæä¾›äº†å¤šç§é¢„å®šä¹‰çš„è¯·æ±‚ä¼˜å…ˆçº§ï¼š

```python
from crawlo import Request, RequestPriority

# è®¾ç½®ä¸åŒçš„ä¼˜å…ˆçº§
urgent_request = Request('https://example.com', priority=RequestPriority.URGENT)
high_request = Request('https://example.com', priority=RequestPriority.HIGH)
normal_request = Request('https://example.com', priority=RequestPriority.NORMAL)
low_request = Request('https://example.com', priority=RequestPriority.LOW)
background_request = Request('https://example.com', priority=RequestPriority.BACKGROUND)
```

#### åŠ¨æ€åŠ è½½å™¨

å¯¹äºéœ€è¦JavaScriptæ¸²æŸ“çš„é¡µé¢ï¼Œå¯ä»¥å¯ç”¨åŠ¨æ€åŠ è½½å™¨ï¼š

```python
# å¯ç”¨åŠ¨æ€åŠ è½½å™¨
request = Request('https://example.com')\
    .set_dynamic_loader(use_dynamic=True)

# æˆ–è€…ä½¿ç”¨é“¾å¼è°ƒç”¨
request = Request('https://example.com')\
    .set_dynamic_loader(True, {'wait_time': 3, 'timeout': 30})
```

### ä¸­é—´ä»¶ç³»ç»Ÿ
çµæ´»çš„ä¸­é—´ä»¶ç³»ç»Ÿï¼Œæ”¯æŒè¯·æ±‚é¢„å¤„ç†ã€å“åº”å¤„ç†å’Œå¼‚å¸¸å¤„ç†ã€‚

Crawloæ¡†æ¶å†…ç½®äº†å¤šç§ä¸­é—´ä»¶ï¼Œå…¶ä¸­ä»£ç†ä¸­é—´ä»¶æœ‰ä¸¤ç§å®ç°ï¼š

1. **ProxyMiddlewareï¼ˆå¤æ‚ç‰ˆï¼‰**ï¼š
   - åŠ¨æ€ä»APIè·å–ä»£ç†
   - ä»£ç†æ± ç®¡ç†
   - å¥åº·æ£€æŸ¥å’ŒæˆåŠŸç‡ç»Ÿè®¡
   - å¤æ‚çš„ä»£ç†æå–é€»è¾‘
   - é€‚ç”¨äºéœ€è¦é«˜çº§ä»£ç†ç®¡ç†åŠŸèƒ½çš„åœºæ™¯

2. **SimpleProxyMiddlewareï¼ˆç®€åŒ–ç‰ˆï¼‰**ï¼š
   - åŸºäºå›ºå®šä»£ç†åˆ—è¡¨çš„ç®€å•å®ç°
   - è½»é‡çº§ï¼Œä»£ç ç®€æ´
   - æ˜“äºé…ç½®å’Œä½¿ç”¨
   - é€‚ç”¨äºåªéœ€è¦åŸºæœ¬ä»£ç†åŠŸèƒ½çš„åœºæ™¯

å¦‚æœéœ€è¦ä½¿ç”¨ç®€åŒ–ç‰ˆä»£ç†ä¸­é—´ä»¶ï¼Œå¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­æ›¿æ¢é»˜è®¤çš„ä»£ç†ä¸­é—´ä»¶ï¼š

``python
# settings.py
MIDDLEWARES = [
    # æ³¨é‡Šæ‰å¤æ‚ç‰ˆä»£ç†ä¸­é—´ä»¶
    # 'crawlo.middleware.proxy.ProxyMiddleware',
    # å¯ç”¨ç®€åŒ–ç‰ˆä»£ç†ä¸­é—´ä»¶
    'crawlo.middleware.simple_proxy.SimpleProxyMiddleware',
]

# é…ç½®ä»£ç†åˆ—è¡¨
PROXY_ENABLED = True
PROXY_LIST = [
    "http://proxy1.example.com:8080",
    "http://proxy2.example.com:8080",
]
```

æœ‰å…³ä»£ç†ä¸­é—´ä»¶çš„è¯¦ç»†ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒ[ä»£ç†ä¸­é—´ä»¶ç¤ºä¾‹é¡¹ç›®](examples/simple_proxy_example/)ã€‚

### ç®¡é“ç³»ç»Ÿ
å¯æ‰©å±•çš„æ•°æ®å¤„ç†ç®¡é“ï¼Œæ”¯æŒå¤šç§å­˜å‚¨æ–¹å¼ï¼ˆæ§åˆ¶å°ã€æ•°æ®åº“ç­‰ï¼‰å’Œå»é‡åŠŸèƒ½ï¼š
- **ConsolePipeline**: æ§åˆ¶å°è¾“å‡ºç®¡é“
- **JsonPipeline**: JSONæ–‡ä»¶å­˜å‚¨ç®¡é“
- **RedisDedupPipeline**: Rediså»é‡ç®¡é“ï¼ŒåŸºäºRedisé›†åˆå®ç°åˆ†å¸ƒå¼å»é‡
- **AsyncmyMySQLPipeline**: MySQLæ•°æ®åº“å­˜å‚¨ç®¡é“ï¼ŒåŸºäºasyncmyé©±åŠ¨

### æ‰©å±•ç»„ä»¶
åŠŸèƒ½å¢å¼ºæ‰©å±•ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€ç›‘æ§ã€æ€§èƒ½åˆ†æç­‰ï¼š
- **LogIntervalExtension**: å®šæ—¶æ—¥å¿—æ‰©å±•
- **LogStats**: ç»Ÿè®¡æ—¥å¿—æ‰©å±•
- **CustomLoggerExtension**: è‡ªå®šä¹‰æ—¥å¿—æ‰©å±•
- **MemoryMonitorExtension**: å†…å­˜ç›‘æ§æ‰©å±•ï¼ˆç›‘æ§çˆ¬è™«è¿›ç¨‹å†…å­˜ä½¿ç”¨æƒ…å†µï¼‰
- **PerformanceProfilerExtension**: æ€§èƒ½åˆ†ææ‰©å±•
- **HealthCheckExtension**: å¥åº·æ£€æŸ¥æ‰©å±•
- **RequestRecorderExtension**: è¯·æ±‚è®°å½•æ‰©å±•

### è¿‡æ»¤ç³»ç»Ÿ
æ™ºèƒ½å»é‡è¿‡æ»¤ï¼Œæ”¯æŒå¤šç§å»é‡ç­–ç•¥ï¼ˆå†…å­˜ã€Redisã€Bloom Filterï¼‰ã€‚

---

<!-- ç¤ºä¾‹é¡¹ç›® section -->
<h2 align="center">ğŸ“¦ ç¤ºä¾‹é¡¹ç›®</h2>

- [OFweekåˆ†å¸ƒå¼çˆ¬è™«](examples/ofweek_distributed/) - å¤æ‚çš„åˆ†å¸ƒå¼çˆ¬è™«ç¤ºä¾‹ï¼ŒåŒ…å«Rediså»é‡åŠŸèƒ½
- [OFweekç‹¬ç«‹çˆ¬è™«](examples/ofweek_standalone/) - ç‹¬ç«‹è¿è¡Œçš„çˆ¬è™«ç¤ºä¾‹
- [OFweekæ··åˆæ¨¡å¼çˆ¬è™«](examples/ofweek_spider/) - æ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼æ¨¡å¼åˆ‡æ¢çš„çˆ¬è™«ç¤ºä¾‹

---

<!-- æ–‡æ¡£ section -->
<h2 align="center">ğŸ“š æ–‡æ¡£</h2>

å®Œæ•´çš„æ–‡æ¡£è¯·è®¿é—® [Crawlo Documentation](https://crawlo.readthedocs.io/)

- [å¿«é€Ÿå¼€å§‹æŒ‡å—](docs/modules/index.md)
- [æ¨¡å—åŒ–æ–‡æ¡£](docs/modules/index.md)
- [æ ¸å¿ƒå¼•æ“æ–‡æ¡£](docs/modules/core/engine.md)
- [è°ƒåº¦å™¨æ–‡æ¡£](docs/modules/core/scheduler.md)
- [ä¸‹è½½å™¨æ–‡æ¡£](docs/modules/downloader/index.md)
- [ä¸­é—´ä»¶æ–‡æ¡£](docs/modules/middleware/index.md)
- [ç®¡é“æ–‡æ¡£](docs/modules/pipeline/index.md)
- [é˜Ÿåˆ—æ–‡æ¡£](docs/modules/queue/index.md)
- [è¿‡æ»¤å™¨æ–‡æ¡£](docs/modules/filter/index.md)
- [æ‰©å±•ç»„ä»¶æ–‡æ¡£](docs/modules/extension/index.md)

---

<!-- è´¡çŒ® section -->
<h2 align="center">ğŸ¤ è´¡çŒ®</h2>

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥å¸®åŠ©æ”¹è¿› Crawloï¼

---

<!-- è®¸å¯è¯ section -->
<h2 align="center">ğŸ“„ è®¸å¯è¯</h2>

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦æƒ…è¯·è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚
