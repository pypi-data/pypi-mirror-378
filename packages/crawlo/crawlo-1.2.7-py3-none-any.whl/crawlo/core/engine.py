#!/usr/bin/python
# -*- coding:UTF-8 -*-
import asyncio
import time
from inspect import iscoroutine
from typing import Optional, Generator, Callable

from crawlo import Request, Item
from crawlo.spider import Spider
from crawlo.utils.log import get_logger
from crawlo.exceptions import OutputError
from crawlo.core.scheduler import Scheduler
from crawlo.core.processor import Processor
from crawlo.task_manager import TaskManager
from crawlo.project import load_class
from crawlo.downloader import DownloaderBase
from crawlo.utils.func_tools import transform
from crawlo.event import spider_opened, spider_error, request_scheduled


class Engine(object):

    def __init__(self, crawler):
        self.running = False
        self.normal = True
        self.crawler = crawler
        self.settings = crawler.settings
        self.spider: Optional[Spider] = None
        self.downloader: Optional[DownloaderBase] = None
        self.scheduler: Optional[Scheduler] = None
        self.processor: Optional[Processor] = None
        self.start_requests: Optional[Generator] = None
        self.task_manager: Optional[TaskManager] = TaskManager(self.settings.get_int('CONCURRENCY'))

        # å¢å¼ºæ§åˆ¶å‚æ•°
        self.max_queue_size = self.settings.get_int('SCHEDULER_MAX_QUEUE_SIZE', 200)
        self.generation_batch_size = self.settings.get_int('REQUEST_GENERATION_BATCH_SIZE', 10)
        self.generation_interval = self.settings.get_float('REQUEST_GENERATION_INTERVAL', 0.05)
        self.backpressure_ratio = self.settings.get_float('BACKPRESSURE_RATIO', 0.8)  # é˜Ÿåˆ—è¾¾åˆ°80%æ—¶å¯åŠ¨èƒŒå‹
        
        # çŠ¶æ€è·Ÿè¸ª
        self._generation_paused = False
        self._last_generation_time = 0
        self._generation_stats = {
            'total_generated': 0,
            'backpressure_events': 0
        }

        self.logger = get_logger(name=self.__class__.__name__)

    def _get_downloader_cls(self):
        """è·å–ä¸‹è½½å™¨ç±»ï¼Œæ”¯æŒå¤šç§é…ç½®æ–¹å¼"""
        # æ–¹å¼1: ä½¿ç”¨ DOWNLOADER_TYPE ç®€åŒ–åç§°ï¼ˆæ¨èï¼‰
        downloader_type = self.settings.get('DOWNLOADER_TYPE')
        if downloader_type:
            try:
                from crawlo.downloader import get_downloader_class
                downloader_cls = get_downloader_class(downloader_type)
                self.logger.debug(f"ä½¿ç”¨ä¸‹è½½å™¨ç±»å‹: {downloader_type} -> {downloader_cls.__name__}")
                return downloader_cls
            except (ImportError, ValueError) as e:
                self.logger.warning(f"æ— æ³•ä½¿ç”¨ä¸‹è½½å™¨ç±»å‹ '{downloader_type}': {e}ï¼Œå›é€€åˆ°é»˜è®¤é…ç½®")
        
        # æ–¹å¼2: ä½¿ç”¨ DOWNLOADER å®Œæ•´ç±»è·¯å¾„ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
        downloader_cls = load_class(self.settings.get('DOWNLOADER'))
        if not issubclass(downloader_cls, DownloaderBase):
            raise TypeError(f'Downloader {downloader_cls.__name__} is not subclass of DownloaderBase.')
        return downloader_cls

    def engine_start(self):
        self.running = True
        # è·å–ç‰ˆæœ¬å·ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å€¼
        version = self.settings.get('VERSION', '1.0.0')
        if not version or version == 'None':
            version = '1.0.0'
        self.logger.info(
            f"Crawlo Started version {version} . "
            # f"(project name : {self.settings.get('PROJECT_NAME')})"
        )

    async def start_spider(self, spider):
        self.spider = spider

        self.scheduler = Scheduler.create_instance(self.crawler)
        if hasattr(self.scheduler, 'open'):
            if asyncio.iscoroutinefunction(self.scheduler.open):
                await self.scheduler.open()
            else:
                self.scheduler.open()

        downloader_cls = self._get_downloader_cls()
        self.downloader = downloader_cls(self.crawler)
        if hasattr(self.downloader, 'open'):
            if asyncio.iscoroutinefunction(self.downloader.open):
                self.downloader.open()
            else:
                # DownloaderBase.open() æ˜¯åŒæ­¥æ–¹æ³•ï¼Œç›´æ¥è°ƒç”¨è€Œä¸æ˜¯await
                self.downloader.open()

        self.processor = Processor(self.crawler)
        if hasattr(self.processor, 'open'):
            if asyncio.iscoroutinefunction(self.processor.open):
                await self.processor.open()
            else:
                # Processor.open() æ˜¯åŒæ­¥æ–¹æ³•
                self.processor.open()

        # åœ¨å¤„ç†å™¨åˆå§‹åŒ–ä¹‹ååˆå§‹åŒ–æ‰©å±•ç®¡ç†å™¨ï¼Œç¡®ä¿æ—¥å¿—è¾“å‡ºé¡ºåºæ­£ç¡®
        # ä¸­é—´ä»¶ -> ç®¡é“ -> æ‰©å±•
        if not hasattr(self.crawler, 'extension') or not self.crawler.extension:
            self.crawler.extension = self.crawler._create_extension()

        self.start_requests = iter(spider.start_requests())
        await self._open_spider()

    async def crawl(self):
        """
        Crawl the spider
        å¢å¼ºç‰ˆæœ¬æ”¯æŒæ™ºèƒ½è¯·æ±‚ç”Ÿæˆå’ŒèƒŒå‹æ§åˆ¶
        """
        generation_task = None
        
        try:
            # å¯åŠ¨è¯·æ±‚ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æœå¯ç”¨äº†å—æ§ç”Ÿæˆï¼‰
            if (self.start_requests and 
                self.settings.get_bool('ENABLE_CONTROLLED_REQUEST_GENERATION', False)):
                generation_task = asyncio.create_task(
                    self._controlled_request_generation()
                )
            else:
                # ä¼ ç»Ÿæ–¹å¼å¤„ç†å¯åŠ¨è¯·æ±‚
                generation_task = asyncio.create_task(
                    self._traditional_request_generation()
                )
            
            # ä¸»çˆ¬å–å¾ªç¯
            while self.running:
                # è·å–å¹¶å¤„ç†è¯·æ±‚
                if request := await self._get_next_request():
                    await self._crawl(request)
                
                # æ£€æŸ¥é€€å‡ºæ¡ä»¶
                if await self._should_exit():
                    break
                
                # çŸ­æš‚ä¼‘æ¯é¿å…å¿™ç­‰
                await asyncio.sleep(0.001)
        
        finally:
            # æ¸…ç†ç”Ÿæˆä»»åŠ¡
            if generation_task and not generation_task.done():
                generation_task.cancel()
                try:
                    await generation_task
                except asyncio.CancelledError:
                    pass
            
            await self.close_spider()

    async def _traditional_request_generation(self):
        """ä¼ ç»Ÿçš„è¯·æ±‚ç”Ÿæˆæ–¹å¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        while self.running:
            try:
                start_request = next(self.start_requests)
                # è¯·æ±‚å…¥é˜Ÿ
                await self.enqueue_request(start_request)
            except StopIteration:
                self.start_requests = None
                break
            except Exception as exp:
                # 1ã€å‘å»è¯·æ±‚çš„requestå…¨éƒ¨è¿è¡Œå®Œæ¯•
                # 2ã€è°ƒåº¦å™¨æ˜¯å¦ç©ºé—²
                # 3ã€ä¸‹è½½å™¨æ˜¯å¦ç©ºé—²
                if not await self._exit():
                    continue
                self.running = False
                if self.start_requests is not None:
                    self.logger.error(f"å¯åŠ¨è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(exp)}")
            await asyncio.sleep(0.001)

    async def _controlled_request_generation(self):
        """å—æ§çš„è¯·æ±‚ç”Ÿæˆï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰"""
        self.logger.info("ğŸ›ï¸ å¯åŠ¨å—æ§è¯·æ±‚ç”Ÿæˆ")
        
        batch = []
        total_generated = 0
        
        try:
            for request in self.start_requests:
                batch.append(request)
                
                # æ‰¹é‡å¤„ç†
                if len(batch) >= self.generation_batch_size:
                    generated = await self._process_generation_batch(batch)
                    total_generated += generated
                    batch = []
                
                # èƒŒå‹æ£€æŸ¥
                if await self._should_pause_generation():
                    await self._wait_for_capacity()
            
            # å¤„ç†å‰©ä½™è¯·æ±‚
            if batch:
                generated = await self._process_generation_batch(batch)
                total_generated += generated
        
        except Exception as e:
            self.logger.error(f"âŒ è¯·æ±‚ç”Ÿæˆå¤±è´¥: {e}")
        
        finally:
            self.start_requests = None
            self.logger.info(f"ğŸ‰ è¯·æ±‚ç”Ÿæˆå®Œæˆï¼Œæ€»è®¡: {total_generated}")

    async def _process_generation_batch(self, batch) -> int:
        """å¤„ç†ä¸€æ‰¹è¯·æ±‚"""
        generated = 0
        
        for request in batch:
            if not self.running:
                break
            
            # ç­‰å¾…é˜Ÿåˆ—æœ‰ç©ºé—´
            while await self._is_queue_full() and self.running:
                await asyncio.sleep(0.1)
            
            if self.running:
                await self.enqueue_request(request)
                generated += 1
                self._generation_stats['total_generated'] += 1
            
            # æ§åˆ¶ç”Ÿæˆé€Ÿåº¦
            if self.generation_interval > 0:
                await asyncio.sleep(self.generation_interval)
        
        return generated

    async def _should_pause_generation(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æš‚åœç”Ÿæˆ"""
        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°
        if await self._is_queue_full():
            return True
        
        # æ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨è´Ÿè½½
        if self.task_manager:
            current_tasks = len(self.task_manager.current_task)
            if hasattr(self.task_manager, 'semaphore'):
                max_concurrency = getattr(self.task_manager.semaphore, '_initial_value', 8)
                if current_tasks >= max_concurrency * self.backpressure_ratio:
                    return True
        
        return False

    async def _is_queue_full(self) -> bool:
        """æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡"""
        if not self.scheduler:
            return False
        
        queue_size = len(self.scheduler)
        return queue_size >= self.max_queue_size * self.backpressure_ratio

    async def _wait_for_capacity(self):
        """ç­‰å¾…ç³»ç»Ÿæœ‰è¶³å¤Ÿå®¹é‡"""
        self._generation_stats['backpressure_events'] += 1
        self.logger.debug("â¸ï¸ è§¦å‘èƒŒå‹ï¼Œæš‚åœè¯·æ±‚ç”Ÿæˆ")
        
        wait_time = 0.1
        max_wait = 2.0
        
        while await self._should_pause_generation() and self.running:
            await asyncio.sleep(wait_time)
            wait_time = min(wait_time * 1.1, max_wait)

    async def _open_spider(self):
        asyncio.create_task(self.crawler.subscriber.notify(spider_opened))
        crawling = asyncio.create_task(self.crawl())
        await crawling

    async def _crawl(self, request):
        # TODO å®ç°å¹¶å‘
        async def crawl_task():
            outputs = await self._fetch(request)
            # TODO å¤„ç†output
            if outputs:
                await self._handle_spider_output(outputs)

        # ä½¿ç”¨å¼‚æ­¥ä»»åŠ¡åˆ›å»ºï¼Œéµå®ˆå¹¶å‘é™åˆ¶
        await self.task_manager.create_task(crawl_task())

    async def _fetch(self, request):
        async def _successful(_response):
            callback: Callable = request.callback or self.spider.parse
            if _outputs := callback(_response):
                if iscoroutine(_outputs):
                    await _outputs
                else:
                    return transform(_outputs, _response)

        _response = await self.downloader.fetch(request)
        if _response is None:
            return None
        output = await _successful(_response)
        return output

    async def enqueue_request(self, start_request):
        await self._schedule_request(start_request)

    async def _schedule_request(self, request):
        # TODO å»é‡
        if await self.scheduler.enqueue_request(request):
            asyncio.create_task(self.crawler.subscriber.notify(request_scheduled, request, self.crawler.spider))

    async def _get_next_request(self):
        return await self.scheduler.next_request()

    async def _handle_spider_output(self, outputs):
        async for spider_output in outputs:
            if isinstance(spider_output, (Request, Item)):
                await self.processor.enqueue(spider_output)
            elif isinstance(spider_output, Exception):
                asyncio.create_task(
                    self.crawler.subscriber.notify(spider_error, spider_output, self.spider)
                )
                raise spider_output
            else:
                raise OutputError(f'{type(self.spider)} must return `Request` or `Item`.')

    async def _exit(self):
        if self.scheduler.idle() and self.downloader.idle() and self.task_manager.all_done() and self.processor.idle():
            return True
        return False

    async def _should_exit(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥é€€å‡ºï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰"""
        # æ²¡æœ‰å¯åŠ¨è¯·æ±‚ï¼Œä¸”æ‰€æœ‰é˜Ÿåˆ—éƒ½ç©ºé—²
        if self.start_requests is None:
            # ä½¿ç”¨å¼‚æ­¥çš„idleæ£€æŸ¥æ–¹æ³•ä»¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœ
            scheduler_idle = await self.scheduler.async_idle() if hasattr(self.scheduler, 'async_idle') else self.scheduler.idle()
            
            if (scheduler_idle and 
                self.downloader.idle() and 
                self.task_manager.all_done() and 
                self.processor.idle()):
                # å¢åŠ é¢å¤–æ£€æŸ¥ç¡®ä¿æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆ
                await asyncio.sleep(0.1)  # çŸ­æš‚ç­‰å¾…ç¡®ä¿æ²¡æœ‰æ–°çš„ä»»åŠ¡åŠ å…¥
                if (await self.scheduler.async_idle() and 
                    self.downloader.idle() and 
                    self.task_manager.all_done() and 
                    self.processor.idle()):
                    return True
        
        return False

    async def close_spider(self):
        await asyncio.gather(*self.task_manager.current_task)
        await self.scheduler.close()
        await self.downloader.close()
        if self.normal:
            await self.crawler.close()
    
    def get_generation_stats(self) -> dict:
        """è·å–ç”Ÿæˆç»Ÿè®¡"""
        return {
            **self._generation_stats,
            'queue_size': len(self.scheduler) if self.scheduler else 0,
            'active_tasks': len(self.task_manager.current_task) if self.task_manager else 0
        }