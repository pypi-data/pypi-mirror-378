#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•é…ç½®ä¸€è‡´æ€§ä¼˜åŒ–
"""
import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from crawlo.project import get_settings
from crawlo.crawler import Crawler
from crawlo.spider import Spider
from crawlo.utils.log import get_logger
from crawlo import Request


class TestSpider(Spider):
    name = "test_spider"
    
    def start_requests(self):
        yield Request("https://example.com")


async def test_config_consistency():
    """æµ‹è¯•é…ç½®ä¸€è‡´æ€§ä¼˜åŒ–"""
    print("ğŸ” æµ‹è¯•é…ç½®ä¸€è‡´æ€§ä¼˜åŒ–...")
    
    # æ¨¡æ‹Ÿå•æœºæ¨¡å¼é…ç½®ä½†Rediså¯ç”¨çš„æƒ…å†µ
    custom_settings = {
        'QUEUE_TYPE': 'auto',  # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼
        'CONCURRENCY': 4,
        'DOWNLOAD_DELAY': 1.0,
        'LOG_LEVEL': 'INFO'
    }
    
    try:
        # è·å–é…ç½®
        settings = get_settings(custom_settings)
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = Crawler(TestSpider, settings)
        
        # å¯åŠ¨çˆ¬è™«ï¼ˆè¿™ä¼šè§¦å‘è°ƒåº¦å™¨åˆå§‹åŒ–ï¼‰
        print("ğŸš€ å¼€å§‹åˆå§‹åŒ–çˆ¬è™«...")
        await crawler.crawl()
        
        print("âœ… é…ç½®ä¸€è‡´æ€§æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•é…ç½®ä¸€è‡´æ€§ä¼˜åŒ–...")
    print("=" * 50)
    
    try:
        await test_config_consistency()
        
        print("=" * 50)
        print("ğŸ‰ é…ç½®ä¸€è‡´æ€§ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print("=" * 50)
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    import logging
    logging.basicConfig(level=logging.INFO)
    
    asyncio.run(main())