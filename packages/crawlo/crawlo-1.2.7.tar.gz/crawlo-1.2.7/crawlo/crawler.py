#!/usr/bin/python
# -*- coding: UTF-8 -*-
"""
Crawlo Crawler Module
====================
æä¾›çˆ¬è™«è¿›ç¨‹ç®¡ç†å’Œè¿è¡Œæ—¶æ ¸å¿ƒåŠŸèƒ½ã€‚

æ ¸å¿ƒç»„ä»¶:
- Crawler: å•ä¸ªçˆ¬è™«è¿è¡Œå®ä¾‹ï¼Œç®¡ç†Spiderä¸å¼•æ“çš„ç”Ÿå‘½å‘¨æœŸ
- CrawlerProcess: çˆ¬è™«è¿›ç¨‹ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šçˆ¬è™«å¹¶å‘è°ƒåº¦å’Œèµ„æºç®¡ç†

åŠŸèƒ½ç‰¹æ€§:
- æ™ºèƒ½å¹¶å‘æ§åˆ¶å’Œèµ„æºç®¡ç†
- ä¼˜é›…å…³é—­å’Œä¿¡å·å¤„ç†
- ç»Ÿè®¡ç›‘æ§å’Œæ€§èƒ½è¿½è¸ª
- è‡ªåŠ¨æ¨¡å—å‘ç°å’Œæ³¨å†Œ
- é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶
- å¤§è§„æ¨¡çˆ¬è™«ä¼˜åŒ–æ”¯æŒ

ç¤ºä¾‹ç”¨æ³•:
    # å•ä¸ªçˆ¬è™«è¿è¡Œ
    crawler = Crawler(MySpider, settings)
    await crawler.crawl()
    
    # å¤šçˆ¬è™«å¹¶å‘ç®¡ç†
    process = CrawlerProcess()
    await process.crawl([Spider1, Spider2])
"""
from __future__ import annotations
import asyncio
import signal
import time
import threading
from typing import Type, Optional, Set, List, Union, Dict, Any
from .spider import Spider, get_global_spider_registry
from .core.engine import Engine
from .utils.log import get_logger
from .subscriber import Subscriber
from .extension import ExtensionManager
from .stats_collector import StatsCollector
from .event import spider_opened, spider_closed
from .settings.setting_manager import SettingManager
from crawlo.project import merge_settings, get_settings


logger = get_logger(__name__)


class CrawlerContext:
    """
    çˆ¬è™«ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    æä¾›å…±äº«çŠ¶æ€å’Œèµ„æºç®¡ç†
    """
    
    def __init__(self):
        self.start_time = time.time()
        self.total_crawlers = 0
        self.active_crawlers = 0
        self.completed_crawlers = 0
        self.failed_crawlers = 0
        self.error_log = []
        self._lock = threading.RLock()
        
    def increment_total(self):
        with self._lock:
            self.total_crawlers += 1
            
    def increment_active(self):
        with self._lock:
            self.active_crawlers += 1
            
    def decrement_active(self):
        with self._lock:
            self.active_crawlers -= 1
            
    def increment_completed(self):
        with self._lock:
            self.completed_crawlers += 1
            
    def increment_failed(self, error: str):
        with self._lock:
            self.failed_crawlers += 1
            self.error_log.append({
                'timestamp': time.time(),
                'error': error
            })
            
    def get_stats(self) -> Dict[str, Any]:
        with self._lock:
            duration = time.time() - self.start_time
            return {
                'total_crawlers': self.total_crawlers,
                'active_crawlers': self.active_crawlers,
                'completed_crawlers': self.completed_crawlers,
                'failed_crawlers': self.failed_crawlers,
                'success_rate': (self.completed_crawlers / max(1, self.total_crawlers)) * 100,
                'duration_seconds': round(duration, 2),
                'error_count': len(self.error_log)
            }


class Crawler:
    """
    å•ä¸ªçˆ¬è™«è¿è¡Œå®ä¾‹ï¼Œç®¡ç† Spider ä¸å¼•æ“çš„ç”Ÿå‘½å‘¨æœŸ
    
    æä¾›åŠŸèƒ½:
    - Spider ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆåˆå§‹åŒ–ã€è¿è¡Œã€å…³é—­ï¼‰
    - å¼•æ“ç»„ä»¶çš„åè°ƒç®¡ç†
    - é…ç½®åˆå¹¶å’ŒéªŒè¯
    - ç»Ÿè®¡æ•°æ®æ”¶é›†
    - æ‰©å±•ç®¡ç†
    - å¼‚å¸¸å¤„ç†å’Œæ¸…ç†
    """

    def __init__(self, spider_cls: Type[Spider], settings: SettingManager, context: Optional[CrawlerContext] = None):
        self.spider_cls = spider_cls
        self.spider: Optional[Spider] = None
        self.engine: Optional[Engine] = None
        self.stats: Optional[StatsCollector] = None
        self.subscriber: Optional[Subscriber] = None
        self.extension: Optional[ExtensionManager] = None
        self.settings: SettingManager = settings.copy()
        self.context = context or CrawlerContext()
        
        # çŠ¶æ€ç®¡ç†
        self._closed = False
        self._close_lock = asyncio.Lock()
        self._start_time = None
        self._end_time = None
        
        # æ€§èƒ½ç›‘æ§
        self._performance_metrics = {
            'initialization_time': 0,
            'crawl_duration': 0,
            'memory_peak': 0,
            'request_count': 0,
            'error_count': 0
        }

    async def crawl(self):
        """
        å¯åŠ¨çˆ¬è™«æ ¸å¿ƒæµç¨‹
        
        åŒ…å«ä»¥ä¸‹é˜¶æ®µ:
        1. åˆå§‹åŒ–é˜¶æ®µ: åˆ›å»ºæ‰€æœ‰ç»„ä»¶
        2. éªŒè¯é˜¶æ®µ: æ£€æŸ¥é…ç½®å’ŒçŠ¶æ€
        3. è¿è¡Œé˜¶æ®µ: å¯åŠ¨çˆ¬è™«å¼•æ“
        4. æ¸…ç†é˜¶æ®µ: èµ„æºé‡Šæ”¾
        """
        init_start = time.time()
        self._start_time = init_start
        
        try:
            # æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€
            self.context.increment_active()
            
            # é˜¶æ®µ 1: åˆå§‹åŒ–ç»„ä»¶
            # è°ƒæ•´ç»„ä»¶åˆå§‹åŒ–é¡ºåºï¼Œç¡®ä¿æ—¥å¿—è¾“å‡ºé¡ºåºç¬¦åˆè¦æ±‚
            self.subscriber = self._create_subscriber()
            self.spider = self._create_spider()
            self.engine = self._create_engine() 
            self.stats = self._create_stats()
            # æ³¨æ„ï¼šè¿™é‡Œä¸åˆå§‹åŒ–æ‰©å±•ç®¡ç†å™¨ï¼Œè®©å®ƒåœ¨å¼•æ“ä¸­åˆå§‹åŒ–
            
            # è®°å½•åˆå§‹åŒ–æ—¶é—´
            self._performance_metrics['initialization_time'] = time.time() - init_start
            
            # é˜¶æ®µ 2: éªŒè¯çŠ¶æ€
            self._validate_crawler_state()
            
            # é˜¶æ®µ 3: æ˜¾ç¤ºè¿è¡Œé…ç½®æ‘˜è¦
            self._log_runtime_summary()
            
            # é˜¶æ®µ 4: å¯åŠ¨çˆ¬è™«
            crawl_start = time.time()
            await self.engine.start_spider(self.spider)
            
            # è®°å½•çˆ¬å–æ—¶é—´
            self._performance_metrics['crawl_duration'] = time.time() - crawl_start
            self._end_time = time.time()
            
            # æ›´æ–°ä¸Šä¸‹æ–‡çŠ¶æ€
            self.context.increment_completed()
            
            logger.info(f"çˆ¬è™« {self.spider.name} å®Œæˆï¼Œè€—æ—¶ {self._get_total_duration():.2f}ç§’")
            
        except Exception as e:
            self._performance_metrics['error_count'] += 1
            self.context.increment_failed(str(e))
            logger.error(f"çˆ¬è™« {getattr(self.spider, 'name', 'Unknown')} è¿è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
        finally:
            self.context.decrement_active()
            # ç¡®ä¿èµ„æºæ¸…ç†
            await self._ensure_cleanup()

    def _log_runtime_summary(self):
        """è®°å½•è¿è¡Œæ—¶é…ç½®æ‘˜è¦"""
        # è·å–çˆ¬è™«åç§°
        spider_name = getattr(self.spider, 'name', 'Unknown')
        
        # æ˜¾ç¤ºç®€åŒ–çš„è¿è¡Œæ—¶ä¿¡æ¯ï¼Œé¿å…ä¸é¡¹ç›®åˆå§‹åŒ–é‡å¤
        logger.info(f"ğŸ•·ï¸  å¼€å§‹è¿è¡Œçˆ¬è™«: {spider_name}")
        
        # æ³¨æ„ï¼šå¹¶å‘æ•°å’Œä¸‹è½½å»¶è¿Ÿä¿¡æ¯å·²åœ¨å…¶ä»–åœ°æ–¹æ˜¾ç¤ºï¼Œé¿å…é‡å¤
        # å¦‚æœéœ€è¦æ˜¾ç¤ºå…¶ä»–è¿è¡Œæ—¶ç‰¹å®šä¿¡æ¯ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 

    def _validate_crawler_state(self):
        """
        éªŒè¯çˆ¬è™«çŠ¶æ€å’Œé…ç½®
        ç¡®ä¿æ‰€æœ‰å¿…è¦ç»„ä»¶éƒ½å·²æ­£ç¡®åˆå§‹åŒ–
        """
        if not self.spider:
            raise RuntimeError("çˆ¬è™«å®ä¾‹æœªåˆå§‹åŒ–")
        if not self.engine:
            raise RuntimeError("å¼•æ“æœªåˆå§‹åŒ–")
        if not self.stats:
            raise RuntimeError("ç»Ÿè®¡æ”¶é›†å™¨æœªåˆå§‹åŒ–")
        if not self.subscriber:
            raise RuntimeError("äº‹ä»¶è®¢é˜…å™¨æœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥å…³é”®é…ç½®
        if not self.spider.name:
            raise ValueError("çˆ¬è™«åç§°ä¸èƒ½ä¸ºç©º")
            
        logger.debug(f"çˆ¬è™« {self.spider.name} çŠ¶æ€éªŒè¯é€šè¿‡")
    
    def _get_total_duration(self) -> float:
        """è·å–æ€»è¿è¡Œæ—¶é—´"""
        if self._start_time and self._end_time:
            return self._end_time - self._start_time
        return 0.0
    
    async def _ensure_cleanup(self):
        """ç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            if not self._closed:
                await self.close()
        except Exception as e:
            logger.warning(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        metrics = self._performance_metrics.copy()
        metrics['total_duration'] = self._get_total_duration()
        if self.stats:
            # æ·»åŠ ç»Ÿè®¡æ•°æ®
            stats_data = getattr(self.stats, 'get_stats', lambda: {})()
            metrics.update(stats_data)
        return metrics
    @staticmethod
    def _create_subscriber() -> Subscriber:
        """åˆ›å»ºäº‹ä»¶è®¢é˜…å™¨"""
        return Subscriber()

    def _create_spider(self) -> Spider:
        """
        åˆ›å»ºå¹¶éªŒè¯çˆ¬è™«å®ä¾‹ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æ‰§è¡Œä»¥ä¸‹éªŒè¯:
        - çˆ¬è™«åç§°å¿…é¡»å­˜åœ¨
        - start_requests æ–¹æ³•å¿…é¡»å¯è°ƒç”¨
        - start_urls ä¸èƒ½æ˜¯å­—ç¬¦ä¸²
        - parse æ–¹æ³•å»ºè®®å­˜åœ¨
        """
        spider = self.spider_cls.create_instance(self)

        # å¿…è¦å±æ€§æ£€æŸ¥
        if not getattr(spider, 'name', None):
            raise AttributeError(
                f"çˆ¬è™«ç±» '{self.spider_cls.__name__}' å¿…é¡»å®šä¹‰ 'name' å±æ€§ã€‚\n"
                f"ç¤ºä¾‹: name = 'my_spider'"
            )

        if not callable(getattr(spider, 'start_requests', None)):
            raise AttributeError(
                f"çˆ¬è™« '{spider.name}' å¿…é¡»å®ç°å¯è°ƒç”¨çš„ 'start_requests' æ–¹æ³•ã€‚\n"
                f"ç¤ºä¾‹: def start_requests(self): yield Request(url='...')"
            )

        # start_urls ç±»å‹æ£€æŸ¥
        start_urls = getattr(spider, 'start_urls', [])
        if isinstance(start_urls, str):
            raise TypeError(
                f"çˆ¬è™« '{spider.name}' çš„ 'start_urls' å¿…é¡»æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸²ã€‚\n"
                f"æ­£ç¡®å†™æ³•: start_urls = ['http://example.com']\n"
                f"é”™è¯¯å†™æ³•: start_urls = 'http://example.com'"
            )

        # parse æ–¹æ³•æ£€æŸ¥ï¼ˆè­¦å‘Šè€Œéé”™è¯¯ï¼‰
        if not callable(getattr(spider, 'parse', None)):
            logger.warning(
                f"çˆ¬è™« '{spider.name}' æœªå®šä¹‰ 'parse' æ–¹æ³•ã€‚\n"
                f"è¯·ç¡®ä¿æ‰€æœ‰ Request éƒ½æŒ‡å®šäº†å›è°ƒå‡½æ•°ï¼Œå¦åˆ™å“åº”å°†è¢«å¿½ç•¥ã€‚"
            )
        
        # è®¾ç½®çˆ¬è™«é…ç½®
        self._set_spider(spider)
        
        logger.debug(f"çˆ¬è™« '{spider.name}' åˆå§‹åŒ–å®Œæˆ")
        return spider

    def _create_engine(self) -> Engine:
        """åˆ›å»ºå¹¶åˆå§‹åŒ–å¼•æ“"""
        engine = Engine(self)
        engine.engine_start()
        logger.debug(f"å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œçˆ¬è™«: {getattr(self.spider, 'name', 'Unknown')}")
        return engine

    def _create_stats(self) -> StatsCollector:
        """åˆ›å»ºç»Ÿè®¡æ”¶é›†å™¨"""
        stats = StatsCollector(self)
        logger.debug(f"ç»Ÿè®¡æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆï¼Œçˆ¬è™«: {getattr(self.spider, 'name', 'Unknown')}")
        return stats

    def _create_extension(self) -> ExtensionManager:
        """åˆ›å»ºæ‰©å±•ç®¡ç†å™¨"""
        # ä¿®æ”¹æ‰©å±•ç®¡ç†å™¨çš„åˆ›å»ºæ–¹å¼ï¼Œå»¶è¿Ÿåˆå§‹åŒ–ç›´åˆ°éœ€è¦æ—¶
        extension = ExtensionManager.create_instance(self)
        logger.debug(f"æ‰©å±•ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œçˆ¬è™«: {getattr(self.spider, 'name', 'Unknown')}")
        return extension

    def _set_spider(self, spider: Spider):
        """
        è®¾ç½®çˆ¬è™«é…ç½®å’Œäº‹ä»¶è®¢é˜…
        å°†çˆ¬è™«çš„ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ä¸è®¢é˜…å™¨ç»‘å®š
        """
        # è®¢é˜…çˆ¬è™«ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
        self.subscriber.subscribe(spider.spider_opened, event=spider_opened)
        self.subscriber.subscribe(spider.spider_closed, event=spider_closed)
        
        # åˆå¹¶çˆ¬è™«è‡ªå®šä¹‰é…ç½®
        merge_settings(spider, self.settings)
        
        logger.debug(f"çˆ¬è™« '{spider.name}' é…ç½®åˆå¹¶å®Œæˆ")

    async def close(self, reason='finished') -> None:
        """
        å…³é—­çˆ¬è™«å¹¶æ¸…ç†èµ„æºï¼ˆå¢å¼ºç‰ˆï¼‰
        
        ç¡®ä¿åªå…³é—­ä¸€æ¬¡ï¼Œå¹¶å¤„ç†æ‰€æœ‰æ¸…ç†æ“ä½œ
        """
        async with self._close_lock:
            if self._closed:
                return
            
            self._closed = True
            self._end_time = time.time()
            
            try:
                # é€šçŸ¥çˆ¬è™«å…³é—­äº‹ä»¶
                if self.subscriber:
                    await self.subscriber.notify(spider_closed)
                
                # ç»Ÿè®¡æ•°æ®æ”¶é›†
                if self.stats and self.spider:
                    self.stats.close_spider(spider=self.spider, reason=reason)
                    # è®°å½•ç»Ÿè®¡æ•°æ®
                    try:
                        from crawlo.commands.stats import record_stats
                        record_stats(self)
                    except ImportError:
                        logger.debug("ç»Ÿè®¡è®°å½•æ¨¡å—ä¸å­˜åœ¨ï¼Œè·³è¿‡ç»Ÿè®¡è®°å½•")
                
                logger.info(
                    f"çˆ¬è™« '{getattr(self.spider, 'name', 'Unknown')}' å·²å…³é—­ï¼Œ"
                    f"åŸå› : {reason}ï¼Œè€—æ—¶: {self._get_total_duration():.2f}ç§’"
                )
                
            except Exception as e:
                logger.error(f"å…³é—­çˆ¬è™«æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            finally:
                # ç¡®ä¿èµ„æºæ¸…ç†
                await self._cleanup_resources()
    
    async def _cleanup_resources(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        cleanup_tasks = []
        
        # å¼•æ“æ¸…ç†
        if self.engine:
            try:
                cleanup_tasks.append(self.engine.close())
            except AttributeError:
                pass  # å¼•æ“æ²¡æœ‰closeæ–¹æ³•
        
        # æ‰©å±•æ¸…ç†
        if self.extension:
            try:
                cleanup_tasks.append(self.extension.close())
            except AttributeError:
                pass
        
        # ç»Ÿè®¡æ”¶é›†å™¨æ¸…ç†
        if self.stats:
            try:
                cleanup_tasks.append(self.stats.close())
            except AttributeError:
                pass
        
        # å¹¶å‘æ‰§è¡Œæ¸…ç†ä»»åŠ¡
        if cleanup_tasks:
            await asyncio.gather(*cleanup_tasks, return_exceptions=True)
            
        logger.debug("èµ„æºæ¸…ç†å®Œæˆ")


class CrawlerProcess:
    """
    çˆ¬è™«è¿›ç¨‹ç®¡ç†å™¨
    
    æ”¯æŒåŠŸèƒ½:
    - å¤šçˆ¬è™«å¹¶å‘è°ƒåº¦å’Œèµ„æºç®¡ç†
    - è‡ªåŠ¨æ¨¡å—å‘ç°å’Œçˆ¬è™«æ³¨å†Œ
    - æ™ºèƒ½å¹¶å‘æ§åˆ¶å’Œè´Ÿè½½å‡è¡¡
    - ä¼˜é›…å…³é—­å’Œä¿¡å·å¤„ç†
    - å®æ—¶çŠ¶æ€ç›‘æ§å’Œç»Ÿè®¡
    - é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶
    - å¤§è§„æ¨¡çˆ¬è™«ä¼˜åŒ–æ”¯æŒ
    
    ä½¿ç”¨ç¤ºä¾‹:
        # åŸºæœ¬ç”¨æ³•
        process = CrawlerProcess()
        await process.crawl(MySpider)
        
        # å¤šçˆ¬è™«å¹¶å‘
        await process.crawl([Spider1, Spider2, 'spider_name'])
        
        # è‡ªå®šä¹‰å¹¶å‘æ•°
        process = CrawlerProcess(max_concurrency=8)
    """

    def __init__(
        self,
        settings: Optional[SettingManager] = None,
        max_concurrency: Optional[int] = None,
        spider_modules: Optional[List[str]] = None,
        enable_monitoring: bool = True
    ):
        # åŸºç¡€é…ç½®
        self.settings: SettingManager = settings or self._get_default_settings()
        self.crawlers: Set[Crawler] = set()
        self._active_tasks: Set[asyncio.Task] = set()
        
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context = CrawlerContext()
        
        # å¹¶å‘æ§åˆ¶é…ç½®
        self.max_concurrency: int = (
            max_concurrency
            or self.settings.get('MAX_RUNNING_SPIDERS')
            or self.settings.get('CONCURRENCY', 3)
        )
        self.semaphore = asyncio.Semaphore(self.max_concurrency)
        
        # ç›‘æ§é…ç½®
        self.enable_monitoring = enable_monitoring
        self._monitoring_task = None
        self._shutdown_event = asyncio.Event()
        
        # è‡ªåŠ¨å‘ç°å¹¶å¯¼å…¥çˆ¬è™«æ¨¡å—
        if spider_modules:
            self.auto_discover(spider_modules)

        # ä½¿ç”¨å…¨å±€æ³¨å†Œè¡¨çš„å¿«ç…§ï¼ˆé¿å…åç»­å¯¼å…¥å½±å“ï¼‰
        self._spider_registry: Dict[str, Type[Spider]] = get_global_spider_registry()
        
        # æ€§èƒ½ç›‘æ§
        self._performance_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'memory_usage_mb': 0,
            'cpu_usage_percent': 0
        }

        # æ³¨å†Œä¿¡å·é‡
        signal.signal(signal.SIGINT, self._shutdown)
        signal.signal(signal.SIGTERM, self._shutdown)

        self._log_startup_info()
        
        logger.debug(
            f"CrawlerProcess åˆå§‹åŒ–å®Œæˆ\n"
            f"  - æœ€å¤§å¹¶è¡Œçˆ¬è™«æ•°: {self.max_concurrency}\n"
            f"  - å·²æ³¨å†Œçˆ¬è™«æ•°: {len(self._spider_registry)}\n"
            f"  - ç›‘æ§å¯ç”¨: {self.enable_monitoring}"
        )

    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§ä»»åŠ¡"""
        if not self.enable_monitoring:
            return
            
        self._monitoring_task = asyncio.create_task(self._monitor_loop())
        logger.debug("ç›‘æ§ä»»åŠ¡å·²å¯åŠ¨")
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§ä»»åŠ¡"""
        if self._monitoring_task and not self._monitoring_task.done():
            self._monitoring_task.cancel()
            try:
                await self._monitoring_task
            except asyncio.CancelledError:
                pass
            logger.debug("ç›‘æ§ä»»åŠ¡å·²åœæ­¢")
    
    async def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯ï¼Œå®šæœŸæ”¶é›†å’ŒæŠ¥å‘ŠçŠ¶æ€"""
        try:
            while not self._shutdown_event.is_set():
                await self._collect_performance_stats()
                
                # æ¯30ç§’è¾“å‡ºä¸€æ¬¡çŠ¶æ€
                stats = self.context.get_stats()
                if stats['active_crawlers'] > 0:
                    logger.debug(
                        f"çˆ¬è™«çŠ¶æ€: æ´»è·ƒ {stats['active_crawlers']}, "
                        f"å®Œæˆ {stats['completed_crawlers']}, "
                        f"å¤±è´¥ {stats['failed_crawlers']}, "
                        f"æˆåŠŸç‡ {stats['success_rate']:.1f}%"
                    )
                
                await asyncio.sleep(30)  # 30ç§’é—´éš”
                
        except asyncio.CancelledError:
            logger.debug("ç›‘æ§å¾ªç¯è¢«å–æ¶ˆ")
        except Exception as e:
            logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}", exc_info=True)
    
    async def _collect_performance_stats(self):
        """æ”¶é›†æ€§èƒ½ç»Ÿè®¡æ•°æ®"""
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            
            self._performance_stats.update({
                'memory_usage_mb': round(memory_info.rss / 1024 / 1024, 2),
                'cpu_usage_percent': round(process.cpu_percent(), 2)
            })
            
        except ImportError:
            # psutil ä¸å­˜åœ¨æ—¶è·³è¿‡æ€§èƒ½ç›‘æ§
            pass
        except Exception as e:
            logger.debug(f"æ”¶é›†æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")
    @staticmethod
    def auto_discover(modules: List[str]):
        """
        è‡ªåŠ¨å¯¼å…¥æ¨¡å—ï¼Œè§¦å‘ Spider ç±»å®šä¹‰å’Œæ³¨å†Œï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æ”¯æŒé€’å½’æ‰«æå’Œé”™è¯¯æ¢å¤
        """
        import importlib
        import pkgutil
        
        discovered_count = 0
        error_count = 0
        
        for module_name in modules:
            try:
                module = importlib.import_module(module_name)
                
                if hasattr(module, '__path__'):
                    # åŒ…æ¨¡å—ï¼Œé€’å½’æ‰«æ
                    for _, name, _ in pkgutil.walk_packages(module.__path__, module.__name__ + "."):
                        try:
                            importlib.import_module(name)
                            discovered_count += 1
                        except Exception as sub_e:
                            error_count += 1
                            logger.warning(f"å¯¼å…¥å­æ¨¡å— {name} å¤±è´¥: {sub_e}")
                else:
                    # å•ä¸ªæ¨¡å—
                    importlib.import_module(module_name)
                    discovered_count += 1
                    
                logger.debug(f"å·²æ‰«ææ¨¡å—: {module_name}")
                
            except Exception as e:
                error_count += 1
                logger.error(f"æ‰«ææ¨¡å— {module_name} å¤±è´¥: {e}", exc_info=True)
        
        logger.debug(
            f"çˆ¬è™«æ³¨å†Œå®Œæˆ: æˆåŠŸ {discovered_count} ä¸ªï¼Œå¤±è´¥ {error_count} ä¸ª"
        )

    # === å…¬å…±åªè¯»æ¥å£ï¼šé¿å…ç›´æ¥è®¿é—® _spider_registry ===

    def get_spider_names(self) -> List[str]:
        """è·å–æ‰€æœ‰å·²æ³¨å†Œçš„çˆ¬è™«åç§°"""
        return list(self._spider_registry.keys())

    def get_spider_class(self, name: str) -> Optional[Type[Spider]]:
        """æ ¹æ® name è·å–çˆ¬è™«ç±»"""
        return self._spider_registry.get(name)

    def is_spider_registered(self, name: str) -> bool:
        """æ£€æŸ¥æŸä¸ª name æ˜¯å¦å·²æ³¨å†Œ"""
        return name in self._spider_registry

    async def crawl(self, spiders: Union[Type[Spider], str, List[Union[Type[Spider], str]]]):
        """
        å¯åŠ¨ä¸€ä¸ªæˆ–å¤šä¸ªçˆ¬è™«
        
        å¢å¼ºåŠŸèƒ½:
        - æ™ºèƒ½å¹¶å‘æ§åˆ¶
        - å®æ—¶ç›‘æ§å’Œç»Ÿè®¡
        - é”™è¯¯æ¢å¤å’Œé‡è¯•
        - ä¼˜é›…å…³é—­å¤„ç†
        """
        # é˜¶æ®µ 1: é¢„å¤„ç†å’ŒéªŒè¯
        spider_classes_to_run = self._resolve_spiders_to_run(spiders)
        total = len(spider_classes_to_run)

        if total == 0:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªçˆ¬è™«ç±»æˆ–åç§°")

        # é˜¶æ®µ 2: åˆå§‹åŒ–ä¸Šä¸‹æ–‡å’Œç›‘æ§
        for _ in range(total):
            self.context.increment_total()
        
        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        await self.start_monitoring()
        
        try:
            # é˜¶æ®µ 3: æŒ‰ç±»åæ’åºï¼Œä¿è¯å¯åŠ¨é¡ºåºå¯é¢„æµ‹
            spider_classes_to_run.sort(key=lambda cls: cls.__name__.lower())
            
            logger.debug(
                f"å¼€å§‹å¯åŠ¨ {total} ä¸ªçˆ¬è™«\n"
                f"  - æœ€å¤§å¹¶å‘æ•°: {self.max_concurrency}\n"
                f"  - çˆ¬è™«åˆ—è¡¨: {[cls.__name__ for cls in spider_classes_to_run]}"
            )

            # é˜¶æ®µ 4: æµå¼å¯åŠ¨æ‰€æœ‰çˆ¬è™«ä»»åŠ¡
            tasks = [
                asyncio.create_task(
                    self._run_spider_with_limit(spider_cls, index + 1, total),
                    name=f"spider-{spider_cls.__name__}-{index+1}"
                )
                for index, spider_cls in enumerate(spider_classes_to_run)
            ]

            # é˜¶æ®µ 5: ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆå¤±è´¥ä¸ä¸­æ–­ï¼‰
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # é˜¶æ®µ 6: ç»Ÿè®¡å¼‚å¸¸å’Œç»“æœ
            failed = [i for i, r in enumerate(results) if isinstance(r, Exception)]
            successful = total - len(failed)
            
            if failed:
                failed_spiders = [spider_classes_to_run[i].__name__ for i in failed]
                logger.error(
                    f"çˆ¬è™«æ‰§è¡Œç»“æœ: æˆåŠŸ {successful}/{total}ï¼Œå¤±è´¥ {len(failed)}/{total}\n"
                    f"  - å¤±è´¥çˆ¬è™«: {failed_spiders}"
                )
                
                # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                for i in failed:
                    error = results[i]
                    logger.error(f"çˆ¬è™« {spider_classes_to_run[i].__name__} é”™è¯¯è¯¦æƒ…: {error}")
            else:
                logger.info(f"æ‰€æœ‰ {total} ä¸ªçˆ¬è™«å‡æˆåŠŸå®Œæˆ! ğŸ‰")
            
            # è¿”å›ç»Ÿè®¡ç»“æœ
            return {
                'total': total,
                'successful': successful,
                'failed': len(failed),
                'success_rate': (successful / total) * 100 if total > 0 else 0,
                'context_stats': self.context.get_stats()
            }
            
        finally:
            # é˜¶æ®µ 7: æ¸…ç†å’Œå…³é—­
            await self.stop_monitoring()
            await self._cleanup_process()

    async def _cleanup_process(self):
        """æ¸…ç†è¿›ç¨‹èµ„æº"""
        try:
            # ç­‰å¾…æ‰€æœ‰æ´»è·ƒçˆ¬è™«å®Œæˆ
            if self.crawlers:
                close_tasks = [crawler.close() for crawler in self.crawlers]
                await asyncio.gather(*close_tasks, return_exceptions=True)
                self.crawlers.clear()
            
            # æ¸…ç†æ´»è·ƒä»»åŠ¡
            if self._active_tasks:
                for task in list(self._active_tasks):
                    if not task.done():
                        task.cancel()
                await asyncio.gather(*self._active_tasks, return_exceptions=True)
                self._active_tasks.clear()
            
            logger.debug("è¿›ç¨‹èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†è¿›ç¨‹èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
    
    def get_process_stats(self) -> Dict[str, Any]:
        """è·å–è¿›ç¨‹ç»Ÿè®¡ä¿¡æ¯"""
        context_stats = self.context.get_stats()
        
        return {
            'context': context_stats,
            'performance': self._performance_stats.copy(),
            'crawlers': {
                'total_registered': len(self._spider_registry),
                'active_crawlers': len(self.crawlers),
                'max_concurrency': self.max_concurrency
            },
            'registry': {
                'spider_names': list(self._spider_registry.keys()),
                'spider_classes': [cls.__name__ for cls in self._spider_registry.values()]
            }
        }
    def _resolve_spiders_to_run(
        self,
        spiders_input: Union[Type[Spider], str, List[Union[Type[Spider], str]]]
    ) -> List[Type[Spider]]:
        """
        è§£æè¾“å…¥ä¸ºçˆ¬è™«ç±»åˆ—è¡¨
        
        æ”¯æŒå„ç§è¾“å…¥æ ¼å¼å¹¶éªŒè¯å”¯ä¸€æ€§
        """
        inputs = self._normalize_inputs(spiders_input)
        seen_spider_names: Set[str] = set()
        spider_classes: List[Type[Spider]] = []
        
        for item in inputs:
            try:
                spider_cls = self._resolve_spider_class(item)
                spider_name = getattr(spider_cls, 'name', None)
                
                if not spider_name:
                    raise ValueError(f"çˆ¬è™«ç±» {spider_cls.__name__} ç¼ºå°‘ 'name' å±æ€§")

                if spider_name in seen_spider_names:
                    raise ValueError(
                        f"æœ¬æ¬¡è¿è¡Œä¸­çˆ¬è™«åç§° '{spider_name}' é‡å¤ã€‚\n"
                        f"è¯·ç¡®ä¿æ¯ä¸ªçˆ¬è™«çš„ name å±æ€§åœ¨æœ¬æ¬¡è¿è¡Œä¸­å”¯ä¸€ã€‚"
                    )

                seen_spider_names.add(spider_name)
                spider_classes.append(spider_cls)
                
                logger.debug(f"è§£æçˆ¬è™«æˆåŠŸ: {item} -> {spider_cls.__name__} (name='{spider_name}')")
                
            except Exception as e:
                logger.error(f"è§£æçˆ¬è™«å¤±è´¥: {item} - {e}")
                raise

        return spider_classes

    @staticmethod
    def _normalize_inputs(spiders_input) -> List[Union[Type[Spider], str]]:
        """
        æ ‡å‡†åŒ–è¾“å…¥ä¸ºåˆ—è¡¨
        
        æ”¯æŒæ›´å¤šè¾“å…¥ç±»å‹å¹¶æä¾›æ›´å¥½çš„é”™è¯¯ä¿¡æ¯
        """
        if isinstance(spiders_input, (type, str)):
            return [spiders_input]
        elif isinstance(spiders_input, (list, tuple, set)):
            spider_list = list(spiders_input)
            if not spider_list:
                raise ValueError("çˆ¬è™«åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            return spider_list
        else:
            raise TypeError(
                f"spiders å‚æ•°ç±»å‹ä¸æ”¯æŒ: {type(spiders_input)}\n"
                f"æ”¯æŒçš„ç±»å‹: Spiderç±»ã€nameå­—ç¬¦ä¸²ï¼Œæˆ–å®ƒä»¬çš„åˆ—è¡¨/å…ƒç»„/é›†åˆ"
            )

    def _resolve_spider_class(self, item: Union[Type[Spider], str]) -> Type[Spider]:
        """
        è§£æå•ä¸ªè¾“å…¥é¡¹ä¸ºçˆ¬è™«ç±»
        
        æä¾›æ›´å¥½çš„é”™è¯¯æç¤ºå’Œè°ƒè¯•ä¿¡æ¯
        """
        if isinstance(item, type) and issubclass(item, Spider):
            # ç›´æ¥æ˜¯ Spider ç±»
            return item
        elif isinstance(item, str):
            # æ˜¯å­—ç¬¦ä¸²åç§°ï¼Œéœ€è¦æŸ¥æ‰¾æ³¨å†Œè¡¨
            spider_cls = self._spider_registry.get(item)
            if not spider_cls:
                available_spiders = list(self._spider_registry.keys())
                raise ValueError(
                    f"æœªæ‰¾åˆ°åä¸º '{item}' çš„çˆ¬è™«ã€‚\n"
                    f"å·²æ³¨å†Œçš„çˆ¬è™«: {available_spiders}\n"
                    f"è¯·æ£€æŸ¥çˆ¬è™«åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…ç¡®ä¿çˆ¬è™«å·²è¢«æ­£ç¡®å¯¼å…¥å’Œæ³¨å†Œã€‚"
                )
            return spider_cls
        else:
            raise TypeError(
                f"æ— æ•ˆç±»å‹ {type(item)}: {item}\n"
                f"å¿…é¡»æ˜¯ Spider ç±»æˆ–å­—ç¬¦ä¸² nameã€‚\n"
                f"ç¤ºä¾‹: MySpider æˆ– 'my_spider'"
            )

    async def _run_spider_with_limit(self, spider_cls: Type[Spider], seq: int, total: int):
        """
        å—ä¿¡å·é‡é™åˆ¶çš„çˆ¬è™«è¿è¡Œå‡½æ•°
        
        åŒ…å«å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œç›‘æ§åŠŸèƒ½
        """
        task = asyncio.current_task()
        crawler = None
        
        try:
            # æ³¨å†Œä»»åŠ¡
            if task:
                self._active_tasks.add(task)
            
            # è·å–å¹¶å‘è®¸å¯
            await self.semaphore.acquire()
            
            start_msg = f"[{seq}/{total}] å¯åŠ¨çˆ¬è™«: {spider_cls.__name__}"
            logger.info(start_msg)
            
            # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
            crawler = Crawler(spider_cls, self.settings, self.context)
            self.crawlers.add(crawler)
            
            # è®°å½•å¯åŠ¨æ—¶é—´
            start_time = time.time()
            
            # è¿è¡Œçˆ¬è™«
            await crawler.crawl()
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            duration = time.time() - start_time
            
            end_msg = (
                f"[{seq}/{total}] çˆ¬è™«å®Œæˆ: {spider_cls.__name__}, "
                f"è€—æ—¶: {duration:.2f}ç§’"
            )
            logger.info(end_msg)
            
            # è®°å½•æˆåŠŸç»Ÿè®¡
            self._performance_stats['successful_requests'] += 1
            
        except Exception as e:
            # è®°å½•å¤±è´¥ç»Ÿè®¡
            self._performance_stats['failed_requests'] += 1
            
            error_msg = f"çˆ¬è™« {spider_cls.__name__} æ‰§è¡Œå¤±è´¥: {e}"
            logger.error(error_msg, exc_info=True)
            
            # å°†é”™è¯¯ä¿¡æ¯è®°å½•åˆ°ä¸Šä¸‹æ–‡
            if hasattr(self, 'context'):
                self.context.increment_failed(error_msg)
            
            raise
        finally:
            # æ¸…ç†èµ„æº
            try:
                if crawler and crawler in self.crawlers:
                    self.crawlers.remove(crawler)
                    
                if task and task in self._active_tasks:
                    self._active_tasks.remove(task)
                    
                self.semaphore.release()
                
            except Exception as cleanup_error:
                logger.warning(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {cleanup_error}")

    def _shutdown(self, _signum, _frame):
        """
        ä¼˜é›…å…³é—­ä¿¡å·å¤„ç†
        
        æä¾›æ›´å¥½çš„å…³é—­ä½“éªŒå’Œèµ„æºæ¸…ç†
        """
        signal_name = {signal.SIGINT: 'SIGINT', signal.SIGTERM: 'SIGTERM'}.get(_signum, str(_signum))
        logger.warning(f"æ”¶åˆ°å…³é—­ä¿¡å· {signal_name}ï¼Œæ­£åœ¨åœæ­¢æ‰€æœ‰çˆ¬è™«...")
        
        # è®¾ç½®å…³é—­äº‹ä»¶
        if hasattr(self, '_shutdown_event'):
            self._shutdown_event.set()
        
        # åœæ­¢æ‰€æœ‰çˆ¬è™«å¼•æ“
        for crawler in list(self.crawlers):
            if crawler.engine:
                crawler.engine.running = False
                crawler.engine.normal = False
                logger.debug(f"å·²åœæ­¢çˆ¬è™«å¼•æ“: {getattr(crawler.spider, 'name', 'Unknown')}")
        
        # åˆ›å»ºå…³é—­ä»»åŠ¡
        asyncio.create_task(self._wait_for_shutdown())
        
        logger.info("å…³é—­æŒ‡ä»¤å·²å‘é€ï¼Œç­‰å¾…çˆ¬è™«å®Œæˆå½“å‰ä»»åŠ¡...")

    async def _wait_for_shutdown(self):
        """
        ç­‰å¾…æ‰€æœ‰æ´»è·ƒä»»åŠ¡å®Œæˆ
        
        æä¾›æ›´å¥½çš„å…³é—­æ—¶é—´æ§åˆ¶å’Œè¿›åº¦åé¦ˆ
        """
        try:
            # åœæ­¢ç›‘æ§ä»»åŠ¡
            await self.stop_monitoring()
            
            # ç­‰å¾…æ´»è·ƒä»»åŠ¡å®Œæˆ
            pending = [t for t in self._active_tasks if not t.done()]
            
            if pending:
                logger.info(
                    f"ç­‰å¾… {len(pending)} ä¸ªæ´»è·ƒä»»åŠ¡å®Œæˆ..."
                    f"(æœ€å¤§ç­‰å¾…æ—¶é—´: 30ç§’)"
                )
                
                # è®¾ç½®è¶…æ—¶æ—¶é—´
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*pending, return_exceptions=True),
                        timeout=30.0
                    )
                except asyncio.TimeoutError:
                    logger.warning("éƒ¨åˆ†ä»»åŠ¡è¶…æ—¶ï¼Œå¼ºåˆ¶å–æ¶ˆä¸­...")
                    
                    # å¼ºåˆ¶å–æ¶ˆè¶…æ—¶ä»»åŠ¡
                    for task in pending:
                        if not task.done():
                            task.cancel()
                    
                    # ç­‰å¾…å–æ¶ˆå®Œæˆ
                    await asyncio.gather(*pending, return_exceptions=True)
            
            # æœ€ç»ˆæ¸…ç†
            await self._cleanup_process()
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            final_stats = self.context.get_stats()
            logger.info(
                f"æ‰€æœ‰çˆ¬è™«å·²ä¼˜é›…å…³é—­ ğŸ‘‹\n"
                f"  - æ€»è®¡çˆ¬è™«: {final_stats['total_crawlers']}\n"
                f"  - æˆåŠŸå®Œæˆ: {final_stats['completed_crawlers']}\n"
                f"  - å¤±è´¥æ•°é‡: {final_stats['failed_crawlers']}\n"
                f"  - æˆåŠŸç‡: {final_stats['success_rate']:.1f}%\n"
                f"  - æ€»è¿è¡Œæ—¶é—´: {final_stats['duration_seconds']}ç§’"
            )
            
        except Exception as e:
            logger.error(f"å…³é—­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

    @classmethod
    def _get_default_settings(cls) -> SettingManager:
        """
        åŠ è½½é»˜è®¤é…ç½®
        
        æä¾›æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
        """
        try:
            settings = get_settings()
            logger.debug("æˆåŠŸåŠ è½½é»˜è®¤é…ç½®")
            return settings
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½é»˜è®¤é…ç½®: {e}ï¼Œä½¿ç”¨ç©ºé…ç½®")
            return SettingManager()

    def _log_startup_info(self):
        """æ‰“å°å¯åŠ¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¿è¡Œæ¨¡å¼å’Œå…³é”®é…ç½®æ£€æŸ¥"""
        # è·å–è¿è¡Œæ¨¡å¼
        run_mode = self.settings.get('RUN_MODE', 'standalone')
        
        # æ„å»ºå¯åŠ¨ä¿¡æ¯æ—¥å¿—
        startup_info = [
            "ğŸš€ Crawlo çˆ¬è™«æ¡†æ¶å¯åŠ¨"
        ]
        
        # è·å–å®é™…çš„é˜Ÿåˆ—ç±»å‹
        queue_type = self.settings.get('QUEUE_TYPE', 'memory')
        
        # æ ¹æ®è¿è¡Œæ¨¡å¼å’Œé˜Ÿåˆ—ç±»å‹ç»„åˆæ˜¾ç¤ºä¿¡æ¯
        if run_mode == 'distributed':
            startup_info.append("  è¿è¡Œæ¨¡å¼: distributed")
            startup_info.append("  ğŸŒ åˆ†å¸ƒå¼æ¨¡å¼ - æ”¯æŒå¤šèŠ‚ç‚¹ååŒå·¥ä½œ")
            # æ˜¾ç¤ºRedisé…ç½®
            redis_host = self.settings.get('REDIS_HOST', 'localhost')
            redis_port = self.settings.get('REDIS_PORT', 6379)
            startup_info.append(f"  Redisåœ°å€: {redis_host}:{redis_port}")
        elif run_mode == 'standalone':
            if queue_type == 'redis':
                startup_info.append("  è¿è¡Œæ¨¡å¼: standalone+redis")
                # startup_info.append("  ğŸŒ åˆ†å¸ƒå¼æ¨¡å¼ - æ”¯æŒå¤šèŠ‚ç‚¹ååŒå·¥ä½œ")
                # æ˜¾ç¤ºRedisé…ç½®
                redis_host = self.settings.get('REDIS_HOST', 'localhost')
                redis_port = self.settings.get('REDIS_PORT', 6379)
                startup_info.append(f"  Redisåœ°å€: {redis_host}:{redis_port}")
            elif queue_type == 'auto':
                startup_info.append("  è¿è¡Œæ¨¡å¼: standalone+auto")
                # startup_info.append("  ğŸ¤– è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ - æ™ºèƒ½é€‰æ‹©æœ€ä½³è¿è¡Œæ–¹å¼")
            else:  # memory
                startup_info.append("  è¿è¡Œæ¨¡å¼: standalone")
                # startup_info.append("  ğŸ  å•æœºæ¨¡å¼ - é€‚ç”¨äºå¼€å‘å’Œå°è§„æ¨¡æ•°æ®é‡‡é›†")
        else:  # auto mode
            if queue_type == 'redis':
                startup_info.append("  è¿è¡Œæ¨¡å¼: auto+redis")
                # startup_info.append("  ğŸŒ åˆ†å¸ƒå¼æ¨¡å¼ - æ”¯æŒå¤šèŠ‚ç‚¹ååŒå·¥ä½œ")
                # æ˜¾ç¤ºRedisé…ç½®
                redis_host = self.settings.get('REDIS_HOST', 'localhost')
                redis_port = self.settings.get('REDIS_PORT', 6379)
                startup_info.append(f"  Redisåœ°å€: {redis_host}:{redis_port}")
            elif queue_type == 'memory':
                startup_info.append("  è¿è¡Œæ¨¡å¼: auto+memory")
                # startup_info.append("  ğŸ  å•æœºæ¨¡å¼ - é€‚ç”¨äºå¼€å‘å’Œå°è§„æ¨¡æ•°æ®é‡‡é›†")
            else:  # auto
                startup_info.append("  è¿è¡Œæ¨¡å¼: auto")
                # startup_info.append("  ğŸ¤– è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ - æ™ºèƒ½é€‰æ‹©æœ€ä½³è¿è¡Œæ–¹å¼")
        
        # æ‰“å°å¯åŠ¨ä¿¡æ¯
        for info in startup_info:
            logger.info(info)


# === å·¥å…·å‡½æ•° ===

def create_crawler_with_optimizations(
    spider_cls: Type[Spider],
    settings: Optional[SettingManager] = None,
    **optimization_kwargs
) -> Crawler:
    """
    åˆ›å»ºä¼˜åŒ–çš„çˆ¬è™«å®ä¾‹
    
    :param spider_cls: çˆ¬è™«ç±»
    :param settings: è®¾ç½®ç®¡ç†å™¨
    :param optimization_kwargs: ä¼˜åŒ–å‚æ•°
    :return: çˆ¬è™«å®ä¾‹
    """
    if settings is None:
        settings = SettingManager()
    
    # åº”ç”¨ä¼˜åŒ–é…ç½®
    for key, value in optimization_kwargs.items():
        settings.set(key, value)
    
    context = CrawlerContext()
    return Crawler(spider_cls, settings, context)


def create_process_with_large_scale_config(
    config_type: str = 'balanced',
    concurrency: int = 16,
    **kwargs
) -> CrawlerProcess:
    """
    åˆ›å»ºæ”¯æŒå¤§è§„æ¨¡ä¼˜åŒ–çš„è¿›ç¨‹ç®¡ç†å™¨
    
    :param config_type: é…ç½®ç±»å‹ ('conservative', 'balanced', 'aggressive', 'memory_optimized')
    :param concurrency: å¹¶å‘æ•°
    :param kwargs: å…¶ä»–å‚æ•°
    :return: è¿›ç¨‹ç®¡ç†å™¨
    """
    try:
        from crawlo.utils.large_scale_config import LargeScaleConfig
        
        # è·å–ä¼˜åŒ–é…ç½®
        config_methods = {
            'conservative': LargeScaleConfig.conservative_config,
            'balanced': LargeScaleConfig.balanced_config,
            'aggressive': LargeScaleConfig.aggressive_config,
            'memory_optimized': LargeScaleConfig.memory_optimized_config
        }
        
        if config_type not in config_methods:
            logger.warning(f"æœªçŸ¥çš„é…ç½®ç±»å‹: {config_type}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            settings = SettingManager()
        else:
            config = config_methods[config_type](concurrency)
            settings = SettingManager()
            settings.update(config)
        
        return CrawlerProcess(
            settings=settings,
            max_concurrency=concurrency,
            **kwargs
        )
        
    except ImportError:
        logger.warning("å¤§è§„æ¨¡é…ç½®æ¨¡å—ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return CrawlerProcess(max_concurrency=concurrency, **kwargs)


# === å¯¼å‡ºæ¥å£ ===

__all__ = [
    'Crawler',
    'CrawlerProcess', 
    'CrawlerContext',
    'create_crawler_with_optimizations',
    'create_process_with_large_scale_config'
]