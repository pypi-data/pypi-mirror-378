#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆéªŒè¯æµ‹è¯•ï¼šç¡®è®¤åˆ†å¸ƒå¼é˜Ÿåˆ—çš„ logger åºåˆ—åŒ–é—®é¢˜å·²å®Œå…¨è§£å†³
"""
import asyncio
import pickle
import sys
sys.path.insert(0, "..")

from crawlo.network.request import Request
from crawlo.spider import Spider
from crawlo.core.scheduler import Scheduler
from crawlo.queue.redis_priority_queue import RedisPriorityQueue
from crawlo.utils.log import get_logger
from unittest.mock import Mock


class TestSpider(Spider):
    """æµ‹è¯•çˆ¬è™«"""
    name = "validation_spider"
    
    def __init__(self):
        super().__init__()
        # æ•…æ„æ·»åŠ å¤šä¸ª logger æ¥æµ‹è¯•æ¸…ç†
        self.custom_logger = get_logger("custom")
        self.debug_logger = get_logger("debug")
        self.nested_data = {
            'logger': get_logger("nested"),
            'sub': {
                'logger_ref': get_logger("sub_logger")
            }
        }
    
    def parse(self, response):
        # éªŒè¯ä¸» logger è¿˜åœ¨
        self.logger.info(f"âœ… ä¸» logger å·¥ä½œæ­£å¸¸: {response.url}")
        return {"url": response.url, "status": "success"}


def test_scheduler_cleaning():
    """æµ‹è¯•è°ƒåº¦å™¨çš„ logger æ¸…ç†"""
    print("ğŸ” æµ‹è¯•è°ƒåº¦å™¨ logger æ¸…ç†...")
    
    spider = TestSpider()
    request = Request(
        url="https://scheduler-test.com",
        callback=spider.parse,
        meta={"logger": get_logger("meta_logger")}
    )
    
    # Mock crawler å’Œ scheduler
    class MockCrawler:
        def __init__(self):
            self.spider = spider
    
    class MockScheduler(Scheduler):
        def __init__(self):
            self.crawler = MockCrawler()
            self.logger = get_logger("MockScheduler")
    
    scheduler = MockScheduler()
    
    # æ¸…ç†å‰æ£€æŸ¥
    print(f"   ğŸ”§ æ¸…ç†å‰ - spider.logger: {spider.logger is not None}")
    print(f"   ğŸ”§ æ¸…ç†å‰ - spider.custom_logger: {spider.custom_logger is not None}")
    print(f"   ğŸ”§ æ¸…ç†å‰ - request.callback: {request.callback is not None}")
    
    # æ‰§è¡Œæ¸…ç†
    cleaned_request = scheduler._deep_clean_loggers(request)
    
    # æ¸…ç†åæ£€æŸ¥
    print(f"   âœ… æ¸…ç†å - spider.logger: {spider.logger is not None}")
    print(f"   âœ… æ¸…ç†å - spider.custom_logger: {spider.custom_logger is None}")
    print(f"   âœ… æ¸…ç†å - request.callback: {cleaned_request.callback is None}")
    
    # åºåˆ—åŒ–æµ‹è¯•
    try:
        serialized = pickle.dumps(cleaned_request)
        print(f"   âœ… è°ƒåº¦å™¨æ¸…ç†ååºåˆ—åŒ–æˆåŠŸï¼Œå¤§å°: {len(serialized)} bytes")
        return True
    except Exception as e:
        print(f"   âŒ è°ƒåº¦å™¨æ¸…ç†ååºåˆ—åŒ–å¤±è´¥: {e}")
        return False


async def test_redis_queue_cleaning():
    """æµ‹è¯• Redis é˜Ÿåˆ—çš„ logger æ¸…ç†"""
    print("\\nğŸ” æµ‹è¯• Redis é˜Ÿåˆ— logger æ¸…ç†...")
    
    spider = TestSpider()
    request = Request(
        url="https://redis-test.com",
        callback=spider.parse,
        meta={"logger": get_logger("meta_logger")}
    )
    
    try:
        queue = RedisPriorityQueue(redis_url="redis://127.0.0.1:6379/0")
        await queue.connect()
        
        # å…¥é˜Ÿæµ‹è¯•
        success = await queue.put(request, priority=0)
        print(f"   âœ… Redis é˜Ÿåˆ—å…¥é˜ŸæˆåŠŸ: {success}")
        
        if success:
            # å‡ºé˜Ÿæµ‹è¯•
            retrieved = await queue.get(timeout=2.0)
            if retrieved:
                print(f"   âœ… Redis é˜Ÿåˆ—å‡ºé˜ŸæˆåŠŸ: {retrieved.url}")
                print(f"   âœ… callback ä¿¡æ¯ä¿å­˜: {'_callback_info' in retrieved.meta}")
                await queue.close()
                return True
            else:
                print("   âŒ å‡ºé˜Ÿå¤±è´¥")
                await queue.close()
                return False
        else:
            await queue.close()
            return False
            
    except Exception as e:
        print(f"   âŒ Redis é˜Ÿåˆ—æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æœ€ç»ˆéªŒè¯æµ‹è¯•...")
    print("=" * 60)
    
    # æµ‹è¯• 1: è°ƒåº¦å™¨æ¸…ç†
    scheduler_ok = test_scheduler_cleaning()
    
    # æµ‹è¯• 2: Redis é˜Ÿåˆ—æ¸…ç†
    redis_ok = await test_redis_queue_cleaning()
    
    print("\\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"   è°ƒåº¦å™¨ logger æ¸…ç†: {'âœ… é€šè¿‡' if scheduler_ok else 'âŒ å¤±è´¥'}")
    print(f"   Redis é˜Ÿåˆ—æ¸…ç†: {'âœ… é€šè¿‡' if redis_ok else 'âŒ å¤±è´¥'}")
    
    if scheduler_ok and redis_ok:
        print("\\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… åˆ†å¸ƒå¼é˜Ÿåˆ—çš„ logger åºåˆ—åŒ–é—®é¢˜å·²å®Œå…¨ä¿®å¤ï¼")
        print("âœ… Crawlo ç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨ Redis åˆ†å¸ƒå¼é˜Ÿåˆ—äº†ï¼")
        return True
    else:
        print("\\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤")
        return False


if __name__ == "__main__":
    asyncio.run(main())