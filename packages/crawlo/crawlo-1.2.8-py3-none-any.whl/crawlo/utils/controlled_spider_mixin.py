#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å—æ§çˆ¬è™«æ··å…¥ç±»
è§£å†³ start_requests() yield ä¸Šä¸‡ä¸ªè¯·æ±‚æ—¶çš„å¹¶å‘æ§åˆ¶é—®é¢˜
"""
import asyncio
import time
from collections import deque
from typing import Generator, Optional

from crawlo import Request
from crawlo.utils.log import get_logger


class ControlledRequestMixin:
    """
    å—æ§è¯·æ±‚ç”Ÿæˆæ··å…¥ç±»
    
    è§£å†³é—®é¢˜ï¼š
    1. start_requests() åŒæ—¶yieldä¸Šä¸‡ä¸ªè¯·æ±‚å¯¼è‡´å†…å­˜çˆ†ç‚¸
    2. ä¸éµå®ˆCONCURRENCYè®¾ç½®ï¼Œæ— é™åˆ¶åˆ›å»ºè¯·æ±‚
    3. é˜Ÿåˆ—ç§¯å‹è¿‡å¤šè¯·æ±‚å½±å“æ€§èƒ½
    
    è§£å†³æ–¹æ¡ˆï¼š
    1. æŒ‰éœ€ç”Ÿæˆè¯·æ±‚ï¼Œæ ¹æ®å®é™…å¹¶å‘èƒ½åŠ›æ§åˆ¶
    2. åŠ¨æ€ç›‘æ§é˜Ÿåˆ—çŠ¶æ€ï¼Œæ™ºèƒ½è°ƒèŠ‚ç”Ÿæˆé€Ÿåº¦
    3. æ”¯æŒèƒŒå‹æ§åˆ¶ï¼Œé¿å…é˜Ÿåˆ—ç§¯å‹
    """
    
    def __init__(self):
        self.logger = get_logger(self.__class__.__name__)
        
        # å—æ§ç”Ÿæˆé…ç½®
        self.max_pending_requests = 100     # æœ€å¤§å¾…å¤„ç†è¯·æ±‚æ•°
        self.batch_size = 50               # æ¯æ‰¹ç”Ÿæˆè¯·æ±‚æ•°
        self.generation_interval = 0.1      # ç”Ÿæˆé—´éš”ï¼ˆç§’ï¼‰
        self.backpressure_threshold = 200   # èƒŒå‹é˜ˆå€¼
        
        # å†…éƒ¨çŠ¶æ€
        self._original_start_requests = None
        self._pending_count = 0
        self._total_generated = 0
        self._generation_paused = False
        
        # æ€§èƒ½ç›‘æ§
        self._last_generation_time = 0
        self._generation_stats = {
            'generated': 0,
            'skipped': 0,
            'backpressure_events': 0
        }
    
    def start_requests(self) -> Generator[Request, None, None]:
        """
        å—æ§çš„ start_requests å®ç°
        
        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•ä¼šæ›¿æ¢åŸå§‹çš„ start_requestsï¼Œ
        åŸå§‹è¯·æ±‚å°†é€šè¿‡ _original_start_requests() æä¾›
        """
        # ä¿å­˜åŸå§‹çš„è¯·æ±‚ç”Ÿæˆå™¨
        if hasattr(self, '_original_start_requests') and self._original_start_requests:
            original_generator = self._original_start_requests()
        else:
            # å¦‚æœå­ç±»æ²¡æœ‰å®šä¹‰ _original_start_requestsï¼Œå°è¯•è°ƒç”¨åŸå§‹æ–¹æ³•
            original_generator = self._get_original_requests()
        
        # ä½¿ç”¨å—æ§ç”Ÿæˆå™¨åŒ…è£…åŸå§‹ç”Ÿæˆå™¨
        yield from self._controlled_request_generator(original_generator)
    
    def _original_start_requests(self) -> Generator[Request, None, None]:
        """
        å­ç±»åº”è¯¥å®ç°è¿™ä¸ªæ–¹æ³•ï¼Œæä¾›åŸå§‹çš„è¯·æ±‚ç”Ÿæˆé€»è¾‘
        
        ç¤ºä¾‹ï¼š
        def _original_start_requests(self):
            for i in range(50000):  # 5ä¸‡ä¸ªè¯·æ±‚
                yield Request(url=f"https://example.com/page/{i}")
        """
        raise NotImplementedError(
            "å­ç±»å¿…é¡»å®ç° _original_start_requests() æ–¹æ³•ï¼Œ"
            "æˆ–è€…ç¡®ä¿åŸå§‹çš„ start_requests() æ–¹æ³•å­˜åœ¨"
        )
    
    def _get_original_requests(self) -> Generator[Request, None, None]:
        """å°è¯•è·å–åŸå§‹è¯·æ±‚ï¼ˆå‘åå…¼å®¹ï¼‰"""
        # è¿™é‡Œå¯ä»¥å°è¯•è°ƒç”¨çˆ¶ç±»çš„ start_requests æˆ–å…¶ä»–æ–¹å¼
        # å…·ä½“å®ç°å–å†³äºä½ çš„éœ€æ±‚
        return iter([])  # é»˜è®¤è¿”å›ç©ºç”Ÿæˆå™¨
    
    def _controlled_request_generator(self, original_generator) -> Generator[Request, None, None]:
        """å—æ§çš„è¯·æ±‚ç”Ÿæˆå™¨"""
        self.logger.info(f"ğŸ›ï¸ å¯åŠ¨å—æ§è¯·æ±‚ç”Ÿæˆå™¨ (æœ€å¤§å¾…å¤„ç†: {self.max_pending_requests})")
        
        request_buffer = deque()
        batch_count = 0
        
        try:
            # åˆ†æ‰¹å¤„ç†åŸå§‹è¯·æ±‚
            for request in original_generator:
                request_buffer.append(request)
                
                # å½“ç¼“å†²åŒºè¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œè¿›è¡Œæ§åˆ¶æ£€æŸ¥
                if len(request_buffer) >= self.batch_size:
                    yield from self._yield_controlled_batch(request_buffer)
                    batch_count += 1
                    
                    # æ¯æ‰¹æ¬¡åæ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                    if self._should_pause_generation():
                        self._wait_for_capacity()
            
            # å¤„ç†å‰©ä½™çš„è¯·æ±‚
            if request_buffer:
                yield from self._yield_controlled_batch(request_buffer)
        
        except Exception as e:
            self.logger.error(f"âŒ å—æ§è¯·æ±‚ç”Ÿæˆå¤±è´¥: {e}")
            raise
        
        self.logger.info(
            f"ğŸ‰ å—æ§è¯·æ±‚ç”Ÿæˆå®Œæˆï¼"
            f"æ€»è®¡: {self._generation_stats['generated']}, "
            f"è·³è¿‡: {self._generation_stats['skipped']}, "
            f"èƒŒå‹äº‹ä»¶: {self._generation_stats['backpressure_events']}"
        )
    
    def _yield_controlled_batch(self, request_buffer: deque) -> Generator[Request, None, None]:
        """åˆ†æ‰¹å—æ§ yield è¯·æ±‚"""
        while request_buffer:
            # æ£€æŸ¥å½“å‰ç³»ç»Ÿè´Ÿè½½
            if self._should_pause_generation():
                self.logger.debug("â¸ï¸ æ£€æµ‹åˆ°ç³»ç»Ÿè´Ÿè½½è¿‡é«˜ï¼Œæš‚åœç”Ÿæˆ")
                self._generation_stats['backpressure_events'] += 1
                self._wait_for_capacity()
            
            # yield ä¸€ä¸ªè¯·æ±‚
            request = request_buffer.popleft()
            
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é¢å¤–çš„è¯·æ±‚å¤„ç†é€»è¾‘
            processed_request = self._process_request_before_yield(request)
            if processed_request:
                self._total_generated += 1
                self._generation_stats['generated'] += 1
                self._last_generation_time = time.time()
                yield processed_request
            else:
                self._generation_stats['skipped'] += 1
            
            # æ§åˆ¶ç”Ÿæˆé€Ÿåº¦
            if self.generation_interval > 0:
                time.sleep(self.generation_interval)
    
    def _should_pause_generation(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æš‚åœè¯·æ±‚ç”Ÿæˆ"""
        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°ï¼ˆå¦‚æœå¯ä»¥è®¿é—®schedulerçš„è¯ï¼‰
        if hasattr(self, 'crawler') and self.crawler:
            engine = getattr(self.crawler, 'engine', None)
            if engine and engine.scheduler:
                queue_size = len(engine.scheduler)
                if queue_size > self.backpressure_threshold:
                    return True
        
        # æ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨è´Ÿè½½
        if hasattr(self, 'crawler') and self.crawler:
            engine = getattr(self.crawler, 'engine', None)
            if engine and engine.task_manager:
                current_tasks = len(engine.task_manager.current_task)
                concurrency = getattr(engine.task_manager, 'semaphore', None)
                if concurrency and hasattr(concurrency, '_initial_value'):
                    max_concurrency = concurrency._initial_value
                    # å¦‚æœå½“å‰ä»»åŠ¡æ•°æ¥è¿‘æœ€å¤§å¹¶å‘æ•°ï¼Œæš‚åœç”Ÿæˆ
                    if current_tasks >= max_concurrency * 0.8:  # 80% é˜ˆå€¼
                        return True
        
        return False
    
    def _wait_for_capacity(self):
        """ç­‰å¾…ç³»ç»Ÿæœ‰è¶³å¤Ÿå®¹é‡"""
        wait_time = 0.1
        max_wait = 5.0
        
        while self._should_pause_generation() and wait_time < max_wait:
            time.sleep(wait_time)
            wait_time = min(wait_time * 1.2, max_wait)  # æŒ‡æ•°é€€é¿
    
    def _process_request_before_yield(self, request: Request) -> Optional[Request]:
        """
        åœ¨ yield è¯·æ±‚å‰è¿›è¡Œå¤„ç†
        å­ç±»å¯ä»¥é‡å†™è¿™ä¸ªæ–¹æ³•æ¥æ·»åŠ è‡ªå®šä¹‰é€»è¾‘
        
        è¿”å› None è¡¨ç¤ºè·³è¿‡è¿™ä¸ªè¯·æ±‚
        """
        return request
    
    def get_generation_stats(self) -> dict:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self._generation_stats,
            'total_generated': self._total_generated,
            'last_generation_time': self._last_generation_time
        }


class AsyncControlledRequestMixin:
    """
    å¼‚æ­¥ç‰ˆæœ¬çš„å—æ§è¯·æ±‚æ··å…¥ç±»
    
    ä½¿ç”¨asyncioæ¥å®ç°æ›´ç²¾ç¡®çš„å¹¶å‘æ§åˆ¶
    """
    
    def __init__(self):
        self.logger = get_logger(self.__class__.__name__)
        
        # å¼‚æ­¥æ§åˆ¶é…ç½®
        self.max_concurrent_generations = 10   # æœ€å¤§åŒæ—¶ç”Ÿæˆæ•°
        self.generation_semaphore = None
        self.queue_monitor_interval = 1.0       # é˜Ÿåˆ—ç›‘æ§é—´éš”
        
        # å¼‚æ­¥çŠ¶æ€
        self._generation_tasks = set()
        self._monitoring_task = None
        self._stop_generation = False
    
    def _original_start_requests(self) -> Generator[Request, None, None]:
        """
        å­ç±»åº”è¯¥å®ç°è¿™ä¸ªæ–¹æ³•ï¼Œæä¾›åŸå§‹çš„è¯·æ±‚ç”Ÿæˆé€»è¾‘
        
        ç¤ºä¾‹ï¼š
        def _original_start_requests(self):
            for i in range(50000):  # 5ä¸‡ä¸ªè¯·æ±‚
                yield Request(url=f"https://example.com/page/{i}")
        """
        raise NotImplementedError(
            "å­ç±»å¿…é¡»å®ç° _original_start_requests() æ–¹æ³•ï¼Œ"
            "æˆ–è€…ç¡®ä¿åŸå§‹çš„ start_requests() æ–¹æ³•å­˜åœ¨"
        )
    
    def _get_original_requests(self) -> Generator[Request, None, None]:
        """å°è¯•è·å–åŸå§‹è¯·æ±‚ï¼ˆå‘åå…¼å®¹ï¼‰"""
        # è¿™é‡Œå¯ä»¥å°è¯•è°ƒç”¨çˆ¶ç±»çš„ start_requests æˆ–å…¶ä»–æ–¹å¼
        # å…·ä½“å®ç°å–å†³äºä½ çš„éœ€æ±‚
        return iter([])  # é»˜è®¤è¿”å›ç©ºç”Ÿæˆå™¨
    
    def _should_pause_generation(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æš‚åœè¯·æ±‚ç”Ÿæˆ"""
        # æ£€æŸ¥é˜Ÿåˆ—å¤§å°ï¼ˆå¦‚æœå¯ä»¥è®¿é—®schedulerçš„è¯ï¼‰
        if hasattr(self, 'crawler') and self.crawler:
            engine = getattr(self.crawler, 'engine', None)
            if engine and engine.scheduler:
                queue_size = len(engine.scheduler)
                if queue_size > 200:  # èƒŒå‹é˜ˆå€¼
                    return True
        
        # æ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨è´Ÿè½½
        if hasattr(self, 'crawler') and self.crawler:
            engine = getattr(self.crawler, 'engine', None)
            if engine and engine.task_manager:
                current_tasks = len(engine.task_manager.current_task)
                concurrency = getattr(engine.task_manager, 'semaphore', None)
                if concurrency and hasattr(concurrency, '_initial_value'):
                    max_concurrency = concurrency._initial_value
                    # å¦‚æœå½“å‰ä»»åŠ¡æ•°æ¥è¿‘æœ€å¤§å¹¶å‘æ•°ï¼Œæš‚åœç”Ÿæˆ
                    if current_tasks >= max_concurrency * 0.8:  # 80% é˜ˆå€¼
                        return True
        
        return False
    
    def _process_request_before_yield(self, request: Request) -> Optional[Request]:
        """
        åœ¨ yield è¯·æ±‚å‰è¿›è¡Œå¤„ç†
        å­ç±»å¯ä»¥é‡å†™è¿™ä¸ªæ–¹æ³•æ¥æ·»åŠ è‡ªå®šä¹‰é€»è¾‘
        
        è¿”å› None è¡¨ç¤ºè·³è¿‡è¿™ä¸ªè¯·æ±‚
        """
        return request
    
    async def start_requests_async(self) -> Generator[Request, None, None]:
        """å¼‚æ­¥ç‰ˆæœ¬çš„å—æ§è¯·æ±‚ç”Ÿæˆ"""
        # åˆå§‹åŒ–ä¿¡å·é‡
        self.generation_semaphore = asyncio.Semaphore(self.max_concurrent_generations)
        
        # å¯åŠ¨é˜Ÿåˆ—ç›‘æ§
        self._monitoring_task = asyncio.create_task(self._monitor_queue_load())
        
        try:
            # è·å–åŸå§‹è¯·æ±‚
            original_requests = self._original_start_requests()
            
            # åˆ†æ‰¹å¼‚æ­¥å¤„ç†
            batch = []
            async for request in self._async_request_wrapper(original_requests):
                batch.append(request)
                
                if len(batch) >= 50:  # æ‰¹æ¬¡å¤§å°
                    async for request in self._process_async_batch(batch):
                        yield request
                    batch = []
            
            # å¤„ç†å‰©ä½™è¯·æ±‚
            if batch:
                async for request in self._process_async_batch(batch):
                    yield request
        
        finally:
            # æ¸…ç†
            self._stop_generation = True
            if self._monitoring_task:
                self._monitoring_task.cancel()
            
            # ç­‰å¾…æ‰€æœ‰ç”Ÿæˆä»»åŠ¡å®Œæˆ
            if self._generation_tasks:
                await asyncio.gather(*self._generation_tasks, return_exceptions=True)
    
    async def _async_request_wrapper(self, sync_generator):
        """å°†åŒæ­¥ç”Ÿæˆå™¨åŒ…è£…ä¸ºå¼‚æ­¥"""
        for request in sync_generator:
            yield request
            await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
    
    async def _process_async_batch(self, batch):
        """å¼‚æ­¥å¤„ç†æ‰¹æ¬¡è¯·æ±‚"""
        async def process_single_request(request):
            async with self.generation_semaphore:
                # ç­‰å¾…åˆé€‚çš„æ—¶æœº
                while self._should_pause_generation() and not self._stop_generation:
                    await asyncio.sleep(0.1)
                
                if not self._stop_generation:
                    return self._process_request_before_yield(request)
                return None
        
        # å¹¶å‘å¤„ç†æ‰¹æ¬¡ä¸­çš„è¯·æ±‚
        tasks = [process_single_request(req) for req in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # yield å¤„ç†å®Œçš„è¯·æ±‚
        for result in results:
            if result and not isinstance(result, Exception):
                yield result
    
    async def _monitor_queue_load(self):
        """ç›‘æ§é˜Ÿåˆ—è´Ÿè½½"""
        while not self._stop_generation:
            try:
                # è¿™é‡Œå¯ä»¥æ·»åŠ é˜Ÿåˆ—è´Ÿè½½ç›‘æ§é€»è¾‘
                await asyncio.sleep(self.queue_monitor_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.warning(f"é˜Ÿåˆ—ç›‘æ§å¼‚å¸¸: {e}")
                await asyncio.sleep(1.0)


# ä½¿ç”¨ç¤ºä¾‹å’Œæ–‡æ¡£
USAGE_EXAMPLE = '''
# åŒæ­¥ç‰ˆæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼š

class MyControlledSpider(Spider, ControlledRequestMixin):
    name = 'controlled_spider'
    
    def __init__(self):
        Spider.__init__(self)
        ControlledRequestMixin.__init__(self)
        
        # é…ç½®å—æ§ç”Ÿæˆå‚æ•°
        self.max_pending_requests = 200
        self.batch_size = 100
        self.generation_interval = 0.05
    
    def _original_start_requests(self):
        """æä¾›åŸå§‹çš„å¤§é‡è¯·æ±‚"""
        for i in range(50000):  # 5ä¸‡ä¸ªè¯·æ±‚
            yield Request(url=f"https://example.com/page/{i}")
    
    def _process_request_before_yield(self, request):
        """å¯é€‰ï¼šåœ¨yieldå‰å¤„ç†è¯·æ±‚"""
        # å¯ä»¥æ·»åŠ å»é‡ã€ä¼˜å…ˆçº§è®¾ç½®ç­‰é€»è¾‘
        return request
    
    async def parse(self, response):
        # è§£æé€»è¾‘
        yield {"url": response.url}

# å¼‚æ­¥ç‰ˆæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼š

class MyAsyncControlledSpider(Spider, AsyncControlledRequestMixin):
    name = 'async_controlled_spider'
    
    def __init__(self):
        Spider.__init__(self)
        AsyncControlledRequestMixin.__init__(self)
        
        # é…ç½®å¼‚æ­¥æ§åˆ¶å‚æ•°
        self.max_concurrent_generations = 15
        self.queue_monitor_interval = 0.5
    
    def _original_start_requests(self):
        """æä¾›åŸå§‹çš„å¤§é‡è¯·æ±‚"""
        categories = ['tech', 'finance', 'sports']
        for category in categories:
            for page in range(1, 10000):  # æ¯ä¸ªåˆ†ç±»1ä¸‡é¡µ
                yield Request(
                    url=f"https://news-site.com/{category}?page={page}",
                    meta={'category': category}
                )
    
    def _process_request_before_yield(self, request):
        """å¼‚æ­¥ç‰ˆæœ¬çš„è¯·æ±‚é¢„å¤„ç†"""
        # æ ¹æ®åˆ†ç±»è®¾ç½®ä¼˜å…ˆçº§
        category = request.meta.get('category', '')
        if category == 'tech':
            request.priority = 10
        return request
    
    async def parse(self, response):
        # å¼‚æ­¥è§£æé€»è¾‘
        yield {
            "url": response.url,
            "category": response.meta['category']
        }

# ä½¿ç”¨æ—¶ï¼š
from crawlo.crawler import CrawlerProcess
from crawlo.config import CrawloConfig

# åŒæ­¥ç‰ˆæœ¬
config = CrawloConfig.standalone(concurrency=16)
process = CrawlerProcess(config)
process.crawl(MyControlledSpider)
process.start()

# å¼‚æ­¥ç‰ˆæœ¬
async_config = CrawloConfig.standalone(
    concurrency=30,
    downloader='httpx'  # æ¨èä½¿ç”¨æ”¯æŒå¼‚æ­¥çš„ä¸‹è½½å™¨
)
async_process = CrawlerProcess(async_config)
async_process.crawl(MyAsyncControlledSpider)
async_process.start()
'''