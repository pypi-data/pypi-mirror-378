/home/robv/distals/src/distals/wiki.py:50: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("html.parser"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

The code that caused this warning is on line 50 of the file /home/robv/distals/src/distals/wiki.py. To get rid of this warning, pass the additional argument 'features="html.parser"' to the BeautifulSoup constructor.


Now:
- Return most distant/close language

TODO:
- better versioning of database (how?)
- convert language information to feature vector
- filter miltale to only contain the "main" scripts, add GLOTLID/UDHR
- Use https://en.wal.unesco.org/index.php/about-unesco-wal/introduction for endangerment?
- Add docstrings
- Add tests
- multiple language entries in phoible
- `data` folder:  update files to most recent version
- Double check macro conversions
	- see also list below -- are there any we should upgrade to macrolanguage conversions?
	- instead of returning a warning about a (macrolanguage) code not being found, can we add a message that they should try again with a more specific code?
- update results in paper
- update results in readme

Other potential features
- Swadesh
- countries overlap from ../language_metadata/data.tsv
- POS tags?
- character type-token ratio
- subwords?
- https://clics.clld.org/ ?
- Uriel+: still cant pip install it here..
- colex2lang



Wikis with macrolanguage codes -- can these be resolved meaningfully? (None = output by langname_tools ISO code matcher)
fa None --> resolving to pes (Iranian Farsi) might be reasonable
no None
az None
sw None
uz None
sq None
mn None
ku None
sa None
mg None
yi None
ps None
qu None
ff None
sc None
nah None
kg None
om None
iu None
ay None
ik None
za None
cr None
din None
