#!/usr/bin/env python3
"""
Script to convert old LucaOneVirus checkpoint to new LucaGPLMModel format
"""

import os
import sys
import torch
import argparse
import json
from collections import OrderedDict
from lucagplm.modeling_lucagplm import LucaGPLMModel, LucaGPLMForPretraining
from lucagplm.configuration_lucagplm import LucaGPLMConfig
from lucagplm.tokenization_lucagplm import LucaGPLMTokenizer

def convert_old_weights(old_checkpoint_path, output_dir, with_pretraining_heads=False):
    # Path to the old checkpoint
    print(f"Loading old checkpoint from: {old_checkpoint_path}")
    print("=" * 80)
    
    # Load the model and tokenizer using the from_pretrained_old method
    try:
        if with_pretraining_heads:
            print("Converting model with pretraining heads...")
            model = LucaGPLMForPretraining.from_pretrained_old(
                old_model_path=old_checkpoint_path,
                config=None,  # Let it auto-convert the config
            )
        else:
            model = LucaGPLMModel.from_pretrained_old(
                old_model_path=old_checkpoint_path,
                config=None,  # Let it auto-convert the config
                add_pooling_layer=True
            )
        
        # Load the tokenizer using from_pretrained_old method
        tokenizer = LucaGPLMTokenizer.from_pretrained_old(old_checkpoint_path)
        
        print("âœ… Successfully loaded and converted the old model and tokenizer!")
        print(f"Model config: {model.config}")
        print(f"Number of parameters: {sum(p.numel() for p in model.parameters()):,}")
        print(f"Tokenizer vocab size: {tokenizer.vocab_size}")
        print(f"Tokenizer vocab type: {tokenizer.vocab_type}")
        
        # If we want pretraining heads, we need to load them from the old model
        if with_pretraining_heads:
            print("Loading pretraining heads from old model...")
            load_pretraining_heads(model, old_checkpoint_path)
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Save the model and tokenizer in the new format
        print(f"\nSaving converted model and tokenizer to: {output_dir}")
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… Model and tokenizer successfully saved as: {output_dir}")
        print("=" * 80)
        
        # Test loading the converted model and tokenizer
        print("Testing the converted model and tokenizer...")
        if with_pretraining_heads:
            test_model = LucaGPLMForPretraining.from_pretrained(output_dir)
        else:
            test_model = LucaGPLMModel.from_pretrained(output_dir)
        test_tokenizer = LucaGPLMTokenizer.from_pretrained(output_dir)
        print(f"âœ… Successfully loaded converted model and tokenizer from {output_dir}")
        print(f"Test model config: {test_model.config}")
        print(f"Test tokenizer vocab size: {test_tokenizer.vocab_size}")
        
        # Simple forward pass test
        print("\nRunning a simple forward pass test...")
        test_input = torch.randint(0, 39, (1, 100))  # vocab_size = 39
        with torch.no_grad():
            if with_pretraining_heads:
                # For pretraining model, we don't need labels for a simple test
                outputs = test_model(test_input)
                print(f"âœ… Forward pass successful! Output keys: {outputs.keys()}")
                if 'logits' in outputs:
                    print(f"Task logits: {list(outputs['logits'].keys())}")
            else:
                outputs = test_model(test_input)
                print(f"âœ… Forward pass successful! Output shape: {outputs.last_hidden_state.shape}")
        
        # Test tokenizer functionality
        print("\nTesting tokenizer functionality...")
        test_sequence = "ATCGATCGATCG"  # Example gene sequence
        encoded = test_tokenizer.encode(test_sequence, add_special_tokens=True)
        decoded = test_tokenizer.decode(encoded)
        print(f"âœ… Tokenizer test successful! Input: {test_sequence}, Encoded: {encoded}, Decoded: {decoded}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error during conversion: {e}")
        import traceback
        traceback.print_exc()
        return False

def load_pretraining_heads(model, old_checkpoint_path):
    """
    Load pretraining heads from the old LucaVirus model
    """
    # Load old model weights
    old_weights_path_pth = os.path.join(old_checkpoint_path, "pytorch.pth")
    old_weights_path_pt = os.path.join(old_checkpoint_path, "pytorch.pt")
    
    old_state_dict = None
    if os.path.exists(old_weights_path_pth):
        old_state_dict = torch.load(old_weights_path_pth, map_location="cpu", weights_only=False)
    elif os.path.exists(old_weights_path_pt):
        old_state_dict = torch.load(old_weights_path_pt, map_location="cpu", weights_only=False)
    else:
        print(f"Warning: Could not find old model weights at {old_weights_path_pth} or {old_weights_path_pt}")
        return
    
    # Handle wrapped models (remove 'module.' prefix if present)
    if isinstance(old_state_dict, dict):
        new_old_state_dict = OrderedDict()
        for k, v in old_state_dict.items():
            name = k[7:] if k.startswith("module.") else k
            new_old_state_dict[name] = v
        old_state_dict = new_old_state_dict
    else:
        # If it's a model object, get state dict
        old_state_dict = old_state_dict.state_dict()
    
    # Create mapping for pretraining heads
    new_state_dict = OrderedDict()
    model_state_dict = model.state_dict()
    
    # Map LM head
    if "lm_head.weight" in old_state_dict and "lm_head.weight" in model_state_dict:
        new_state_dict["lm_head.weight"] = old_state_dict["lm_head.weight"]
    
    # Map pretraining task heads
    for task_level in ["token_level", "span_level", "seq_level"]:
        if task_level in model.pretrain_tasks:
            for task_name, task_head in model.pretrain_tasks[task_level].items():
                for component_name, component in task_head.items():
                    if component_name == "classifier_dropout":
                        continue  # Skip dropout layers as they don't have weights
                    
                    old_key = f"pretrain_tasks.{task_level}.{task_name}.{component_name}.weight"
                    old_bias_key = f"pretrain_tasks.{task_level}.{task_name}.{component_name}.bias"
                    
                    new_key = f"pretrain_tasks.{task_level}.{task_name}.{component_name}.weight"
                    new_bias_key = f"pretrain_tasks.{task_level}.{task_name}.{component_name}.bias"
                    
                    if old_key in old_state_dict and new_key in model_state_dict:
                        new_state_dict[new_key] = old_state_dict[old_key]
                    
                    if old_bias_key in old_state_dict and new_bias_key in model_state_dict:
                        new_state_dict[new_bias_key] = old_state_dict[old_bias_key]
    
    # Load the mapped state dict
    if new_state_dict:
        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
        
        if missing_keys:
            print(f"Warning: Missing keys when loading pretraining heads: {missing_keys}")
        if unexpected_keys:
            print(f"Warning: Unexpected keys when loading pretraining heads: {unexpected_keys}")
        
        print(f"Successfully loaded {len(new_state_dict)} pretraining head parameters from old model")
    else:
        print("No pretraining head parameters found in old model")

def convert_with_pretraining_heads(old_model_path, output_path):
    """
    è½¬æ¢å¸¦æœ‰é¢„è®­ç»ƒä»»åŠ¡å¤´çš„æ¨¡å‹æƒé‡ï¼Œç›´æ¥åœ¨è¾“å‡ºç›®å½•ä¸­ä¿å­˜å¸¦æœ‰é¢„è®­ç»ƒä»»åŠ¡å¤´çš„æ¨¡å‹å’Œtokenizer
    
    Args:
        old_model_path: æ—§æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
    """
    print("=" * 80)
    print("è½¬æ¢å¸¦æœ‰é¢„è®­ç»ƒä»»åŠ¡å¤´çš„æ¨¡å‹æƒé‡")
    print("=" * 80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_path, exist_ok=True)
    
    # åŠ è½½æ—§æ¨¡å‹æƒé‡
    print("\nğŸ”„ åŠ è½½æ—§æ¨¡å‹æƒé‡...")
    old_weights_path_pth = os.path.join(old_model_path, "pytorch.pth")
    old_weights_path_pt = os.path.join(old_model_path, "pytorch.pt")
    
    old_state_dict = None
    if os.path.exists(old_weights_path_pth):
        old_state_dict = torch.load(old_weights_path_pth, map_location="cpu", weights_only=False)
    elif os.path.exists(old_weights_path_pt):
        old_state_dict = torch.load(old_weights_path_pt, map_location="cpu", weights_only=False)
    else:
        raise FileNotFoundError(f"æ—§æ¨¡å‹æƒé‡æœªæ‰¾åˆ°äº {old_weights_path_pth} æˆ– {old_weights_path_pt}")
    
    # å¤„ç†module.å‰ç¼€
    print("ğŸ”„ å¤„ç†module.å‰ç¼€...")
    processed_old_state_dict = {}
    for key, value in old_state_dict.items():
        if key.startswith("module."):
            new_key = key[7:]  # ç§»é™¤"module."å‰ç¼€
            processed_old_state_dict[new_key] = value
        else:
            processed_old_state_dict[key] = value
    
    # åŠ è½½å¸¦æœ‰é¢„è®­ç»ƒä»»åŠ¡å¤´çš„æ¨¡å‹
    print("ğŸ”„ åŠ è½½å¸¦æœ‰é¢„è®­ç»ƒä»»åŠ¡å¤´çš„æ¨¡å‹...")
    model = LucaGPLMForPretraining.from_pretrained_old(old_model_path)
    model.eval()
    
    # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
    new_state_dict = model.state_dict()
    
    # è¯†åˆ«ä¸å¤´ç›¸å…³çš„é”®
    print("\nğŸ” è¯†åˆ«ä¸å¤´ç›¸å…³çš„é”®...")
    
    # æ—§æ¨¡å‹ä¸­ä¸å¤´ç›¸å…³çš„é”®
    old_head_keys = [k for k in processed_old_state_dict.keys() if any(term in k for term in ["head", "lm_", "contact_head"])]
    print(f"æ—§æ¨¡å‹ä¸­ä¸å¤´ç›¸å…³çš„é”®æ•°é‡: {len(old_head_keys)}")
    for key in old_head_keys:
        print(f"  - {key} - å½¢çŠ¶: {processed_old_state_dict[key].shape}")
    
    # æ–°æ¨¡å‹ä¸­ä¸å¤´ç›¸å…³çš„é”®
    new_head_keys = [k for k in new_state_dict.keys() if any(term in k for term in ["head", "lm_", "classifier", "hidden_layer"])]
    print(f"\næ–°æ¨¡å‹ä¸­ä¸å¤´ç›¸å…³çš„é”®æ•°é‡: {len(new_head_keys)}")
    for key in new_head_keys:
        print(f"  - {key} - å½¢çŠ¶: {new_state_dict[key].shape}")
    
    # åˆ›å»ºæ˜ å°„
    print("\nğŸ”„ åˆ›å»ºæ˜ å°„...")
    mapping = {}
    
    # ç›´æ¥æ˜ å°„
    direct_mappings = {
        "lm_head.weight": "lm_head.weight"
    }
    
    for old_key, new_key in direct_mappings.items():
        if old_key in processed_old_state_dict and new_key in new_state_dict:
            mapping[old_key] = new_key
            print(f"âœ… ç›´æ¥æ˜ å°„: {old_key} -> {new_key}")
    
    # åº”ç”¨æ˜ å°„
    print("\nğŸ”„ åº”ç”¨æ˜ å°„...")
    converted_state_dict = {}
    
    # é¦–å…ˆå¤åˆ¶æ‰€æœ‰æ–°æ¨¡å‹çš„åŸå§‹æƒé‡
    for key, value in new_state_dict.items():
        converted_state_dict[key] = value.clone()
    
    # ç„¶ååº”ç”¨æ˜ å°„
    for old_key, new_key in mapping.items():
        if old_key in processed_old_state_dict and new_key in new_state_dict:
            old_value = processed_old_state_dict[old_key]
            new_value_shape = new_state_dict[new_key].shape
            
            # å¦‚æœå½¢çŠ¶åŒ¹é…ï¼Œç›´æ¥å¤åˆ¶
            if old_value.shape == new_value_shape:
                converted_state_dict[new_key] = old_value.clone()
                print(f"âœ… æ˜ å°„: {old_key} -> {new_key}")
            else:
                print(f"âš ï¸  å½¢çŠ¶ä¸åŒ¹é…: {old_key} {old_value.shape} -> {new_key} {new_value_shape}")
                print(f"   å°†ä½¿ç”¨æ–°æ¨¡å‹çš„åŸå§‹å€¼")
    
    # ç»Ÿè®¡æ˜ å°„ç»“æœ
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ˜ å°„æ•°: {len(mapping)}")
    print(f"  æˆåŠŸæ˜ å°„æ•°: {sum(1 for old_key, new_key in mapping.items() if old_key in processed_old_state_dict and new_key in new_state_dict)}")
    
    # ä¿å­˜è½¬æ¢åçš„æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜è½¬æ¢åçš„æ¨¡å‹...")
    model.load_state_dict(converted_state_dict)
    model.save_pretrained(output_path)
    
    # åŠ è½½å¹¶ä¿å­˜tokenizer
    print("\nğŸ”„ åŠ è½½å¹¶ä¿å­˜tokenizer...")
    tokenizer = LucaGPLMTokenizer.from_pretrained_old(old_model_path)
    tokenizer.save_pretrained(output_path)
    
    # ä¿å­˜æ˜ å°„æ–‡ä»¶
    mapping_file = os.path.join(output_path, "final_mapping.json")
    with open(mapping_file, "w") as f:
        json.dump(mapping, f, indent=2)
    
    print(f"âœ… è½¬æ¢åçš„æ¨¡å‹å·²ä¿å­˜åˆ° {output_path}")
    print(f"âœ… Tokenizerå·²ä¿å­˜åˆ° {output_path}")
    print(f"âœ… æ˜ å°„æ–‡ä»¶å·²ä¿å­˜åˆ° {mapping_file}")
    
    # æ£€æŸ¥è½¬æ¢åçš„æ¨¡å‹
    print("\nğŸ” æ£€æŸ¥è½¬æ¢åçš„æ¨¡å‹...")
    loaded_model = LucaGPLMForPretraining.from_pretrained(output_path)
    loaded_state_dict = loaded_model.state_dict()
    
    # æ£€æŸ¥é¢„è®­ç»ƒä»»åŠ¡å¤´å‚æ•°
    task_params = [k for k in loaded_state_dict.keys() if any(term in k for term in ["classifier", "hidden_layer", "lm_head"])]
    print(f"é¢„è®­ç»ƒä»»åŠ¡å¤´å‚æ•°æ•°: {len(task_params)}")
    
    # æ£€æŸ¥pretrain_taskså­—å…¸ç»“æ„
    print("\nğŸ” æ£€æŸ¥pretrain_taskså­—å…¸ç»“æ„:")
    if hasattr(loaded_model, 'pretrain_tasks'):
        print("  âœ… æ¨¡å‹åŒ…å«pretrain_taskså­—å…¸")
        
        # æ£€æŸ¥token_levelä»»åŠ¡
        if "token_level" in loaded_model.pretrain_tasks:
            print("  âœ… åŒ…å«token_levelä»»åŠ¡")
            for task_name in loaded_model.pretrain_tasks["token_level"]:
                print(f"    - {task_name}")
        else:
            print("  âŒ ä¸åŒ…å«token_levelä»»åŠ¡")
        
        # æ£€æŸ¥span_levelä»»åŠ¡
        if "span_level" in loaded_model.pretrain_tasks:
            print("  âœ… åŒ…å«span_levelä»»åŠ¡")
            for task_name in loaded_model.pretrain_tasks["span_level"]:
                print(f"    - {task_name}")
        else:
            print("  âŒ ä¸åŒ…å«span_levelä»»åŠ¡")
        
        # æ£€æŸ¥seq_levelä»»åŠ¡
        if "seq_level" in loaded_model.pretrain_tasks:
            print("  âœ… åŒ…å«seq_levelä»»åŠ¡")
            for task_name in loaded_model.pretrain_tasks["seq_level"]:
                print(f"    - {task_name}")
        else:
            print("  âŒ ä¸åŒ…å«seq_levelä»»åŠ¡")
    else:
        print("  âŒ æ¨¡å‹ä¸åŒ…å«pretrain_taskså­—å…¸")
    
    # æµ‹è¯•tokenizer
    print("\nğŸ” æµ‹è¯•tokenizer...")
    test_tokenizer = LucaGPLMTokenizer.from_pretrained(output_path)
    test_sequence = "ATCGATCGATCG"  # Example gene sequence
    encoded = test_tokenizer.encode(test_sequence, add_special_tokens=True)
    decoded = test_tokenizer.decode(encoded)
    print(f"âœ… Tokenizeræµ‹è¯•æˆåŠŸ! è¾“å…¥: {test_sequence}, ç¼–ç : {encoded}, è§£ç : {decoded}")
    
    return mapping

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Convert old LucaOneVirus checkpoint to new LucaGPLMModel format",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--old-checkpoint",
        type=str,
        help="Path to the old checkpoint directory to convert"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="Output directory name/path for the converted model"
    )
    
    parser.add_argument(
        "--with-pretraining-heads",
        action="store_true",
        help="Whether to include pretraining heads in the converted model"
    )
    
    parser.add_argument(
        "--convert-with-pretraining-heads",
        action="store_true",
        help="Convert model weights with pretraining heads directly from old model"
    )
    
    return parser.parse_args()

def main():
    """Main entry point for the command line interface."""
    args = parse_arguments()
    
    if args.convert_with_pretraining_heads:
        # ä½¿ç”¨æ–°çš„è½¬æ¢æ¨¡å¼
        if not args.old_checkpoint:
            print("é”™è¯¯: --convert-with-pretraining-heads æ¨¡å¼éœ€è¦ --old-checkpoint å‚æ•°")
            sys.exit(1)
            
        print("è½¬æ¢å¸¦æœ‰é¢„è®­ç»ƒä»»åŠ¡å¤´çš„æ¨¡å‹æƒé‡...")
        print(f"æ—§æ¨¡å‹è·¯å¾„: {args.old_checkpoint}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print()
        
        try:
            mapping = convert_with_pretraining_heads(args.old_checkpoint, args.output_dir)
            print(f"\nğŸ‰ è½¬æ¢å®Œæˆï¼")
            print(f"è½¬æ¢åçš„æ¨¡å‹å·²ä¿å­˜åˆ° '{args.output_dir}'")
        except Exception as e:
            print(f"\nğŸ’¥ è½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        # ä½¿ç”¨åŸæœ‰çš„è½¬æ¢æ¨¡å¼
        if not args.old_checkpoint:
            print("é”™è¯¯: éœ€è¦ --old-checkpoint å‚æ•°")
            sys.exit(1)
            
        print("è½¬æ¢LucaOneVirusæ£€æŸ¥ç‚¹åˆ°LucaGPLMModelæ ¼å¼...")
        print(f"æ—§æ£€æŸ¥ç‚¹: {args.old_checkpoint}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"åŒ…å«é¢„è®­ç»ƒå¤´: {args.with_pretraining_heads}")
        print()
        
        success = convert_old_weights(args.old_checkpoint, args.output_dir, args.with_pretraining_heads)
        
        if success:
            print(f"\nğŸ‰ è½¬æ¢å®Œæˆï¼")
            print(f"è½¬æ¢åçš„æ¨¡å‹å·²ä¿å­˜ä¸º '{args.output_dir}'")
        else:
            print("\nğŸ’¥ è½¬æ¢å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯ã€‚")

if __name__ == "__main__":
    main()