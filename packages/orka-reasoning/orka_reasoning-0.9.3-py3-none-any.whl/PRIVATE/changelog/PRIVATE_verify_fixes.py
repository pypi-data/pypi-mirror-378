#!/usr/bin/env python3
"""
Verification script for the OrKa fixes applied to address the three main issues:
1. Agreement score extraction from synthesis responses
2. Memory readers returning empty results  
3. Template resolution in memory storage showing unresolved variables

This script tests each fix independently and provides clear pass/fail results.
"""

import json
import re
import subprocess
import sys
import time
from pathlib import Path


def test_agreement_score_extraction():
    """Test the enhanced agreement score extraction patterns."""
    print("🧪 Testing Agreement Score Extraction...")

    # Import the score extraction logic
    sys.path.append("orka")
    from nodes.loop_node import LoopNode

    # Create a test loop node with default score extraction config
    test_node = LoopNode("test", max_loops=1, score_threshold=0.5)

    # Test various agreement response formats
    test_responses = {
        "AGREEMENT format": {
            "agreement_moderator": {
                "response": "The consensus shows AGREEMENT: 0.85 between all parties"
            }
        },
        "SCORE format": {"synthesis_attempt": {"response": "Final synthesis SCORE: 7.5 out of 10"}},
        "Percentage format": {
            "quality_moderator": {"response": "Quality assessment shows 78% agreement"}
        },
        "Points format": {
            "agreement_finder": {"response": "Consensus strength: 8.2 points on this issue"}
        },
        "Fraction format": {"moderator": {"response": "Agreement level: 7.8/10 based on analysis"}},
    }

    results = []
    for format_name, response_data in test_responses.items():
        score = test_node._extract_score(response_data)
        expected_scores = {
            "AGREEMENT format": 0.85,
            "SCORE format": 7.5,
            "Percentage format": 78.0,
            "Points format": 8.2,
            "Fraction format": 7.8,
        }
        expected = expected_scores[format_name]

        if abs(score - expected) < 0.1:
            results.append(f"  ✅ {format_name}: {score} (expected ~{expected})")
        else:
            results.append(f"  ❌ {format_name}: {score} (expected ~{expected})")

    print("\n".join(results))
    print()


def test_template_resolution():
    """Test template resolution in memory writer and prompt rendering."""
    print("🧪 Testing Template Resolution...")

    sys.path.append("orka")
    from nodes.memory_writer_node import MemoryWriterNode
    from orchestrator.prompt_rendering import PromptRenderer

    # Test prompt rendering helper functions
    renderer = PromptRenderer()
    test_payload = {
        "input": {
            "input": "What is effective collaboration?",
            "loop_number": 2,
            "previous_outputs": {
                "past_loops": [{"round": 1, "synthesis_insights": "Collaboration requires trust"}]
            },
        },
        "previous_outputs": {"test_agent": {"response": "Test response for collaboration"}},
    }

    # Test helper functions
    test_template = "Topic: {{ get_input() }}, Loop: {{ get_loop_number() }}, Agent: {{ get_agent_response('test_agent') }}"
    result = renderer.render_prompt(test_template, test_payload)

    if "{{ get_input() }}" not in result and "What is effective collaboration?" in result:
        print("  ✅ Template helper functions work correctly")
        print(f"  📝 Result: {result}")
    else:
        print("  ❌ Template helper functions failed")
        print(f"  📝 Result: {result}")

    # Test memory writer template resolution
    memory_writer = MemoryWriterNode("test_writer")

    test_metadata = {
        "original_input": "{{ get_input() }}",
        "loop_number": "{{ get_loop_number() }}",
        "agent_type": "test_agent",
    }

    rendered_metadata = memory_writer._render_metadata_templates(test_metadata, test_payload)

    if "{{ get_input() }}" not in str(
        rendered_metadata
    ) and "What is effective collaboration?" in str(rendered_metadata):
        print("  ✅ Memory writer template resolution works correctly")
        print(f"  📝 Metadata: {rendered_metadata}")
    else:
        print("  ❌ Memory writer template resolution failed")
        print(f"  📝 Metadata: {rendered_metadata}")

    print()


def test_redisstack_search_syntax():
    """Test RedisStack search query construction."""
    print("🧪 Testing RedisStack Search Query Construction...")

    sys.path.append("orka")
    from memory.redisstack_logger import RedisStackMemoryLogger

    # Test query construction logic
    test_cases = [
        ("", "*"),  # Empty query should become wildcard
        ("simple query", "@content:simple query"),
        ("query: with colon", "@content:query\\: with colon"),  # Should escape colons
        ("query (with parens)", "@content:query \\(with parens\\)"),  # Should escape parens
    ]

    results = []
    for input_query, expected_pattern in test_cases:
        # Simulate the query construction logic
        if input_query.strip():
            escaped_query = input_query.replace(":", "\\:").replace("(", "\\(").replace(")", "\\)")
            search_query = f"@content:{escaped_query}"
        else:
            search_query = "*"

        if expected_pattern in search_query or search_query == expected_pattern:
            results.append(f"  ✅ Query '{input_query}' → '{search_query}'")
        else:
            results.append(
                f"  ❌ Query '{input_query}' → '{search_query}' (expected pattern: {expected_pattern})"
            )

    print("\n".join(results))
    print()


def test_memory_ttl_configuration():
    """Test memory TTL configuration in the workflow."""
    print("🧪 Testing Memory TTL Configuration...")

    workflow_path = Path("docs/exp_local_SOC-02/cognitive_society_with_memory_local.yml")

    if not workflow_path.exists():
        print("  ❌ Workflow file not found")
        return

    with open(workflow_path, "r") as f:
        content = f.read()

    # Check for proper TTL values (should be hours, not fractions)
    issues = []

    if "default_short_term_hours: 2.0" in content:
        print("  ✅ Global short_term_hours set to 2.0 hours")
    else:
        issues.append("Global short_term_hours not set to 2.0")

    if "default_long_term_hours: 24.0" in content:
        print("  ✅ Global long_term_hours set to 24.0 hours")
    else:
        issues.append("Global long_term_hours not set to 24.0")

    # Check for old problematic values
    if "0.08" in content or "0.15" in content or "0.18" in content:
        issues.append("Found old short TTL values (0.08, 0.15, 0.18 hours)")
    else:
        print("  ✅ No short TTL values found")

    if issues:
        print("  ❌ TTL Configuration Issues:")
        for issue in issues:
            print(f"    - {issue}")
    else:
        print("  ✅ All TTL configurations look good")

    print()


def run_quick_workflow_test():
    """Run a quick test of the cognitive society workflow."""
    print("🧪 Running Quick Workflow Test...")

    try:
        # Run the workflow with a simple question
        cmd = [
            "python",
            "-m",
            "orka.orka_cli",
            "run",
            "docs/exp_local_SOC-02/cognitive_society_with_memory_local.yml",
            "What is teamwork?",
        ]

        print("  🔄 Starting workflow test (this may take a few minutes)...")
        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=300
        )  # 5 minute timeout

        if result.returncode == 0:
            print("  ✅ Workflow completed successfully")

            # Check for trace file
            trace_files = list(Path("logs").glob("orka_trace_*.json"))
            if trace_files:
                latest_trace = max(trace_files, key=lambda p: p.stat().st_mtime)
                print(f"  📁 Latest trace: {latest_trace}")

                # Quick validation of trace content
                with open(latest_trace, "r") as f:
                    trace_data = json.load(f)

                # Check for score extraction
                executions = trace_data.get("agent_executions", [])
                score_found = False
                memory_found = False
                template_resolved = False

                for execution in executions:
                    payload = execution.get("payload", {})

                    # Check for non-zero scores in past_loops
                    if "input" in payload:
                        input_data = payload["input"]
                        if "previous_outputs" in input_data:
                            past_loops = input_data["previous_outputs"].get("past_loops", [])
                            for loop in past_loops:
                                if loop.get("score", 0) > 0:
                                    score_found = True
                                    print(f"  ✅ Found non-zero score: {loop['score']}")
                                    break

                    # Check for stored memories
                    if "stored_metadata" in payload:
                        memory_found = True
                        metadata = payload["stored_metadata"]
                        original_input = metadata.get("original_input", "")
                        if "{{ get_input() }}" not in original_input and original_input != "":
                            template_resolved = True
                            print(f"  ✅ Template resolved in metadata: {original_input}")

                if score_found:
                    print("  ✅ Agreement scores are being extracted properly")
                else:
                    print("  ⚠️  No non-zero agreement scores found")

                if memory_found:
                    print("  ✅ Memories are being stored")
                else:
                    print("  ⚠️  No memory storage detected")

                if template_resolved:
                    print("  ✅ Templates are being resolved in metadata")
                else:
                    print("  ⚠️  Template resolution in metadata not confirmed")

        else:
            print("  ❌ Workflow failed")
            print(f"  📝 Error: {result.stderr}")

    except subprocess.TimeoutExpired:
        print("  ⚠️  Workflow test timed out (5 minutes)")
    except Exception as e:
        print(f"  ❌ Workflow test failed: {e}")

    print()


def main():
    """Run all verification tests."""
    print("🔧 OrKa Fixes Verification Script")
    print("=" * 50)
    print()

    # Run all tests
    test_agreement_score_extraction()
    test_template_resolution()
    test_redisstack_search_syntax()
    test_memory_ttl_configuration()

    # Optional workflow test (can be commented out for faster testing)
    print("🚀 Ready to run full workflow test? (This will take a few minutes)")
    response = input("Run workflow test? (y/N): ").strip().lower()
    if response in ["y", "yes"]:
        run_quick_workflow_test()
    else:
        print("  ⏭️  Skipping workflow test")

    print()
    print("🎉 Verification complete!")
    print()
    print("Summary of fixes applied:")
    print("1. ✅ Enhanced agreement score extraction patterns")
    print("2. ✅ Fixed RedisStack FT.SEARCH syntax errors")
    print("3. ✅ Added template helper functions for memory storage")
    print("4. ✅ Updated memory TTL from minutes to hours")
    print()
    print("Next steps:")
    print("- Test the cognitive society workflow with a real question")
    print("- Monitor trace files for proper score extraction and memory storage")
    print("- Check TUI memory display for stored memories")


if __name__ == "__main__":
    main()
