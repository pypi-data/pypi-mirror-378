# GraphScout Agent Example Configuration
# =====================================
#
# This example demonstrates how to use the GraphScout agent for intelligent
# path discovery and selection in complex workflows.

orchestrator:
  id: graph_scout_demo
  strategy: sequential
  queue: redis
  agents:
    - graph_scout_router
    - input_classifier
    - search_agent
    - analysis_agent
    - memory_writer
    - response_builder

agents:
  # GraphScout agent for intelligent routing with two-stage LLM evaluation
  - id: graph_scout_router
    type: graph-scout
    params:
      k_beam: 5  # Limit to top 5 candidates for efficiency
      max_depth: 3  # Limit to 2-hop paths for faster evaluation
      commit_margin: 0.1
      require_terminal: true
      score_weights:
        llm: 0.5
        heuristics: 0.25
        prior: 0.15
        cost: 0.05
        latency: 0.05
      safety_profile: default
      cost_budget_tokens: 1000
      latency_budget_ms: 5000
      max_preview_tokens: 200
      # Two-stage LLM evaluation configuration (now configurable instead of hardcoded)
      evaluation_model: "local_llm"                # Fast local model for Stage 1
      evaluation_model_name: "gpt-oss:20b"     # Configurable model name for evaluation
      validation_model: "local_llm"                # Fast local model for Stage 2
      validation_model_name: "gpt-oss:20b"     # Configurable model name for validation
      llm_evaluation_enabled: true                 # Enable LLM-powered evaluation
      fallback_to_heuristics: true                 # Fallback if LLM fails
    prompt: |
      You are GraphScout, an intelligent routing agent.
      
      Analyze the question and available workflow paths:
      Question: {{ input }}
      Classification: {{ previous_outputs.input_classifier.response }}
      
      Consider:
      - Question type and complexity
      - Required capabilities
      - Cost and latency constraints
      - Safety requirements
      
      Select the optimal next path based on relevance, efficiency, and safety.
      Return your decision as JSON with reasoning.

  # Input classification to understand the question type
  - id: input_classifier
    type: openai-classification
    prompt: |
      Classify this question into one of these categories:
      - factual: Questions asking for facts or information
      - analytical: Questions requiring analysis or reasoning
      - creative: Questions asking for creative content
      - technical: Technical or programming questions
      
      Question: {{ input }}
      
      Return only the category name.

  # Search agent for factual questions
  - id: search_agent
    type: duckduckgo
    capabilities: [data_retrieval, web_search]
    prompt: |
      {% if "news" in input.lower() or "today" in input.lower() %}
      latest news headlines today current events
      {% else %}
      {{ input }}
      {% endif %}

  # Analysis agent for complex reasoning
  - id: analysis_agent
    type: local_llm
    capabilities: [reasoning, analysis]
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    prompt: |
      Provide a detailed analysis of this question:
      {{ input }}
      
      Consider multiple perspectives and provide reasoning.

  # Memory storage for important information
  - id: memory_writer
    type: memory
    namespace: graph_scout_demo
    memory_preset: "episodic"  # Personal narrative and interactions
    config:
      operation: write
      vector: true
    prompt: |
      Store this information for future reference:
      
      Question: {{ input }}
      {% if previous_outputs.input_classifier %}
      Classification: {{ previous_outputs.input_classifier.response }}
      {% endif %}
      {% if previous_outputs.search_agent %}
      Search Results: {{ previous_outputs.search_agent.result }}
      {% endif %}
      {% if previous_outputs.analysis_agent %}
      Analysis: {{ previous_outputs.analysis_agent.response }}
      {% endif %}

  # Final response builder
  - id: response_builder
    type: local_llm
    capabilities: [answer_emit, response_generation]
    model: gpt-oss:20b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    prompt: |
      Create a comprehensive response based on the available information:
      
      Original Question: {{ input }}
      {% if previous_outputs.input_classifier %}
      Question Type: {{ previous_outputs.input_classifier.response }}
      {% endif %}

      {% if previous_outputs.search_agent %}
      Search Information: {{ previous_outputs.search_agent }}
      {% endif %}

      {% if previous_outputs.analysis_agent %}
      Analysis: {{ previous_outputs.analysis_agent.response }}
      {% endif %}

      {% if previous_outputs.memory_writer %}
      Memory Context: Information has been stored for future reference.
      {% endif %}

      Provide a clear, helpful response that addresses the user's question.
      Include sources if available and explain your reasoning.
