{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase 2 - Data Dictionaries\n",
    "\n",
    "In this notebook, we will be working with data downloaded from the Saudi portal, specifically in CSV format. Our goal is to create a data dictionary using OpenAI's capabilities. The data dictionary will provide detailed descriptions of the data fields based on the content of the CSV files. By leveraging OpenAI, we can generate comprehensive and accurate descriptions, enhancing our understanding of the dataset and facilitating further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import re\n",
    "load_dotenv()\n",
    "\n",
    "openai = OpenAI(\n",
    "            base_url=os.getenv(\"BASE_URL\"),\n",
    "            api_key=os.getenv(\"OR_API_KEY\") or os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(base_dir, exts='.csv'):\n",
    "    files_list = []\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        # Skip directories that contain 'data_dictionary' in their path\n",
    "        if 'data_dictionary' in root.split(os.sep):\n",
    "            continue\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in exts):\n",
    "                # Append the full path of the csv file to the list\n",
    "                files_list.append(os.path.join(root, file))\n",
    "    return files_list\n",
    "\n",
    "base_directory = 'opendata'\n",
    "file_paths = list_files(base_directory,exts = \".csv\")\n",
    "#print(file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CSVs\n",
    "\n",
    "We want to load the CSVs to be able to get a view of the top rows including a sample of the first few values to be able to view the data and the corresponding data types.\n",
    "\n",
    "This will allow us to create data dictionaries for the corresponding datasets using OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_previews(file_paths,verbose=None):\n",
    "    previews = {}\n",
    "    for file_path in file_paths:\n",
    "        # Determine the file extension\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_extension == '.csv':\n",
    "            # Load the CSV file into a DataFrame with error handling for encoding issues\n",
    "            #print(file_path)\n",
    "            for encoding in ['utf-8', 'ISO-8859-1', 'cp1256']:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=encoding,low_memory=False)\n",
    "                    break  # Exit the loop if reading is successful\n",
    "                except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "                   # print(file_path)\n",
    "                    continue  # Try the next encoding if there's an error\n",
    "            \n",
    "            # Remove completely empty rows\n",
    "            df.dropna(how='all', inplace=True)\n",
    "        elif file_extension == '.json':\n",
    "            # Load the JSON file into a DataFrame\n",
    "            try:\n",
    "                df = pd.read_json(file_path)\n",
    "            except ValueError as e:\n",
    "                if verbose: print(f\"Error reading JSON file {file_path}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            if verbose: print(f\"Unsupported file extension: {file_extension}\")\n",
    "            continue\n",
    "        # Get the first 30 rows including the column names\n",
    "        preview = df.head(35).to_string(index=False)\n",
    "        \n",
    "        # Append the file path and its preview to the dictionary\n",
    "        previews[file_path] = preview\n",
    "    \n",
    "    return previews\n",
    "\n",
    "\n",
    "\n",
    "previews = get_data_previews(file_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Dictionary Creation\n",
    "\n",
    "The creation of the data dictionary is a step using `openai` to be able to review a sample dataset / csv in a certain format and being able to determine the data type that would likely be necessary to facilitate the use of a certain dataset.\n",
    "\n",
    "So we accomplish the creation of a data dictionary with the following approach:\n",
    "\n",
    "- Set the system to be an expert database administrator whose sole job is to create a CSV of a data dictionary and the types so that we can understand a dataset effectively\n",
    "- Set the assistant as a guide for the system to create a data dictionary effectively based on a single set of instructions with certain limitations on the topic at hand based on the file name\n",
    "- Set the user message to handle the input of the CSV snippet taken from the original datasets taken from the open data platform.\n",
    "\n",
    "Below is the definition of the `create_data_dictionary` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data_dictionary(preview, csv_name):\n",
    "    #You will only respond in valid CSV files that do not include | operators or anything else. Only ensure that you create a comma-separated table as a data dictionary\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=os.getenv(\"OR_MODEL\"),\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in database administration and are able to quickly create a data dictionary from a sample of the first 10 values from a dataset. You will only respond with a data dictionary where you will create the result into 3 columns: variable name (the name of the corresponding variable. If it's not given, provide it), type (describe the actual data type according to the sample provided), description (provide a description of the variable for better understanding of the dataset). You will only respond in valid CSV files that do not include | operators or anything else. Only ensure that you create a comma-separated table as a data dictionary\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"You will help the system understand that it must provide a variable name, data type and description of the variable in a csv format to be able to create a data dictionary. Here is the name of the file for full context: {csv_name}\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the data sample:\" + preview[:min(4020,len(preview))]\n",
    "            }\n",
    "        ],\n",
    "        extra_headers={\n",
    "            \"HTTP-Referer\": os.getenv(\"REFERRER\"),\n",
    "            \"X-Title\": os.getenv(\"TITLE\"),\n",
    "        },\n",
    "        max_completion_tokens=400,\n",
    "        temperature=0\n",
    "    )\n",
    "    return re.sub(r'```csv|```', '', response.choices[0].message.content).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data Dictionary and Process CSVs\n",
    "\n",
    "`save_data_dictionary` - Simple function to save OpenAI's hardwork to a CSV file.\n",
    "`process_csv_files` - This function takes on a few functionalities for the purpose of the vast amounts of messy datasets does the following: - Get all of the CSVs - Get the first preview of the CSVs - Based on the preview, pass it to OpenAI to create the dataset - Record the CSVs of the data dictionaries - Return the results of the data dictionaries to a list/dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_dictionary(data_dict, output_path):\n",
    "    # Save the data dictionary to a CSV file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(data_dict)\n",
    "\n",
    "def process_files(base_dir, max_workers=None):\n",
    "    csv_file_paths = list_files(base_dir)\n",
    "    csv_previews = get_data_previews(csv_file_paths)\n",
    "    data_dictionaries = {}\n",
    "\n",
    "    def process_single_file(file_path, preview):\n",
    "        # Get the directory and file name\n",
    "        dir_name = os.path.dirname(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Define the output directory and file path\n",
    "        output_dir = os.path.join(dir_name, 'data_dictionary')\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}_data_dictionary.csv\")\n",
    "        \n",
    "        # Check if the data dictionary already exists\n",
    "        if os.path.exists(output_path):\n",
    "            # Load the existing data dictionary as a string\n",
    "            with open(output_path, 'r') as f:\n",
    "                data_dict = f.read()\n",
    "        else:\n",
    "            # Create the data dictionary\n",
    "            data_dict = create_data_dictionary(preview, file_name)\n",
    "            # Create the output directory if it doesn't exist\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            # Save the data dictionary\n",
    "            save_data_dictionary(data_dict, output_path)\n",
    "        \n",
    "        # Assign the data dictionary to the dictionary with the full file path as the key\n",
    "        data_dictionaries[file_path] = data_dict\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the processing of CSV files\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_file, file_path, preview) for file_path, preview in csv_previews.items()]\n",
    "        concurrent.futures.wait(futures)\n",
    "    \n",
    "    return data_dictionaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_directory = 'opendata'\n",
    "results = process_files(base_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
