id: litellm-ollama-1.73.0-docker
name: LiteLLM + Ollama
version: 1.73.0 (stable)
maintainer: Your Name
description_short: LiteLLM is an open-source library that simplifies the integration of large language models (LLMs) into applications.
description: |
  LiteLLM is an open-source library that simplifies the integration of large language models (LLMs) into 
  applications. It provides a unified API for interacting with various LLM providers, making it easier to 
  switch between models and manage API keys. In MLOX, LiteLLM is used to provide a flexible and scalable 
  solution for integrating LLMs into your machine learning workflows. This stack also includes Ollama, a 
  lightweight, local LLM serving solution.
links:
  project: https://litellm.vercel.app/
  news: https://litellm.vercel.app/
  security: https://litellm.vercel.app/
  documentation: https://litellm.vercel.app/
  changelog: https://litellm.vercel.app/
requirements:
  cpus: 2.0
  ram_gb: 4.0
  disk_gb: 10.0
groups:
  llm:
  service:
  backend:
    docker:
ports:  # These are 'preferred' ports, they can and possibly will be changed by automation
  web_ui: 5222
ui:
  setup: mlox.services.litellm.ui.setup
  settings: mlox.services.litellm.ui.settings
build:
  class_name: mlox.services.litellm.docker.LiteLLMDockerService
  params:
    name: litellm-ollama-1.73.0
    template: ${MLOX_STACKS_PATH}/litellm/docker-compose-litellm-ollama-1.73.0.yaml 
    target_path: /home/${MLOX_USER}/litellm-1.73.0
    ollama_script: ${MLOX_STACKS_PATH}/litellm/entrypoint.sh
    litellm_config: ${MLOX_STACKS_PATH}/litellm/litellm-config.yaml
    ui_user: ${MLOX_AUTO_USER}
    ui_pw: ${MLOX_AUTO_PW}
    ui_port: ${MLOX_AUTO_PORT_WEB_UI}
    service_port: "4000"
    slack_webhook: ""
    api_key: ${MLOX_AUTO_API_KEY}

