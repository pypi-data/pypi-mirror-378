# TextTools

## ğŸ“Œ Overview

**TextTools** is a high-level **NLP toolkit** built on top of modern **LLMs**.  

It provides ready-to-use utilities for **translation, question detection, keyword extraction, categorization, NER extractor, and more** â€” designed to help you integrate AI-powered text processing into your applications with minimal effort.

**Thread Safety:** All methods in TheTool are thread-safe, allowing concurrent usage across multiple threads without conflicts.

---

## âœ¨ Features

TextTools provides a rich collection of high-level NLP utilities built on top of LLMs.  
Each tool is designed to work out-of-the-box with structured outputs (JSON / Pydantic).

- **Categorizer** â†’ Zero-finetuning text categorization for fast, scalable classification.  
- **Keyword Extractor** â†’ Identify the most important keywords in a text.  
- **Question Merger** â†’ Merge the provided questions, preserving all the main points 
- **NER (Named Entity Recognition) Extractor** â†’ Extract people, places, organizations, and other entities.  
- **Question Detector** â†’ Determine whether a text is a question or not.  
- **Question Generator From Text** â†’ Generate high-quality, context-relevant questions from provided text.
- **Question Generator From Subject** â†’ Generate high-quality, context-relevant questions from a subject.
- **Rewriter** â†’ Rewrite text while preserving meaning or without it.
- **Summarizer** â†’ Condense long passages into clear, structured summaries. 
- **Translator** â†’ Translate text across multiple languages, with support for custom rules.

---

## âš™ï¸ with_analysis, logprobs, output_lang, and user_prompt parameters

TextTools provides several optional flags to customize LLM behavior:

- **`with_analysis=True`** â†’ Adds a reasoning step before generating the final output. Useful for debugging, improving prompts, or understanding model behavior.  
  âš ï¸ Note: This doubles token usage per call because it triggers an additional LLM request.

- **`logprobs=True`** â†’ Returns token-level probabilities for the generated output. You can also specify `top_logprobs=<N>` to get the top N alternative tokens and their probabilities.  

- **`output_lang="en"`** â†’ Forces the model to respond in a specific language. The model will ignore other instructions about language and respond strictly in the requested language.

- **`user_prompt="..."`** â†’ Allows you to inject a custom instruction or prompt into the model alongside the main template. This gives you fine-grained control over how the model interprets or modifies the input text.

All these flags can be used individually or together to tailor the behavior of any tool in **TextTools**.

---

## ğŸš€ Installation

Install the latest release via PyPI:

```bash
pip install -U hamta-texttools
```

---

## âš¡ Quick Start

```python
from openai import OpenAI

from texttools import TheTool

# Create your OpenAI client
client = OpenAI(base_url = "your_url", API_KEY = "your_api_key")

# Specify the model
model = "gpt-4o-mini"

# Create an instance of TheTool
# âš ï¸ Note: Enabling `with_analysis=True` provides deeper insights but incurs additional LLM calls and token usage.
the_tool = TheTool(client = client, model = model, with_analysis = True)

# Example: Question Detection
print(the_tool.detect_question("Is this project open source?")["result"])
# Output: True

# Example: Translation
print(the_tool.translate("Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ", target_language="English")["result"])
# Output: "Hi! How are you?"
```

---

## ğŸ“š Use Cases

Use **TextTools** when you need to:

- ğŸ” **Classify** large datasets quickly without model training  
- ğŸŒ **Translate** and process multilingual corpora with ease  
- ğŸ§© **Integrate** LLMs into production pipelines (structured outputs)  
- ğŸ“Š **Analyze** large text collections using embeddings and categorization  
- ğŸ‘ **Automate** common text-processing tasks without reinventing the wheel  

---

## ğŸ¤ Contributing

Contributions are welcome!  
Feel free to **open issues, suggest new features, or submit pull requests**.  

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
