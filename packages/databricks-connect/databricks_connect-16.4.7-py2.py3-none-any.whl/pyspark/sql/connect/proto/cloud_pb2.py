#
# DATABRICKS CONFIDENTIAL & PROPRIETARY
# __________________
#
# Copyright 2023-present Databricks, Inc.
# All Rights Reserved.
#
# NOTICE:  All information contained herein is, and remains the property of Databricks, Inc.
# and its suppliers, if any.  The intellectual and technical concepts contained herein are
# proprietary to Databricks, Inc. and its suppliers and may be covered by U.S. and foreign Patents,
# patents in process, and are protected by trade secret and/or copyright law. Dissemination, use,
# or reproduction of this information is strictly forbidden unless prior written permission is
# obtained from Databricks, Inc.
#
# If you view or obtain a copy of this information and believe Databricks, Inc. may not have
# intended it to be made available, please promptly report it to Databricks Legal Department
# @ legal@databricks.com.
#
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: spark/connect/cloud.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from pyspark.sql.connect.proto import types_pb2 as spark_dot_connect_dot_types__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n\x19spark/connect/cloud.proto\x12\rspark.connect\x1a\x19spark/connect/types.proto"\xb2\x05\n\rResultOptions\x12\x35\n\x04type\x18\x01 \x01(\x0e\x32!.spark.connect.ResultOptions.TypeR\x04type\x12R\n\x0c\x63loudOptions\x18\x02 \x01(\x0b\x32).spark.connect.ResultOptions.CloudOptionsH\x00R\x0c\x63loudOptions\x88\x01\x01\x1a\xac\x03\n\x0c\x43loudOptions\x12H\n\x06\x66ormat\x18\x01 \x01(\x0e\x32\x30.spark.connect.ResultOptions.CloudOptions.FormatR\x06\x66ormat\x12+\n\x0euseCompression\x18\x02 \x01(\x08H\x00R\x0euseCompression\x88\x01\x01\x12 \n\trow_limit\x18\x03 \x01(\x03H\x01R\x08rowLimit\x88\x01\x01\x12"\n\nbyte_limit\x18\x04 \x01(\x03H\x02R\tbyteLimit\x88\x01\x01\x12=\n\x18result_retention_seconds\x18\x05 \x01(\x03H\x03R\x16resultRetentionSeconds\x88\x01\x01"S\n\x06\x46ormat\x12\x16\n\x12\x46ORMAT_UNSPECIFIED\x10\x00\x12\x10\n\x0c\x46ORMAT_ARROW\x10\x01\x12\x0e\n\nFORMAT_CSV\x10\x02\x12\x0f\n\x0b\x46ORMAT_JSON\x10\x03\x42\x11\n\x0f_useCompressionB\x0c\n\n_row_limitB\r\n\x0b_byte_limitB\x1b\n\x19_result_retention_seconds"V\n\x04Type\x12\x14\n\x10TYPE_UNSPECIFIED\x10\x00\x12\x0f\n\x0bTYPE_INLINE\x10\x01\x12\x0e\n\nTYPE_CLOUD\x10\x02\x12\x17\n\x13TYPE_CLOUD_MANIFEST\x10\x03\x42\x0f\n\r_cloudOptions"\xac\x02\n\x10\x43loudResultBatch\x12@\n\x07results\x18\x01 \x03(\x0b\x32&.spark.connect.CloudResultBatch.ResultR\x07results\x12\x1c\n\ttruncated\x18\x02 \x01(\x08R\ttruncated\x1a\xb7\x01\n\x06Result\x12\x10\n\x03url\x18\x02 \x01(\tR\x03url\x12\x1b\n\trow_count\x18\x03 \x01(\x03R\x08rowCount\x12\'\n\x0f\x63ompressed_size\x18\x04 \x01(\x03R\x0e\x63ompressedSize\x12+\n\x11uncompressed_size\x18\x05 \x01(\x03R\x10uncompressedSize\x12(\n\x10secret_file_path\x18\x06 \x01(\tR\x0esecretFilePath"9\n\x14SubscribeToLiveQuery\x12!\n\x0coperation_id\x18\x01 \x01(\tR\x0boperationId"\xc2\x03\n\x0eResultManifest\x12\x39\n\x07results\x18\x01 \x01(\x0b\x32\x1f.spark.connect.CloudResultBatchR\x07results\x12H\n\x06\x66ormat\x18\x02 \x01(\x0e\x32\x30.spark.connect.ResultOptions.CloudOptions.FormatR\x06\x66ormat\x12!\n\x0c\x61rrow_schema\x18\x03 \x01(\x0cR\x0b\x61rrowSchema\x12]\n\x12result_compression\x18\x04 \x01(\x0e\x32..spark.connect.ResultManifest.CompressionCodecR\x11resultCompression\x12-\n\x12uncompressed_bytes\x18\x05 \x01(\x03R\x11uncompressedBytes\x12)\n\x10\x63ompressed_bytes\x18\x06 \x01(\x03R\x0f\x63ompressedBytes"O\n\x10\x43ompressionCodec\x12\x1a\n\x16\x43OMPRESSION_CODEC_NONE\x10\x00\x12\x1f\n\x1b\x43OMPRESSION_CODEC_LZ4_FRAME\x10\x01"\xd8\x01\n\x13\x43loudResultManifest\x12>\n\x08manifest\x18\x01 \x01(\x0b\x32\x1d.spark.connect.ResultManifestH\x00R\x08manifest\x88\x01\x01\x12(\n\x10secret_file_path\x18\x02 \x01(\tR\x0esecretFilePath\x12\'\n\x0f\x65xpiration_time\x18\x03 \x01(\x03R\x0e\x65xpirationTime\x12!\n\x0c\x63rc_checksum\x18\x04 \x01(\x03R\x0b\x63rcChecksumB\x0b\n\t_manifestB"\n\x1eorg.apache.spark.connect.protoP\x01\x62\x06proto3'
)

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(
    DESCRIPTOR, "pyspark.sql.connect.proto.cloud_pb2", globals()
)
if _descriptor._USE_C_DESCRIPTORS == False:
    DESCRIPTOR._options = None
    DESCRIPTOR._serialized_options = b"\n\036org.apache.spark.connect.protoP\001"
    _RESULTOPTIONS._serialized_start = 72
    _RESULTOPTIONS._serialized_end = 762
    _RESULTOPTIONS_CLOUDOPTIONS._serialized_start = 229
    _RESULTOPTIONS_CLOUDOPTIONS._serialized_end = 657
    _RESULTOPTIONS_CLOUDOPTIONS_FORMAT._serialized_start = 497
    _RESULTOPTIONS_CLOUDOPTIONS_FORMAT._serialized_end = 580
    _RESULTOPTIONS_TYPE._serialized_start = 659
    _RESULTOPTIONS_TYPE._serialized_end = 745
    _CLOUDRESULTBATCH._serialized_start = 765
    _CLOUDRESULTBATCH._serialized_end = 1065
    _CLOUDRESULTBATCH_RESULT._serialized_start = 882
    _CLOUDRESULTBATCH_RESULT._serialized_end = 1065
    _SUBSCRIBETOLIVEQUERY._serialized_start = 1067
    _SUBSCRIBETOLIVEQUERY._serialized_end = 1124
    _RESULTMANIFEST._serialized_start = 1127
    _RESULTMANIFEST._serialized_end = 1577
    _RESULTMANIFEST_COMPRESSIONCODEC._serialized_start = 1498
    _RESULTMANIFEST_COMPRESSIONCODEC._serialized_end = 1577
    _CLOUDRESULTMANIFEST._serialized_start = 1580
    _CLOUDRESULTMANIFEST._serialized_end = 1796
# @@protoc_insertion_point(module_scope)
