{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1555d26d",
   "metadata": {},
   "source": [
    "# 🚀 OpenSloth Demo Training Notebook\n",
    "\n",
    "This notebook have 2 main sections:\n",
    " 1. Training a model with OpenSloth\n",
    " 2. Training with unsloth\n",
    "\n",
    "- Setup:\n",
    "- Both use the same setup with datasets, sequences and global batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fda2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init(project=\"compare-unsloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6008af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 64\n",
      "[MP] Running on 1 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m03:29:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:41\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/exps/qwen3-14b-FineTome-1gpus-seql-packing\u001b[0m\n",
      "\u001b[32m03:29:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:44\u001b[0m | \u001b[1m🚀 Starting total training timer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Using compiler location: .cache/unsloth_compiled_cache_0\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n",
      "\u001b[32m03:29:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_loading: 8.57s\u001b[0m\n",
      "\u001b[32m03:29:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:124\u001b[0m | \u001b[1m[GPU=0] NCCL env: RANK=0, WORLD_SIZE=1, MASTER_ADDR=127.0.0.1, MASTER_PORT=29501\u001b[0m\n",
      "\u001b[32m03:29:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mnccl_grad_sync.py:128\u001b[0m | \u001b[1m[GPU=0] Setting current CUDA device to:0, os.environ['CUDA_VISIBLE_DEVICES']='0'\u001b[0m\n",
      "\u001b[32m03:29:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:50\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Qwen2TokenizerFast\u001b[0m\n",
      "\u001b[32m03:29:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  lora_setup: 3.23s\u001b[0m\n",
      "\u001b[32m03:29:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:74\u001b[0m | \u001b[1mApplied chat template: qwen3\u001b[0m\n",
      "\u001b[32m03:29:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_init: 11.81s\u001b[0m\n",
      "\u001b[32m03:29:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:148\u001b[0m | \u001b[1mCreating final SFTTrainer with prepared dataset...\u001b[0m\n",
      "\u001b[32m03:29:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:161\u001b[0m | \u001b[1mReplacing DataCollatorForLanguageModeling with DataCollatorForSeq2Seq for better sequence handling\u001b[0m\n",
      "\u001b[32m03:29:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:169\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n",
      "\u001b[32m03:29:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:122\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n",
      "\u001b[32m03:29:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  total_setup: 24.12s\u001b[0m\n",
      "\u001b[32m03:29:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_and_training_setup: 24.14s\u001b[0m\n",
      "\u001b[32m03:29:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:70\u001b[0m | \u001b[1mSingle GPU training detected, skipping NCCL gradient sync\u001b[0m\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: anhvth to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/anhvth5/projects/opensloth/wandb/run-20250610_032947-wpikx8uc\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run outputs/exps/qwen3-14b-FineTome-1gpus-seql-packing\n",
      "wandb: ⭐️ View project at https://wandb.ai/anhvth/compare-unsloth\n",
      "wandb: 🚀 View run at https://wandb.ai/anhvth/compare-unsloth/runs/wpikx8uc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "[LOCAL_RANK=0] Patching log. Dir: outputs/exps/qwen3-14b-FineTome-1gpus-seql-packing, GPUs: 1\n",
      "[LOCAL_RANK=0] Log patch initialization complete.\n",
      "🔧 Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:00<?, ?it/s]\u001b[32m03:29:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 1\u001b[0m\n",
      "\u001b[32m03:29:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 0: emitting 10000 indices\n",
      "First ids dataset samples: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "...Last ids: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n",
      "\u001b[32m03:29:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n",
      "\u001b[32m03:29:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 0: emitting 10000 indices\n",
      "First ids dataset samples: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "...Last ids: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "  1%|          | 1/157 [01:11<3:05:32, 71.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE #1 ===\n",
      "\u001b[92m<|im_start|>user\n",
      "What is the similarity between elements in the same vertical column of the periodic table?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\u001b[0m\u001b[93m<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Valence electrons are the electrons in the outermost shell of an atom, and they determine the chemical properties of the element. Elements in the same vertical column have the same number of valence electrons, which means they have similar chemical properties. For example, all the elements in Group 1 (the alkali metals) have one valence electron, and they are all highly reactive. Similarly, all the elements in Group 17 (the halogens) have seven valence electrons, and they are all highly reactive nonmetals.\n",
      "####\n",
      "Elements in the same vertical column, also known as groups or families, share the same number and arrangement of valence electrons.<|im_end|>\n",
      "\u001b[0m\n",
      "\n",
      "More training debug examples written to .log/dataloader_examples.html\n",
      "[Update step: 0]0/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 164\n",
      "[Update step: 0]1/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 768\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "[Update step: 0]2/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 792\n",
      "[Update step: 0]3/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 546\n",
      "[Update step: 0]4/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 197\n",
      "[Update step: 0]5/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 445\n",
      "[Update step: 0]6/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 141\n",
      "[Update step: 0]7/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 714\n",
      "[Update step: 0]8/63 - Total tokens seen: 0.00M, Non-padded tokens: 0.00M - Sequence length: 306\n",
      "[Update step: 0]9/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 970\n",
      "[Update step: 0]10/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 310\n",
      "[Update step: 0]11/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 444\n",
      "[Update step: 0]12/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 322\n",
      "[Update step: 0]13/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 204\n",
      "[Update step: 0]14/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 279\n",
      "[Update step: 0]15/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 489\n",
      "[Update step: 0]16/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 574\n",
      "[Update step: 0]17/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 405\n",
      "[Update step: 0]18/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 556\n",
      "[Update step: 0]19/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 456\n",
      "[Update step: 0]20/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 206\n",
      "[Update step: 0]21/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 385\n",
      "[Update step: 0]22/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 287\n",
      "[Update step: 0]23/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 490\n",
      "[Update step: 0]24/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 208\n",
      "[Update step: 0]25/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 392\n",
      "[Update step: 0]26/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 1358\n",
      "[Update step: 0]27/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 594\n",
      "[Update step: 0]28/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 413\n",
      "[Update step: 0]29/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 113\n",
      "[Update step: 0]30/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 263\n",
      "[Update step: 0]31/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 252\n",
      "[Update step: 0]32/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 430\n",
      "[Update step: 0]33/63 - Total tokens seen: 0.01M, Non-padded tokens: 0.01M - Sequence length: 333\n",
      "[Update step: 0]34/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 313\n",
      "[Update step: 0]35/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 359\n",
      "[Update step: 0]36/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 314\n",
      "[Update step: 0]37/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 278\n",
      "[Update step: 0]38/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 391\n",
      "[Update step: 0]39/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 327\n",
      "[Update step: 0]40/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 213\n",
      "[Update step: 0]41/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 167\n",
      "[Update step: 0]42/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 1260\n",
      "[Update step: 0]43/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 358\n",
      "[Update step: 0]44/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 678\n",
      "[Update step: 0]45/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 1542\n",
      "[Update step: 0]46/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 516\n",
      "[Update step: 0]47/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 800\n",
      "[Update step: 0]48/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 249\n",
      "[Update step: 0]49/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 386\n",
      "[Update step: 0]50/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 1062\n",
      "[Update step: 0]51/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 396\n",
      "[Update step: 0]52/63 - Total tokens seen: 0.02M, Non-padded tokens: 0.02M - Sequence length: 339\n",
      "[Update step: 0]53/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 396\n",
      "[Update step: 0]54/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 250\n",
      "[Update step: 0]55/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 829\n",
      "[Update step: 0]56/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 505\n",
      "[Update step: 0]57/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 445\n",
      "[Update step: 0]58/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 955\n",
      "[Update step: 0]59/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 795\n",
      "[Update step: 0]60/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 844\n",
      "[Update step: 0]61/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 470\n",
      "[Update step: 0]62/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 483\n",
      "[Update step: 0]63/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 1082\n",
      "{'loss': 1.1038000583648682, 'grad_norm': 0.33544641733169556, 'learning_rate': 0.0, 'epoch': 0.01}\n",
      "[Update step: 1]0/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 722\n",
      "[Update step: 1]1/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 457\n",
      "[Update step: 1]2/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 1533\n",
      "[Update step: 1]3/63 - Total tokens seen: 0.03M, Non-padded tokens: 0.03M - Sequence length: 247\n",
      "[Update step: 1]4/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 334\n",
      "[Update step: 1]5/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 244\n",
      "[Update step: 1]6/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 490\n",
      "[Update step: 1]7/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 730\n",
      "[Update step: 1]8/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 663"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/157 [02:13<2:49:43, 65.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Update step: 1]9/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 849\n",
      "[Update step: 1]10/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 438\n",
      "[Update step: 1]11/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 609\n",
      "[Update step: 1]12/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 216\n",
      "[Update step: 1]13/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 575\n",
      "[Update step: 1]14/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 489\n",
      "[Update step: 1]15/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 355\n",
      "[Update step: 1]16/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 709\n",
      "[Update step: 1]17/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 287\n",
      "[Update step: 1]18/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 447\n",
      "[Update step: 1]19/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 635\n",
      "[Update step: 1]20/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 144\n",
      "[Update step: 1]21/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 110\n",
      "[Update step: 1]22/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 450\n",
      "[Update step: 1]23/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 212\n",
      "[Update step: 1]24/63 - Total tokens seen: 0.04M, Non-padded tokens: 0.04M - Sequence length: 700\n",
      "[Update step: 1]25/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 667\n",
      "[Update step: 1]26/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 457\n",
      "[Update step: 1]27/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 297\n",
      "[Update step: 1]28/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 586\n",
      "[Update step: 1]29/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 1419\n",
      "[Update step: 1]30/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 1621\n",
      "[Update step: 1]31/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 1715\n",
      "[Update step: 1]32/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 487\n",
      "[Update step: 1]33/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 272\n",
      "[Update step: 1]34/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 514\n",
      "[Update step: 1]35/63 - Total tokens seen: 0.05M, Non-padded tokens: 0.05M - Sequence length: 998\n",
      "[Update step: 1]36/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 1663\n",
      "[Update step: 1]37/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 653\n",
      "[Update step: 1]38/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 245\n",
      "[Update step: 1]39/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 226\n",
      "[Update step: 1]40/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 420\n",
      "[Update step: 1]41/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 358\n",
      "[Update step: 1]42/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 648\n",
      "[Update step: 1]43/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 463\n",
      "[Update step: 1]44/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 236\n",
      "[Update step: 1]45/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 378\n",
      "[Update step: 1]46/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 404\n",
      "[Update step: 1]47/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 400\n",
      "[Update step: 1]48/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 577\n",
      "[Update step: 1]49/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 1390\n",
      "[Update step: 1]50/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 189\n",
      "[Update step: 1]51/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 1097\n",
      "[Update step: 1]52/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 811\n",
      "[Update step: 1]53/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 157\n",
      "[Update step: 1]54/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 262\n",
      "[Update step: 1]55/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 275\n",
      "[Update step: 1]56/63 - Total tokens seen: 0.06M, Non-padded tokens: 0.06M - Sequence length: 527\n",
      "[Update step: 1]57/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 439\n",
      "[Update step: 1]58/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 1139\n",
      "[Update step: 1]59/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 287\n",
      "[Update step: 1]60/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 559\n",
      "[Update step: 1]61/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 549\n",
      "[Update step: 1]62/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 423\n",
      "[Update step: 1]63/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 423\n",
      "{'loss': 0.973800003528595, 'grad_norm': 0.32004886865615845, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "[Update step: 2]0/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 213\n",
      "[Update step: 2]1/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 516\n",
      "[Update step: 2]2/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 4096\n",
      "[Update step: 2]3/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 203\n",
      "[Update step: 2]4/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 236\n",
      "[Update step: 2]5/63 - Total tokens seen: 0.07M, Non-padded tokens: 0.07M - Sequence length: 349\n",
      "[Update step: 2]6/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 1224\n",
      "[Update step: 2]7/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 448\n",
      "[Update step: 2]8/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 674\n",
      "[Update step: 2]9/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 501\n",
      "[Update step: 2]10/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 855\n",
      "[Update step: 2]11/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 424\n",
      "[Update step: 2]12/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 475\n",
      "[Update step: 2]13/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 230\n",
      "[Update step: 2]14/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 657\n",
      "[Update step: 2]15/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 1423\n",
      "[Update step: 2]16/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 140\n",
      "[Update step: 2]17/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 351\n",
      "[Update step: 2]18/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 325\n",
      "[Update step: 2]19/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 311\n",
      "[Update step: 2]20/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 211\n",
      "[Update step: 2]21/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 573\n",
      "[Update step: 2]22/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 423\n",
      "[Update step: 2]23/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 331\n",
      "[Update step: 2]24/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 535\n",
      "[Update step: 2]25/63 - Total tokens seen: 0.08M, Non-padded tokens: 0.08M - Sequence length: 395\n",
      "[Update step: 2]26/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 396\n",
      "[Update step: 2]27/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 465"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/157 [03:06<2:34:17, 60.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Update step: 2]28/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 350\n",
      "[Update step: 2]29/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 693\n",
      "[Update step: 2]30/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 420\n",
      "[Update step: 2]31/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 642\n",
      "[Update step: 2]32/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 190\n",
      "[Update step: 2]33/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 136\n",
      "[Update step: 2]34/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 277\n",
      "[Update step: 2]35/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 914\n",
      "[Update step: 2]36/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 826\n",
      "[Update step: 2]37/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 1307\n",
      "[Update step: 2]38/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 447\n",
      "[Update step: 2]39/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 171\n",
      "[Update step: 2]40/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 700\n",
      "[Update step: 2]41/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 722\n",
      "[Update step: 2]42/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 615\n",
      "[Update step: 2]43/63 - Total tokens seen: 0.09M, Non-padded tokens: 0.09M - Sequence length: 558\n",
      "[Update step: 2]44/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 1330\n",
      "[Update step: 2]45/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 1036\n",
      "[Update step: 2]46/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 870\n",
      "[Update step: 2]47/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 681\n",
      "[Update step: 2]48/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 585\n",
      "[Update step: 2]49/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 1177\n",
      "[Update step: 2]50/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 503\n",
      "[Update step: 2]51/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 1137\n",
      "[Update step: 2]52/63 - Total tokens seen: 0.10M, Non-padded tokens: 0.10M - Sequence length: 985\n",
      "[Update step: 2]53/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 2081\n",
      "[Update step: 2]54/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 858\n",
      "[Update step: 2]55/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 254\n",
      "[Update step: 2]56/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 410\n",
      "[Update step: 2]57/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 121\n",
      "[Update step: 2]58/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 556\n",
      "[Update step: 2]59/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 755\n",
      "[Update step: 2]60/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 339\n",
      "[Update step: 2]61/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 515\n",
      "[Update step: 2]62/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 977\n",
      "[Update step: 2]63/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 378\n",
      "{'loss': 0.9904999732971191, 'grad_norm': 0.29692158102989197, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "[Update step: 3]0/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 596\n",
      "[Update step: 3]1/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 495\n",
      "[Update step: 3]2/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 202\n",
      "[Update step: 3]3/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 233\n",
      "[Update step: 3]4/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 980\n",
      "[Update step: 3]5/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 963\n",
      "[Update step: 3]6/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 394\n",
      "[Update step: 3]7/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 371\n",
      "[Update step: 3]8/63 - Total tokens seen: 0.11M, Non-padded tokens: 0.11M - Sequence length: 384\n",
      "[Update step: 3]9/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 284\n",
      "[Update step: 3]10/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 865\n",
      "[Update step: 3]11/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 178\n",
      "[Update step: 3]12/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 141\n",
      "[Update step: 3]13/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 143\n",
      "[Update step: 3]14/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 318\n",
      "[Update step: 3]15/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 631\n",
      "[Update step: 3]16/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 568\n",
      "[Update step: 3]17/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 306\n",
      "[Update step: 3]18/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 1169\n",
      "[Update step: 3]19/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 100\n",
      "[Update step: 3]20/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 512\n",
      "[Update step: 3]21/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 106\n",
      "[Update step: 3]22/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 1271\n",
      "[Update step: 3]23/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 1173\n",
      "[Update step: 3]24/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 1422\n",
      "[Update step: 3]25/63 - Total tokens seen: 0.12M, Non-padded tokens: 0.12M - Sequence length: 995\n",
      "[Update step: 3]26/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 310\n",
      "[Update step: 3]27/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 515\n",
      "[Update step: 3]28/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 273\n",
      "[Update step: 3]29/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 1687\n",
      "[Update step: 3]30/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 983\n",
      "[Update step: 3]31/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 491\n",
      "[Update step: 3]32/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 356\n",
      "[Update step: 3]33/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 184\n",
      "[Update step: 3]34/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 240\n",
      "[Update step: 3]35/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 496\n",
      "[Update step: 3]36/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 994\n",
      "[Update step: 3]37/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 469\n",
      "[Update step: 3]38/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 327\n",
      "[Update step: 3]39/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 778\n",
      "[Update step: 3]40/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 493\n",
      "[Update step: 3]41/63 - Total tokens seen: 0.13M, Non-padded tokens: 0.13M - Sequence length: 1295\n",
      "[Update step: 3]42/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 418\n",
      "[Update step: 3]43/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 186\n",
      "[Update step: 3]44/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 327\n",
      "[Update step: 3]45/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 1194\n",
      "[Update step: 3]46/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 270"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/157 [05:01<2:29:18, 58.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Update step: 3]47/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 277\n",
      "[Update step: 3]48/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 469\n",
      "[Update step: 3]49/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 808\n",
      "[Update step: 3]50/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 207\n",
      "[Update step: 3]51/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 659\n",
      "[Update step: 3]52/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 629\n",
      "[Update step: 3]53/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 592\n",
      "[Update step: 3]54/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 1048\n",
      "[Update step: 3]55/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 449\n",
      "[Update step: 3]56/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 815\n",
      "[Update step: 3]57/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 487\n",
      "[Update step: 3]58/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 274\n",
      "[Update step: 3]59/63 - Total tokens seen: 0.14M, Non-padded tokens: 0.14M - Sequence length: 366\n",
      "[Update step: 3]60/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 1037\n",
      "[Update step: 3]61/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 475\n",
      "[Update step: 3]62/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 1081\n",
      "[Update step: 3]63/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 818\n",
      "{'loss': 1.0721999406814575, 'grad_norm': 0.30807197093963623, 'learning_rate': 6e-06, 'epoch': 0.03}\n",
      "[Update step: 4]0/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 503\n",
      "[Update step: 4]1/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 381\n",
      "[Update step: 4]2/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 347\n",
      "[Update step: 4]3/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 310\n",
      "[Update step: 4]4/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 183\n",
      "[Update step: 4]5/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 298\n",
      "[Update step: 4]6/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 1013\n",
      "[Update step: 4]7/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 254\n",
      "[Update step: 4]8/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 706\n",
      "[Update step: 4]9/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 333\n",
      "[Update step: 4]10/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 747\n",
      "[Update step: 4]11/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 375\n",
      "[Update step: 4]12/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 712\n",
      "[Update step: 4]13/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 251\n",
      "[Update step: 4]14/63 - Total tokens seen: 0.15M, Non-padded tokens: 0.15M - Sequence length: 525\n",
      "[Update step: 4]15/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 353\n",
      "[Update step: 4]16/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 1214\n",
      "[Update step: 4]17/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 510\n",
      "[Update step: 4]18/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 570\n",
      "[Update step: 4]19/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 326\n",
      "[Update step: 4]20/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 423\n",
      "[Update step: 4]21/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 211\n",
      "[Update step: 4]22/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 491\n",
      "[Update step: 4]23/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 380\n",
      "[Update step: 4]24/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 791\n",
      "[Update step: 4]25/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 547\n",
      "[Update step: 4]26/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 541\n",
      "[Update step: 4]27/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 524\n",
      "[Update step: 4]28/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 513\n",
      "[Update step: 4]29/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 261\n",
      "[Update step: 4]30/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 348\n",
      "[Update step: 4]31/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 811\n",
      "[Update step: 4]32/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 227\n",
      "[Update step: 4]33/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 274\n",
      "[Update step: 4]34/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 526\n",
      "[Update step: 4]35/63 - Total tokens seen: 0.16M, Non-padded tokens: 0.16M - Sequence length: 170\n",
      "[Update step: 4]36/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 727\n",
      "[Update step: 4]37/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 449\n",
      "[Update step: 4]38/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 323\n",
      "[Update step: 4]39/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 120\n",
      "[Update step: 4]40/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 572\n",
      "[Update step: 4]41/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 225\n",
      "[Update step: 4]42/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 313\n",
      "[Update step: 4]43/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 1179\n",
      "[Update step: 4]44/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 1091\n",
      "[Update step: 4]45/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 264\n",
      "[Update step: 4]46/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 2113\n",
      "[Update step: 4]47/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 113\n",
      "[Update step: 4]48/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 935\n",
      "[Update step: 4]49/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 523\n",
      "[Update step: 4]50/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 341\n",
      "[Update step: 4]51/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 575\n",
      "[Update step: 4]52/63 - Total tokens seen: 0.17M, Non-padded tokens: 0.17M - Sequence length: 313\n",
      "[Update step: 4]53/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 880\n",
      "[Update step: 4]54/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 269\n",
      "[Update step: 4]55/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 304\n",
      "[Update step: 4]56/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 487\n",
      "[Update step: 4]57/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 326\n",
      "[Update step: 4]58/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 1082\n",
      "[Update step: 4]59/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 218\n",
      "[Update step: 4]60/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 867\n",
      "[Update step: 4]61/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 454\n",
      "[Update step: 4]62/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 907\n",
      "[Update step: 4]63/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 1485\n",
      "{'loss': 1.0853999853134155, 'grad_norm': 0.32627415657043457, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}\n",
      "[Update step: 5]0/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 1099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/157 [06:05<2:32:42, 60.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Update step: 5]1/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 419\n",
      "[Update step: 5]2/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 645\n",
      "[Update step: 5]3/63 - Total tokens seen: 0.18M, Non-padded tokens: 0.18M - Sequence length: 107\n",
      "[Update step: 5]4/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 808\n",
      "[Update step: 5]5/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 563\n",
      "[Update step: 5]6/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 609\n",
      "[Update step: 5]7/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 971\n",
      "[Update step: 5]8/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 389\n",
      "[Update step: 5]9/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 1025\n",
      "[Update step: 5]10/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 206\n",
      "[Update step: 5]11/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 91\n",
      "[Update step: 5]12/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 352\n",
      "[Update step: 5]13/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 654\n",
      "[Update step: 5]14/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 296\n",
      "[Update step: 5]15/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 224\n",
      "[Update step: 5]16/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 1168\n",
      "[Update step: 5]17/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 1166\n",
      "[Update step: 5]18/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 378\n",
      "[Update step: 5]19/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 294\n",
      "[Update step: 5]20/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 405\n",
      "[Update step: 5]21/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 107\n",
      "[Update step: 5]22/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 248\n",
      "[Update step: 5]23/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 274\n",
      "[Update step: 5]24/63 - Total tokens seen: 0.19M, Non-padded tokens: 0.19M - Sequence length: 235\n",
      "[Update step: 5]25/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 834\n",
      "[Update step: 5]26/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 595\n",
      "[Update step: 5]27/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 500\n",
      "[Update step: 5]28/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 1164\n",
      "[Update step: 5]29/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 179\n",
      "[Update step: 5]30/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 564\n",
      "[Update step: 5]31/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 142\n",
      "[Update step: 5]32/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 892\n",
      "[Update step: 5]33/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 994\n",
      "[Update step: 5]34/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 302\n",
      "[Update step: 5]35/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 400\n",
      "[Update step: 5]36/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 960\n",
      "[Update step: 5]37/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 306\n",
      "[Update step: 5]38/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 1065\n",
      "[Update step: 5]39/63 - Total tokens seen: 0.20M, Non-padded tokens: 0.20M - Sequence length: 243\n",
      "[Update step: 5]40/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 1068\n",
      "[Update step: 5]41/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 103\n",
      "[Update step: 5]42/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 272\n",
      "[Update step: 5]43/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 334\n",
      "[Update step: 5]44/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 328\n",
      "[Update step: 5]45/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 597\n",
      "[Update step: 5]46/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 1029\n",
      "[Update step: 5]47/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 4096\n",
      "[Update step: 5]48/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 581\n",
      "[Update step: 5]49/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 277\n",
      "[Update step: 5]50/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 266\n",
      "[Update step: 5]51/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 695\n",
      "[Update step: 5]52/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 285\n",
      "[Update step: 5]53/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 442\n",
      "[Update step: 5]54/63 - Total tokens seen: 0.21M, Non-padded tokens: 0.21M - Sequence length: 323\n",
      "[Update step: 5]55/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 308\n",
      "[Update step: 5]56/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 636\n",
      "[Update step: 5]57/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 1041\n",
      "[Update step: 5]58/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 4096\n",
      "[Update step: 5]59/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 592\n",
      "[Update step: 5]60/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 475\n",
      "[Update step: 5]61/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 336\n",
      "[Update step: 5]62/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 194\n",
      "[Update step: 5]63/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 648\n",
      "{'loss': 1.0684000253677368, 'grad_norm': 0.27928271889686584, 'learning_rate': 1e-05, 'epoch': 0.04}\n",
      "[Update step: 6]0/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 353\n",
      "[Update step: 6]1/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 464\n",
      "[Update step: 6]2/63 - Total tokens seen: 0.22M, Non-padded tokens: 0.22M - Sequence length: 872\n",
      "[Update step: 6]3/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 445\n",
      "[Update step: 6]4/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 1623\n",
      "[Update step: 6]5/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 319\n",
      "[Update step: 6]6/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 916\n",
      "[Update step: 6]7/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 310\n",
      "[Update step: 6]8/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 1288\n",
      "[Update step: 6]9/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 301\n",
      "[Update step: 6]10/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 287\n",
      "[Update step: 6]11/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 316\n",
      "[Update step: 6]12/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 188\n",
      "[Update step: 6]13/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 361\n",
      "[Update step: 6]14/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 471\n",
      "[Update step: 6]15/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 189\n",
      "[Update step: 6]16/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 558\n",
      "[Update step: 6]17/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 308\n",
      "[Update step: 6]18/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 474\n",
      "[Update step: 6]19/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7/157 [07:07<2:33:13, 61.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Update step: 6]20/63 - Total tokens seen: 0.23M, Non-padded tokens: 0.23M - Sequence length: 402\n",
      "[Update step: 6]21/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 1205\n",
      "[Update step: 6]22/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 269\n",
      "[Update step: 6]23/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 1053\n",
      "[Update step: 6]24/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 244\n",
      "[Update step: 6]25/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 326\n",
      "[Update step: 6]26/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 807\n",
      "[Update step: 6]27/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 362\n",
      "[Update step: 6]28/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 347\n",
      "[Update step: 6]29/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 660\n",
      "[Update step: 6]30/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 998\n",
      "[Update step: 6]31/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 124\n",
      "[Update step: 6]32/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 190\n",
      "[Update step: 6]33/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 200\n",
      "[Update step: 6]34/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 390\n",
      "[Update step: 6]35/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 121\n",
      "[Update step: 6]36/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 200\n",
      "[Update step: 6]37/63 - Total tokens seen: 0.24M, Non-padded tokens: 0.24M - Sequence length: 405\n",
      "[Update step: 6]38/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 4096\n",
      "[Update step: 6]39/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 437\n",
      "[Update step: 6]40/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 350\n",
      "[Update step: 6]41/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 194\n",
      "[Update step: 6]42/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 576\n",
      "[Update step: 6]43/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 556\n",
      "[Update step: 6]44/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 237\n",
      "[Update step: 6]45/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 265\n",
      "[Update step: 6]46/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 2068\n",
      "[Update step: 6]47/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 102\n",
      "[Update step: 6]48/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 95\n",
      "[Update step: 6]49/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 869\n",
      "[Update step: 6]50/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 525\n",
      "[Update step: 6]51/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 548\n",
      "[Update step: 6]52/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 273\n",
      "[Update step: 6]53/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 367\n",
      "[Update step: 6]54/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 1157\n",
      "[Update step: 6]55/63 - Total tokens seen: 0.25M, Non-padded tokens: 0.25M - Sequence length: 307\n",
      "[Update step: 6]56/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 1725\n",
      "[Update step: 6]57/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 610\n",
      "[Update step: 6]58/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 404\n",
      "[Update step: 6]59/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 448\n",
      "[Update step: 6]60/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 348\n",
      "[Update step: 6]61/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 184\n",
      "[Update step: 6]62/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 502\n",
      "[Update step: 6]63/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 328\n",
      "{'loss': 1.0979000329971313, 'grad_norm': 0.3283103108406067, 'learning_rate': 9.93421052631579e-06, 'epoch': 0.04}\n",
      "[Update step: 7]0/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 427\n",
      "[Update step: 7]1/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 2009\n",
      "[Update step: 7]2/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 414\n",
      "[Update step: 7]3/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 548\n",
      "[Update step: 7]4/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 963\n",
      "[Update step: 7]5/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 483\n",
      "[Update step: 7]6/63 - Total tokens seen: 0.26M, Non-padded tokens: 0.26M - Sequence length: 413\n",
      "[Update step: 7]7/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 1102\n",
      "[Update step: 7]8/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 346\n",
      "[Update step: 7]9/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 624\n",
      "[Update step: 7]10/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 890\n",
      "[Update step: 7]11/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 1940\n",
      "[Update step: 7]12/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 826\n",
      "[Update step: 7]13/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 245\n",
      "[Update step: 7]14/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 415\n",
      "[Update step: 7]15/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 920\n",
      "[Update step: 7]16/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 156\n",
      "[Update step: 7]17/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 675\n",
      "[Update step: 7]18/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 150\n",
      "[Update step: 7]19/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 378\n",
      "[Update step: 7]20/63 - Total tokens seen: 0.27M, Non-padded tokens: 0.27M - Sequence length: 948\n",
      "[Update step: 7]21/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 842\n",
      "[Update step: 7]22/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 828\n",
      "[Update step: 7]23/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 373\n",
      "[Update step: 7]24/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 412\n",
      "[Update step: 7]25/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 468\n",
      "[Update step: 7]26/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 677\n",
      "[Update step: 7]27/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 518\n",
      "[Update step: 7]28/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 1436\n",
      "[Update step: 7]29/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 888\n",
      "[Update step: 7]30/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 434\n",
      "[Update step: 7]31/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 373\n",
      "[Update step: 7]32/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 662\n",
      "[Update step: 7]33/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 550\n",
      "[Update step: 7]34/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 252\n",
      "[Update step: 7]35/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 1233\n",
      "[Update step: 7]36/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 386\n",
      "[Update step: 7]37/63 - Total tokens seen: 0.28M, Non-padded tokens: 0.28M - Sequence length: 376\n",
      "[Update step: 7]38/63 - Total tokens seen: 0.29M, Non-padded tokens: 0.29M - Sequence length: 436"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 8/157 [08:12<2:34:44, 62.31s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['WANDB_PROJECT'] = 'compare-unsloth'\n",
    "from opensloth.scripts.opensloth_sft_trainer import run_mp_training, setup_envs\n",
    "from opensloth.opensloth_config import (\n",
    "    OpenSlothConfig,\n",
    "    HFDatasetConfig,\n",
    "    FastModelArgs,\n",
    "    LoraArgs,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from loguru import logger\n",
    "\n",
    "# from transformers.training_args import TrainingArguments\n",
    "\n",
    "\n",
    "# # Main configuration using Pydantic models\n",
    "def get_configs(devices) -> tuple[OpenSlothConfig, TrainingArguments]:\n",
    "    num_gpu = len(devices)\n",
    "    opensloth_config = OpenSlothConfig(\n",
    "        data=HFDatasetConfig(\n",
    "            tokenizer_name=\"Qwen/Qwen3-8B\",\n",
    "            chat_template=\"qwen3\",\n",
    "            instruction_part=\"<|im_start|>user\\n\",\n",
    "            response_part=\"<|im_start|>assistant\\n\",\n",
    "            num_samples=10000,\n",
    "            nproc=52,\n",
    "            max_seq_length=4096,\n",
    "            source_type=\"hf\",\n",
    "            dataset_name=\"mlabonne/FineTome-100k\",\n",
    "            split=\"train\",\n",
    "        ),\n",
    "        devices=devices,  # list of int representing GPU ids\n",
    "        fast_model_args=FastModelArgs(\n",
    "            model_name=\"model_store/unsloth/Qwen3-14B-bnb-4bit\",\n",
    "            max_seq_length=4096,\n",
    "            load_in_4bit=True,\n",
    "        ),\n",
    "        lora_args=LoraArgs(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_rslora=False,\n",
    "        ),\n",
    "        sequence_packing=True,\n",
    "    )\n",
    "\n",
    "    # # Training arguments using Pydantic model\n",
    "    training_config = TrainingArguments(\n",
    "        output_dir=f\"outputs/exps/qwen3-14b-FineTome-{num_gpu}gpus-seql-packing\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=64,  # Adjust based on n_gpu\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=5,\n",
    "        save_total_limit=1,\n",
    "        weight_decay=0.01,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",  # tensorboard or wawndb\n",
    "    )\n",
    "    setup_envs(opensloth_config, training_config)\n",
    "    return opensloth_config, training_config\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opensloth_config, training_config = get_configs(devices=[0])\n",
    "    # opensloth_config, training_config = get_configs(devices=[0,1,2,3])\n",
    "    run_mp_training(opensloth_config.devices, opensloth_config, training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1985db",
   "metadata": {},
   "source": [
    "## Unsloth default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f33c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:47:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mdataset_utils.py:222\u001b[0m | \u001b[1mPreparing dataset 7fe3c373565b53a9...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f822a5522c0a47368144ef2d4219a40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97a0553988e4151b4ebf36bafd4250b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=104):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5329e0e07f440118c36d6ca2f044927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=104):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H100 80GB HBM3. Max memory = 79.189 GB.\n",
      "0.812 GB of memory reserved.\n",
      "🔧 Patching Trainer to use RandomSamplerSeededByEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,000 | Num Epochs = 1 | Total steps = 157\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 16 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 5,046,272/6,000,000,000 (0.08% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manhvth\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anhvth5/projects/opensloth/wandb/run-20250609_164901-xyyhfuiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anhvth/huggingface/runs/xyyhfuiq' target=\"_blank\">outputs/exps/qwen3-14b-FineTome-unsloth</a></strong> to <a href='https://wandb.ai/anhvth/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anhvth/huggingface' target=\"_blank\">https://wandb.ai/anhvth/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anhvth/huggingface/runs/xyyhfuiq' target=\"_blank\">https://wandb.ai/anhvth/huggingface/runs/xyyhfuiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:49:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 0: emitting 10000 indices\n",
      "First ids dataset samples: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "...Last ids: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 23:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.194500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.176800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.994600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.082000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:13:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 0: dataset_size=10000\n",
      "   📋 First 10 indices: [3771, 6672, 7261, 760, 3779, 1772, 7509, 2679, 2305, 9215]\n",
      "   📋 Last 10 indices: [9674, 1424, 8935, 1679, 2286, 3657, 4012, 4506, 409, 1824]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444.214 seconds used for training.\n",
      "24.07 minutes used for training.\n",
      "Peak reserved memory = 1.725 GB.\n",
      "Peak reserved memory for training = 0.913 GB.\n",
      "Peak reserved memory % of max memory = 2.178 %.\n",
      "Peak reserved memory for training % of max memory = 1.153 %.\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from opensloth.patching.patch_sampler import patch_sampler\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"OPENSLOTH_LOCAL_RANK\"] = \"0\"\n",
    "\n",
    "\n",
    "def train_qwen3_model():\n",
    "    \"\"\"Train Qwen3 model with minimal setup.\"\"\"\n",
    "    from opensloth.dataset_utils import get_tokenized_dataset, HFDatasetConfig\n",
    "\n",
    "    text_dataset = get_tokenized_dataset(\n",
    "        HFDatasetConfig(\n",
    "            tokenizer_name=\"Qwen/Qwen3-8B\",\n",
    "            chat_template=\"qwen3\",\n",
    "            instruction_part=\"<|im_start|>user\\n\",\n",
    "            response_part=\"<|im_start|>assistant\\n\",\n",
    "            num_samples=10000,\n",
    "            nproc=52,\n",
    "            max_seq_length=4096,\n",
    "            source_type=\"hf\",\n",
    "            dataset_name=\"mlabonne/FineTome-100k\",\n",
    "            split=\"train\",\n",
    "        ),\n",
    "        do_tokenize=False,\n",
    "    )\n",
    "    from unsloth import FastLanguageModel\n",
    "    import torch\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Qwen3-0.6B-bnb-4bit\",\n",
    "        max_seq_length=4096,\n",
    "        load_in_4bit=True,\n",
    "        load_in_8bit=False,\n",
    "        full_finetuning=False,\n",
    "    )\n",
    "\n",
    "    # Add LoRA adapters\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=True,\n",
    "        random_state=3407,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    args = SFTConfig(\n",
    "        output_dir=\"outputs/exps/qwen3-14b-FineTome-unsloth\",\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8*2, # Adjust based on n_gpu\n",
    "        warmup_steps=5,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",  \n",
    "    )\n",
    "\n",
    "    # args.skip_prepare_dataset = True\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=text_dataset,\n",
    "        eval_dataset=None,\n",
    "        args=args,\n",
    "    )\n",
    "    from unsloth_zoo.dataset_utils import train_on_responses_only\n",
    "\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_part=\"<|im_start|>user\\n\",\n",
    "        response_part=\"<|im_start|>assistant\\n\",\n",
    "    )\n",
    "\n",
    "    # Show memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "    # from ._patch_sampler import patch_sampler\n",
    "\n",
    "    trainer = patch_sampler(trainer)\n",
    "    trainer_stats = trainer.train()\n",
    "\n",
    "    # Show final memory and time stats\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    print(\n",
    "        f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    "    )\n",
    "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = train_qwen3_model()\n",
    "    print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626094db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
