#!/usr/bin/env python3
"""
Ansible Argument Specs Generator

A Python script to generate argument_specs.yml files for Ansible roles.
Supports multiple input methods and validates the generated specs.

Generated by: claude-4-sonnet
Version: 1.0.1
"""

import argparse
import json
import os
import re
import sys
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Set
from dataclasses import dataclass, asdict
from enum import Enum


class ArgumentType(Enum):
    """Supported argument types in Ansible"""

    STR = "str"
    INT = "int"
    FLOAT = "float"
    BOOL = "bool"
    LIST = "list"
    DICT = "dict"
    PATH = "path"
    RAW = "raw"


@dataclass
class ArgumentSpec:
    """Represents a single argument specification"""

    name: str
    type: str = "str"
    required: bool = False
    default: Optional[Any] = None
    choices: Optional[List[str]] = None
    description: Optional[str] = None
    elements: Optional[str] = None  # For list/dict types
    options: Optional[Dict[str, Any]] = None  # For nested dict types
    version_added: Optional[str] = None  # Version when this argument was added

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary format for YAML output"""
        result = {}

        if self.description:
            result["description"] = self.description

        result["type"] = self.type

        if self.required:
            result["required"] = self.required

        if self.default is not None:
            result["default"] = self.default

        if self.choices:
            result["choices"] = self.choices

        if self.elements:
            result["elements"] = self.elements

        if self.options:
            result["options"] = self.options

        if self.version_added:
            result["version_added"] = self.version_added

        return result


@dataclass
class EntryPointSpec:
    """Represents an entry point specification"""

    name: str = "main"
    short_description: str = ""
    description: List[str] = None
    author: List[str] = None
    options: Dict[str, ArgumentSpec] = None
    required_if: List[List[str]] = None
    required_one_of: List[List[str]] = None
    mutually_exclusive: List[List[str]] = None
    required_together: List[List[str]] = None

    def __post_init__(self):
        if self.description is None:
            self.description = []
        if self.author is None:
            self.author = []
        if self.options is None:
            self.options = {}

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary format for YAML output"""
        result = {}

        if self.short_description:
            result["short_description"] = self.short_description

        if self.description:
            # Keep description as a list for proper YAML formatting
            # Create a copy to avoid YAML reference issues
            if isinstance(self.description, list):
                result["description"] = list(self.description)
            else:
                # Convert single string to list for consistency
                result["description"] = [self.description]

        if self.author:
            # Keep author as a list for consistency with Ansible standards
            # Create a copy to avoid YAML reference issues
            result["author"] = list(self.author)

        if self.options:
            # Sort options alphabetically for consistent output
            result["options"] = {
                name: spec.to_dict() for name, spec in sorted(self.options.items())
            }

        # Add conditional requirements
        if self.required_if:
            result["required_if"] = self.required_if

        if self.required_one_of:
            result["required_one_of"] = self.required_one_of

        if self.mutually_exclusive:
            result["mutually_exclusive"] = self.mutually_exclusive

        if self.required_together:
            result["required_together"] = self.required_together

        return result


class ArgumentSpecsGenerator:
    """Main class for generating argument specs"""

    def __init__(self, collection_mode: bool = True, verbosity: int = 0):
        self.entry_points: Dict[str, EntryPointSpec] = {}
        self.collection_mode = collection_mode
        self.processed_roles: List[str] = []
        self.variable_context: Dict[
            str, Dict[str, Any]
        ] = {}  # Store variable usage context
        self.verbosity = verbosity
        self.current_role = ""  # Track current role being processed
        self.stats = {
            "roles_processed": 0,
            "roles_failed": 0,
            "total_variables": 0,
            "new_variables": 0,
            "existing_variables": 0,
            "entry_points_created": 0,
        }

    def add_entry_point(self, entry_point: EntryPointSpec):
        """Add an entry point to the specification"""
        self.entry_points[entry_point.name] = entry_point

    def log(self, level: int, message: str, role_prefix: bool = True):
        """Log a message if verbosity level is sufficient"""
        if self.verbosity >= level:
            prefix = (
                f"[{self.current_role}] " if role_prefix and self.current_role else ""
            )
            print(f"{prefix}{message}")

    def log_info(self, message: str, role_prefix: bool = True):
        """Log info level message (verbosity >= 1)"""
        self.log(1, message, role_prefix)

    def log_verbose(self, message: str, role_prefix: bool = True):
        """Log verbose message (verbosity >= 2)"""
        self.log(2, f"  {message}", role_prefix)

    def log_debug(self, message: str, role_prefix: bool = True):
        """Log debug message (verbosity >= 3)"""
        self.log(3, f"    {message}", role_prefix)

    def log_trace(self, message: str, role_prefix: bool = True):
        """Log trace message (verbosity >= 3)"""
        self.log(3, f"      {message}", role_prefix)

    def log_error(self, message: str, role_prefix: bool = True):
        """Log error message (always shown regardless of verbosity)"""
        prefix = f"[{self.current_role}] " if role_prefix and self.current_role else ""
        print(f"{prefix}{message}")

    def _safe_load_yaml_file(
        self, file_path: Path, description: str = ""
    ) -> Optional[Dict[str, Any]]:
        """Safely load a YAML file with proper error handling"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                if not content.strip():
                    return {}
                return yaml.safe_load(content)
        except yaml.YAMLError as e:
            self.log_error(f"Invalid YAML in {file_path}: {e}")
            return None
        except (OSError, IOError) as e:
            self.log_error(f"Could not read {file_path}: {e}")
            return None
        except Exception as e:
            self.log_verbose(f"Could not parse {file_path}: {e}")
            return None

    def log_section(self, title: str):
        """Log a section header"""
        if self.verbosity >= 1:
            print(f"\n{'='*60}")
            print(f"  {title}")
            print("=" * 60)

    def log_summary(self):
        """Log final summary"""
        print(f"\n{'='*60}")
        print("  ARGUMENT SPECS GENERATION SUMMARY")
        print("=" * 60)
        print(f"Roles processed: {self.stats['roles_processed']}")
        if self.stats["roles_failed"] > 0:
            print(f"Roles failed: {self.stats['roles_failed']}")
        print(f"Entry points created: {self.stats['entry_points_created']}")
        print(f"Total variables: {self.stats['total_variables']}")
        print(f"  - New variables: {self.stats['new_variables']}")
        print(f"  - Existing variables: {self.stats['existing_variables']}")

        if self.processed_roles:
            print(f"\nProcessed roles: {', '.join(self.processed_roles)}")

        print("=" * 60)

    def is_collection_root(self, path: str = ".") -> bool:
        """Check if the current directory is a collection root"""
        collection_indicators = ["galaxy.yml", "roles/", "plugins/", "meta/runtime.yml"]

        path_obj = Path(path)
        has_roles = (path_obj / "roles").is_dir()
        has_galaxy = (path_obj / "galaxy.yml").is_file()

        # Must have roles directory, and preferably galaxy.yml
        return has_roles and (
            has_galaxy or len(list((path_obj / "roles").iterdir())) > 0
        )

    def find_roles(self, collection_path: str = ".") -> List[str]:
        """Find all roles in the collection"""
        roles_dir = Path(collection_path) / "roles"
        if not roles_dir.exists():
            return []

        roles = []
        for item in roles_dir.iterdir():
            if item.is_dir() and not item.name.startswith("."):
                # Check if it looks like a role (has tasks, defaults, meta, etc.)
                role_indicators = [
                    "tasks/",
                    "defaults/",
                    "meta/",
                    "handlers/",
                    "templates/",
                    "files/",
                ]
                if any((item / indicator).exists() for indicator in role_indicators):
                    roles.append(item.name)

        return sorted(roles)

    def extract_variables_from_task_file(self, task_file_path: Path) -> Set[str]:
        """Extract variables used in a task file"""
        variables = set()

        try:
            with open(task_file_path, "r", encoding="utf-8") as f:
                content = f.read()

            if not content.strip():
                return variables  # Return empty set for empty files

            # Store variable usage context for better descriptions
            self._analyze_variable_usage_context(content, task_file_path)

            # First, find all registered variables to exclude them
            registered_vars = set()
            register_patterns = [
                r"register:\s*([a-zA-Z_][a-zA-Z0-9_]*)",
                r"set_fact:\s*\n\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:",
            ]

            for pattern in register_patterns:
                matches = re.findall(pattern, content, re.MULTILINE)
                for match in matches:
                    var_name = match.strip()
                    if var_name:
                        registered_vars.add(var_name)

            # Also find properties/attributes of registered variables to exclude them
            registered_properties = set()
            if registered_vars:
                # Look for patterns like registered_var.property in conditionals
                property_patterns = [
                    r"when:\s*([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)",
                    r"changed_when:\s*([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)",
                    r"failed_when:\s*([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)",
                    r"until:\s*([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)",
                    r"\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)",
                ]

                for pattern in property_patterns:
                    matches = re.findall(pattern, content, re.MULTILINE)
                    for var_name, property_name in matches:
                        if var_name in registered_vars:
                            registered_properties.add(property_name)

            excluded_vars = registered_vars | registered_properties
            if excluded_vars:
                self.log_trace(
                    f"Excluding registered variables and their properties in {task_file_path.name}: {', '.join(sorted(excluded_vars))}"
                )

            # Multiple patterns for different variable formats (with unicode support)
            patterns = [
                # Standard Jinja2 variables: {{ variable }}
                r"\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}",
                # Variables in when conditions: when: variable
                r"when:\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|is|not)",
                # Variables in conditionals: variable is defined
                r"([a-zA-Z_][a-zA-Z0-9_]*)\s+is\s+(?:defined|not defined)",
                # Variables in assert statements: assert that: variable == value
                r"assert:\s*\n\s*that:\s*\n(?:\s*-\s*)+([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|is|not|in)",
                # Variables in assert conditions: - variable is defined
                r"-\s*([a-zA-Z_][a-zA-Z0-9_]*)\s+is\s+(?:defined|not defined)",
                # Variables in assert comparisons: - variable == "value"
                r"-\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|>|<|>=|<=)",
                # Variables in assert membership: - variable in list
                r"-\s*([a-zA-Z_][a-zA-Z0-9_]*)\s+(?:in|not in)",
                # Variables in assert that clauses with quotes: - "variable == 'value'"
                r'-\s*["\']([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|is|not|in|>|<|>=|<=)',
                # Variables in failed_when conditions: failed_when: variable
                r"failed_when:\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|is|not)",
                # Variables in changed_when conditions: changed_when: variable
                r"changed_when:\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|is|not)",
                # Variables in that: clauses (general): that: variable
                r"that:\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|is|not|in)",
                # Variables in multiline that clauses: that: | or >
                r"that:\s*[|>]\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:==|!=|is|not)",
                # Variables with default filter: {{ variable | default(...) }}
                r"\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\|\s*default",
                # Loop variables: with_items: "{{ variable }}"
                r'with_items:\s*["\']?\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}["\']?',
                # Loop variables: loop: "{{ variable }}"
                r'loop:\s*["\']?\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}["\']?',
                # Register variables used in conditionals: when: result.variable
                r"when:\s*[a-zA-Z_][a-zA-Z0-9_]*\.([a-zA-Z_][a-zA-Z0-9_]*)",
                # Variable references in module parameters: param: "{{ variable }}"
                r':\s*["\']?\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}["\']?',
                # Variables in environment: ENV_VAR: "{{ variable }}"
                r'environment:\s*\n(?:\s*[A-Z_]+:\s*["\']?\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}["\']?\s*\n?)+',
                # Variable references in strings: "string with {{ variable }}"
                r'"[^"]*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}[^"]*"',
                # Single quoted strings
                r"'[^']*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}[^']*'",
                # Variables in tags: tags: "{{ variable }}"
                r'tags:\s*["\']?\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}["\']?',
                # Variables in vars sections: vars: variable: value
                r"vars:\s*\n\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:",
            ]

            # Extract variables using all patterns (with multiline and dotall for complex YAML)
            for pattern in patterns:
                matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                for match in matches:
                    # Handle both single matches and tuple matches (multiple capture groups)
                    if isinstance(match, tuple):
                        # Multiple capture groups - process each one
                        for var_name in match:
                            if var_name and var_name.strip():
                                var_name = var_name.strip()
                                if (
                                    self._is_valid_role_variable(var_name)
                                    and var_name not in excluded_vars
                                ):
                                    variables.add(var_name)
                    else:
                        # Single capture group
                        var_name = match.strip()
                        if (
                            self._is_valid_role_variable(var_name)
                            and var_name not in excluded_vars
                        ):
                            variables.add(var_name)

            # Additional check for variables in task names and other common locations
            task_name_pattern = r'name:\s*["\']?[^"\'\n]*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}'
            task_name_matches = re.findall(
                task_name_pattern, content, re.MULTILINE | re.DOTALL
            )
            for match in task_name_matches:
                var_name = match.strip()
                if (
                    self._is_valid_role_variable(var_name)
                    and var_name not in excluded_vars
                ):
                    variables.add(var_name)

            if variables:
                self.log_trace(
                    f"Variables found in {task_file_path.name}: {', '.join(sorted(variables))}"
                )

        except UnicodeDecodeError as e:
            self.log_verbose(f"Warning: Could not decode file {task_file_path}: {e}")
        except (OSError, IOError) as e:
            self.log_verbose(f"Warning: Could not read file {task_file_path}: {e}")
        except re.error as e:
            self.log_verbose(f"Warning: Regex pattern error in {task_file_path}: {e}")
        except Exception as e:
            self.log_verbose(
                f"Warning: Could not extract variables from {task_file_path}: {e}"
            )

        return variables

    def _is_valid_role_variable(self, var_name: str) -> bool:
        """Check if a variable name is valid for a role (filter out Ansible built-ins)"""
        if not var_name or not isinstance(var_name, str):
            return False

        # Filter out built-in Ansible variables and common false positives
        builtin_variables = {
            "ansible_facts",
            "ansible_hostname",
            "ansible_fqdn",
            "ansible_host",
            "inventory_hostname",
            "inventory_hostname_short",
            "group_names",
            "groups",
            "hostvars",
            "vars",
            "ansible_user",
            "ansible_ssh_user",
            "ansible_become_user",
            "ansible_connection",
            "ansible_port",
            "playbook_dir",
            "role_path",
            "inventory_dir",
            "ansible_check_mode",
            "ansible_diff_mode",
            "ansible_verbosity",
            "ansible_version",
            "ansible_python_interpreter",
            "ansible_date_time",
            "omit",
            "undef",
            "none",
            "false",
            "true",
            "yes",
            "no",
            "item",
            "ansible_loop",
            "loop",
            "ansible_loop_var",
        }

        # Skip built-in variables
        if var_name.lower() in builtin_variables:
            return False

        # Skip variables that start with ansible_ (built-ins)
        if var_name.startswith("ansible_"):
            return False

        # Skip variables that look like loop items or temporary vars
        if var_name in {"item", "loop_var", "outer_item"}:
            return False

        # Skip internal/private variables that start with double underscore
        if var_name.startswith("__"):
            return False

        # Skip very short variable names (likely false positives)
        if len(var_name) < 2:
            return False

        # Must start with letter or underscore and contain only valid characters
        if not re.match(r"^[a-zA-Z_][a-zA-Z0-9_]*$", var_name):
            return False

        return True

    def _analyze_variable_usage_context(self, content: str, task_file_path: Path):
        """Analyze how variables are used in task files to generate better descriptions"""
        try:
            # Parse YAML to understand task structure
            tasks = yaml.safe_load(content)
            if not isinstance(tasks, list):
                return

            for task in tasks:
                if not isinstance(task, dict):
                    continue

                # Analyze different task types and their variable usage
                self._analyze_task_modules(task, task_file_path)

        except yaml.YAMLError:
            # If YAML parsing fails, fall back to regex analysis
            self._analyze_content_patterns(content, task_file_path)
        except Exception:
            # Silently handle any other errors
            pass

    def _analyze_task_modules(self, task: dict, task_file_path: Path):
        """Analyze specific Ansible modules and their variable usage"""
        module_contexts = {
            # File operations
            "copy": {
                "src": "source file path",
                "dest": "destination file path",
                "content": "file content",
            },
            "template": {"src": "template file path", "dest": "destination file path"},
            "file": {
                "path": "file or directory path",
                "state": "file state",
                "mode": "file permissions",
            },
            "lineinfile": {
                "path": "target file path",
                "line": "line content",
                "regexp": "search pattern",
            },
            # Package management
            "package": {"name": "package name", "state": "package state"},
            "yum": {"name": "package name", "state": "package state"},
            "apt": {"name": "package name", "state": "package state"},
            "pip": {"name": "Python package name", "state": "package state"},
            # Service management
            "service": {
                "name": "service name",
                "state": "service state",
                "enabled": "service startup",
            },
            "systemd": {
                "name": "systemd service name",
                "state": "service state",
                "enabled": "service startup",
            },
            # User management
            "user": {
                "name": "username",
                "state": "user account state",
                "home": "home directory",
            },
            "group": {"name": "group name", "state": "group state"},
            # Command execution
            "command": {"cmd": "command to execute", "chdir": "working directory"},
            "shell": {"cmd": "shell command", "chdir": "working directory"},
            "script": {"cmd": "script path", "chdir": "working directory"},
            # Network
            "uri": {
                "url": "target URL",
                "method": "HTTP method",
                "headers": "HTTP headers",
            },
            "get_url": {"url": "source URL", "dest": "destination path"},
            # Archive operations
            "unarchive": {"src": "archive file path", "dest": "extraction path"},
            "archive": {"path": "source path", "dest": "archive destination"},
        }

        for module_name, param_contexts in module_contexts.items():
            if module_name in task:
                module_params = task[module_name]
                if isinstance(module_params, dict):
                    for param, context_desc in param_contexts.items():
                        if param in module_params:
                            param_value = module_params[param]
                            # Extract variables from parameter values
                            variables = self._extract_variables_from_value(param_value)
                            for var in variables:
                                self._store_variable_context(
                                    var,
                                    {
                                        "context": context_desc,
                                        "module": module_name,
                                        "parameter": param,
                                        "file": task_file_path.stem,
                                    },
                                )

    def _analyze_content_patterns(self, content: str, task_file_path: Path):
        """Analyze content patterns when YAML parsing fails"""
        # Look for common patterns that indicate variable purpose
        context_patterns = {
            r'dest:\s*["\']?.*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}': "destination file path",
            r'src:\s*["\']?.*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}': "source file path",
            r'name:\s*["\']?\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}': "resource name",
            r'state:\s*["\']?\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}': "resource state",
            r"enabled:\s*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}": "enable/disable setting",
            r"port:\s*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}": "port number",
            r'url:\s*["\']?.*\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*\}\}': "URL address",
        }

        for pattern, context_desc in context_patterns.items():
            matches = re.findall(pattern, content, re.MULTILINE)
            for var in matches:
                self._store_variable_context(
                    var, {"context": context_desc, "file": task_file_path.stem}
                )

    def _extract_variables_from_value(self, value) -> List[str]:
        """Extract variable names from parameter values"""
        if not isinstance(value, str):
            return []

        # Extract variables from Jinja2 templates
        var_pattern = r"\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\|[^}]*)?\s*\}\}"
        return re.findall(var_pattern, value)

    def _store_variable_context(self, var_name: str, context: Dict[str, Any]):
        """Store context information for a variable"""
        if var_name not in self.variable_context:
            self.variable_context[var_name] = {}

        # Store multiple contexts if variable is used in different ways
        context_key = (
            f"{context.get('module', 'unknown')}_{context.get('parameter', 'param')}"
        )
        self.variable_context[var_name][context_key] = context

    def _is_valid_role_variable(self, var_name: str) -> bool:
        """Check if a variable name is a valid role variable (not built-in)"""
        if not var_name or not isinstance(var_name, str):
            return False

        # Filter out built-in Ansible variables and complex expressions
        builtin_prefixes = [
            "ansible_",
            "hostvars",
            "group_names",
            "groups",
            "inventory_hostname",
            "inventory_hostname_short",
            "play_hosts",
            "omit",
            "item",
            "loop",
        ]

        # Filter out boolean literals and common non-variable words
        non_variables = {
            "true",
            "false",
            "yes",
            "no",
            "on",
            "off",
            "null",
            "none",
            "and",
            "or",
            "not",
            "in",
            "is",
            "defined",
            "undefined",
            "version",
            "default",
            "production",
            "staging",
            "development",
        }

        # Filter out variables that are likely not role parameters
        if (
            any(var_name.startswith(prefix) for prefix in builtin_prefixes)
            or var_name.lower() in non_variables
            or "(" in var_name  # Boolean literals and keywords
            or "[" in var_name  # Function calls
            or "." in var_name  # Array access
            or " " in var_name  # Nested access
            or var_name.isdigit()  # Complex expressions
            or len(var_name) < 2  # Numeric values
            or var_name.startswith("_")  # Too short
        ):  # Private variables
            return False

        return True

    def parse_task_file_includes(self, task_file_path: Path) -> Set[str]:
        """Parse a task file to find included/imported files"""
        includes = set()

        try:
            with open(task_file_path, "r", encoding="utf-8") as f:
                content = f.read()

            if not content.strip():
                return includes  # Return empty set for empty files

            print(f"      Parsing {task_file_path.name} for includes...")

            # First try YAML parsing
            yaml_success = False
            try:
                tasks = yaml.safe_load(content)
                if isinstance(tasks, list):
                    for task in tasks:
                        if isinstance(task, dict):
                            # Look for include_tasks, import_tasks, include, import_playbook (NOT include_role)
                            for include_type in [
                                "include_tasks",
                                "import_tasks",
                                "include",
                                "import_playbook",
                            ]:
                                if include_type in task:
                                    include_value = task[include_type]
                                    if isinstance(include_value, str):
                                        # Extract filename without extension and remove path
                                        include_file = Path(include_value).stem
                                        includes.add(include_file)
                                        print(
                                            f"        Found include via YAML: {include_file}"
                                        )
                                    elif (
                                        isinstance(include_value, dict)
                                        and "file" in include_value
                                    ):
                                        include_file = Path(include_value["file"]).stem
                                        includes.add(include_file)
                                        print(
                                            f"        Found include via YAML (dict): {include_file}"
                                        )
                    yaml_success = True
            except yaml.YAMLError as e:
                print(f"        YAML parsing failed: {e}")

            # If YAML parsing didn't work or found nothing, try regex
            if not yaml_success or not includes:
                print(f"        Falling back to regex parsing...")
                # More comprehensive regex patterns (excluding include_role which includes roles, not task files)
                include_patterns = [
                    # Simple format: include_tasks: filename.yml
                    r"(?:include_tasks|import_tasks|include|import_playbook):\s*([^\s\n\#]+)",
                    # Dict format with file key
                    r"(?:include_tasks|import_tasks|include|import_playbook):\s*\n\s*file:\s*([^\s\n\#]+)",
                    # Block includes
                    r"block:\s*\n.*?include_tasks:\s*([^\s\n\#]+)",
                ]

                for pattern in include_patterns:
                    matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                    for match in matches:
                        # Clean up the match
                        clean_match = match.strip("'\"").strip()
                        if clean_match and not clean_match.startswith("#"):
                            # Extract just the filename without extension and path
                            include_file = Path(clean_match).stem
                            if (
                                include_file and include_file != task_file_path.stem
                            ):  # Don't include self
                                includes.add(include_file)
                                print(
                                    f"        Found include via regex: {include_file}"
                                )

            if not includes:
                print(f"        No includes found in {task_file_path.name}")

        except UnicodeDecodeError as e:
            self.log_error(f"Could not decode task file {task_file_path}: {e}")
        except (OSError, IOError) as e:
            self.log_error(f"Could not read task file {task_file_path}: {e}")
        except yaml.YAMLError as e:
            self.log_verbose(f"YAML parsing error in {task_file_path}: {e}")
        except re.error as e:
            self.log_error(f"Regex pattern error in {task_file_path}: {e}")
        except Exception as e:
            self.log_verbose(f"Could not parse task file {task_file_path}: {e}")

        return includes

    def analyze_role_structure(self, role_path: str) -> Dict[str, Any]:
        """Analyze a role's structure to understand its arguments"""
        role_dir = Path(role_path)

        # Validate role path exists
        if not role_dir.exists():
            raise FileNotFoundError(f"Role path does not exist: {role_path}")

        if not role_dir.is_dir():
            raise NotADirectoryError(f"Role path is not a directory: {role_path}")

        analysis = {
            "defaults": {},
            "vars": {},
            "task_vars": set(),
            "template_vars": set(),
            "variables": {},  # Dict with variable names as keys and metadata as values
            "has_entry_points": False,
            "entry_points": {  # Dict with entry point names as keys
                "main": {
                    "variables": {},  # Variables specific to this entry point
                    "description": "",
                    "short_description": "",
                }
            },
            "included_files": set(),  # Files included by other files
            "all_task_files": set(),  # All task files found
            "file_variables": {},  # Variables found in each file
            "authors": [],  # Authors from meta/main.yml
            "meta_description": [],  # Description from meta/main.yml
            "meta_short_description": "",  # Short description from meta/main.yml
            "meta_info": {},  # Combined meta information - expected by tests
            "version_info": {},  # Version information - expected by tests
            "version": "1.0.0",  # Current version for version_added
            "is_collection": False,  # Whether this is part of a collection
        }

        # Detect version information
        version_info = self._detect_version_info(role_dir)
        analysis["version"] = version_info["version"]
        analysis["is_collection"] = version_info["is_collection"]
        analysis["version_info"] = version_info  # Store full version info for tests
        if version_info["version"] != "1.0.0":
            self.log_debug(
                f"Detected version: {version_info['version']} ({'collection' if version_info['is_collection'] else 'role'})"
            )
        else:
            self.log_trace(f"Using default version: {version_info['version']}")

        # Analyze defaults/main.yml
        defaults_file = role_dir / "defaults" / "main.yml"
        if defaults_file.exists():
            defaults = self._safe_load_yaml_file(defaults_file)
            if defaults is not None:
                # Handle case where YAML file contains only comments or is empty
                analysis["defaults"] = defaults if isinstance(defaults, dict) else {}
            else:
                analysis["defaults"] = {}

        # Analyze vars/main.yml
        vars_file = role_dir / "vars" / "main.yml"
        if vars_file.exists():
            vars_data = self._safe_load_yaml_file(vars_file)
            if vars_data is not None:
                # Handle case where YAML file contains only comments or is empty
                analysis["vars"] = vars_data if isinstance(vars_data, dict) else {}
            else:
                analysis["vars"] = {}

        # Analyze meta/main.yml for author information
        meta_file = role_dir / "meta" / "main.yml"
        if meta_file.exists():
            try:
                with open(meta_file, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    if content:  # Only try to parse if file has content
                        meta_data = yaml.safe_load(content)
                        if isinstance(meta_data, dict):
                            # Extract author information
                            authors = self._extract_authors_from_meta(meta_data)
                            analysis["authors"] = authors
                            if authors:
                                self.log_verbose(
                                    f"Found {len(authors)} author(s): {', '.join(authors)}"
                                )

                            # Extract description information
                            descriptions = self._extract_descriptions_from_meta(
                                meta_data
                            )
                            analysis["meta_description"] = descriptions.get(
                                "description", []
                            )
                            analysis["meta_short_description"] = descriptions.get(
                                "short_description", ""
                            )
                            if descriptions.get("description") or descriptions.get(
                                "short_description"
                            ):
                                self.log_verbose(
                                    f"Found description from meta/main.yml"
                                )
                    else:
                        analysis["authors"] = []
                        analysis["meta_description"] = []
                        analysis["meta_short_description"] = ""
            except yaml.YAMLError as e:
                self.log_verbose(f"Invalid YAML in {meta_file}: {e}")
                analysis["authors"] = []
                analysis["meta_description"] = []
                analysis["meta_short_description"] = ""
            except (OSError, IOError) as e:
                self.log_verbose(f"Could not read {meta_file}: {e}")
                analysis["authors"] = []
                analysis["meta_description"] = []
                analysis["meta_short_description"] = ""
            except Exception as e:
                self.log_verbose(f"Could not parse {meta_file}: {e}")
                analysis["authors"] = []
                analysis["meta_description"] = []
                analysis["meta_short_description"] = ""

        # Analyze task files
        self._analyze_task_files(role_dir, analysis)

        # Populate fields expected by tests
        analysis["meta_info"] = {
            "authors": analysis["authors"],
            "description": analysis["meta_description"],
            "short_description": analysis["meta_short_description"],
        }

        # Combine all variables into the 'variables' field expected by tests (as dict)
        all_variables = {}

        # Add variables from defaults with metadata
        if analysis.get("defaults"):
            for var_name, default_value in analysis["defaults"].items():
                all_variables[var_name] = {
                    "type": "str",  # Default type
                    "default": default_value,
                    "required": False,
                    "description": f"Variable from defaults: {var_name}",
                }

        # Add variables from vars
        if analysis.get("vars"):
            for var_name, var_value in analysis["vars"].items():
                if var_name not in all_variables:
                    all_variables[var_name] = {
                        "type": "str",
                        "required": True,
                        "description": f"Variable from vars: {var_name}",
                    }

        # Add variables found in tasks (basic metadata)
        for var_name in analysis.get("task_vars", set()):
            if var_name not in all_variables:
                all_variables[var_name] = {
                    "type": "str",
                    "required": True,
                    "description": f"Variable used in tasks: {var_name}",
                }

        # Add variables found in templates
        for var_name in analysis.get("template_vars", set()):
            if var_name not in all_variables:
                all_variables[var_name] = {
                    "type": "str",
                    "required": True,
                    "description": f"Variable used in templates: {var_name}",
                }

        analysis["variables"] = all_variables

        # Also populate entry point variables (for tests that expect this structure)
        analysis["entry_points"]["main"]["variables"] = all_variables.copy()

        return analysis

    def _detect_version_info(self, role_dir: Path) -> Dict[str, Any]:
        """Detect version information from collection or role metadata"""
        version_info = {"version": "1.0.0", "is_collection": False, "source": "default"}

        # First check if we're in a collection by looking for galaxy.yml
        current_dir = role_dir
        collection_root = None

        # Look up the directory tree for galaxy.yml (max 3 levels up)
        for _ in range(3):
            current_dir = current_dir.parent
            galaxy_file = current_dir / "galaxy.yml"
            if galaxy_file.exists():
                collection_root = current_dir
                break

        if collection_root:
            # We're in a collection, get collection version
            try:
                with open(collection_root / "galaxy.yml", "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    if content:
                        galaxy_data = yaml.safe_load(content)
                        if isinstance(galaxy_data, dict) and "version" in galaxy_data:
                            version_info["version"] = str(galaxy_data["version"])
                            version_info["is_collection"] = True
                            version_info["source"] = "collection"
                            return version_info
            except Exception as e:
                self.log_verbose(f"Could not parse galaxy.yml: {e}")

        # Not in a collection or no collection version found, check role version
        meta_file = role_dir / "meta" / "main.yml"
        if meta_file.exists():
            try:
                with open(meta_file, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    if content:
                        meta_data = yaml.safe_load(content)
                        if isinstance(meta_data, dict):
                            # Look for version in various locations
                            version_fields = [
                                "version",
                                "galaxy_info.version",
                                "galaxy_info.role_version",
                            ]

                            for field in version_fields:
                                version = self._get_nested_value(meta_data, field)
                                if version:
                                    version_info["version"] = str(version)
                                    version_info["is_collection"] = False
                                    version_info["source"] = "role"
                                    return version_info
            except Exception as e:
                self.log_verbose(f"Could not parse meta/main.yml for version: {e}")

        return version_info

    def _extract_authors_from_meta(self, meta_data: Dict[str, Any]) -> List[str]:
        """Extract author information from meta/main.yml data"""
        authors = []

        # Look for author information in various possible locations
        author_fields = [
            "author",  # Standard field
            "authors",  # Alternative plural form
            "galaxy_info.author",  # Nested in galaxy_info
            "galaxy_info.authors",  # Nested plural form
        ]

        for field in author_fields:
            value = self._get_nested_value(meta_data, field)
            if value:
                if isinstance(value, str):
                    # Single author as string
                    authors.append(value.strip())
                elif isinstance(value, list):
                    # Multiple authors as list
                    for author in value:
                        if isinstance(author, str) and author.strip():
                            authors.append(author.strip())
                break  # Use the first found author field

        # Remove duplicates while preserving order
        unique_authors = []
        for author in authors:
            if author not in unique_authors:
                unique_authors.append(author)

        return unique_authors

    def _extract_descriptions_from_meta(
        self, meta_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract description information from meta/main.yml data"""
        descriptions = {"description": [], "short_description": ""}

        # Look for description information in various possible locations
        description_fields = [
            "description",  # Standard field
            "galaxy_info.description",  # Nested in galaxy_info
            "galaxy_info.role_description",  # Alternative field
        ]

        short_description_fields = [
            "short_description",  # Standard field
            "galaxy_info.short_description",  # Nested in galaxy_info
            "galaxy_info.summary",  # Alternative field
        ]

        # Extract full description
        for field in description_fields:
            value = self._get_nested_value(meta_data, field)
            if value:
                if isinstance(value, str):
                    # Single description as string, keep as string for test compatibility
                    descriptions["description"] = value.strip()
                elif isinstance(value, list):
                    # Multiple description lines as list
                    desc_lines = []
                    for line in value:
                        if isinstance(line, str) and line.strip():
                            desc_lines.append(line.strip())
                    descriptions["description"] = desc_lines
                break  # Use the first found description field

        # Extract short description
        for field in short_description_fields:
            value = self._get_nested_value(meta_data, field)
            if value and isinstance(value, str):
                descriptions["short_description"] = value.strip()
                break  # Use the first found short description field

        return descriptions

    def _get_nested_value(self, data: Dict[str, Any], path: str) -> Any:
        """Get a nested value from a dictionary using dot notation"""
        keys = path.split(".")
        current = data

        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return None

        return current

    def _analyze_task_files(self, role_dir: Path, analysis: Dict[str, Any]):
        """Analyze task files to find variables and entry points"""
        tasks_dir = role_dir / "tasks"
        if not tasks_dir.exists():
            return

        task_files = list(tasks_dir.glob("*.yml")) + list(tasks_dir.glob("*.yaml"))
        all_included_files = set()
        file_includes_map = {}  # Map of file -> files it includes

        # Track all task files
        for task_file in task_files:
            analysis["all_task_files"].add(task_file.stem)

        # Find which files are included by others and build inclusion map
        # Also extract variables from each task file
        for task_file in task_files:
            includes = self.parse_task_file_includes(task_file)
            variables = self.extract_variables_from_task_file(task_file)

            all_included_files.update(includes)
            file_includes_map[task_file.stem] = includes
            analysis["file_variables"][task_file.stem] = variables

            # Store which files this file includes for debugging
            if includes:
                self.log_debug(
                    f"{task_file.stem} includes: {', '.join(sorted(includes))}"
                )

        self.log_debug(
            f"All task files found: {', '.join(sorted([f.stem for f in task_files]))}"
        )
        self.log_debug(
            f"All included files: {', '.join(sorted(all_included_files)) if all_included_files else 'None'}"
        )

        analysis["included_files"] = all_included_files
        analysis["file_includes_map"] = file_includes_map

        # Determine entry points
        self._determine_entry_points(task_files, all_included_files, analysis)

    def _determine_entry_points(
        self, task_files: List[Path], all_included_files: set, analysis: Dict[str, Any]
    ):
        """Determine which task files are entry points"""
        # Entry points are files that are NOT included by others
        # Main is always an entry point, others are only entry points if not included
        standalone_files = set()
        for task_file in task_files:
            if task_file.stem != "main" and task_file.stem not in all_included_files:
                standalone_files.add(task_file.stem)

        # Update entry_points structure (keep as dict for test compatibility)
        # Initialize main entry point if not exists
        if "main" not in analysis["entry_points"]:
            analysis["entry_points"]["main"] = {
                "variables": {},
                "description": "",
                "short_description": "",
            }

        # Add standalone files as additional entry points
        if standalone_files:
            for file_name in sorted(standalone_files):
                if file_name not in analysis["entry_points"]:
                    analysis["entry_points"][file_name] = {
                        "variables": {},
                        "description": f"Entry point for {file_name}",
                        "short_description": f"Standalone task file: {file_name}",
                    }
            analysis["has_entry_points"] = True
            self.log_debug(
                f"Found standalone entry points: {', '.join(sorted(standalone_files))}"
            )
            if all_included_files:
                self.log_debug(
                    f"Files included by others (not entry points): {', '.join(sorted(all_included_files))}"
                )
        else:
            self.log_debug(f"Only 'main' entry point found")
            if all_included_files:
                self.log_debug(
                    f"All other files are included: {', '.join(sorted(all_included_files))}"
                )

    def process_collection(self, collection_path: str = "."):
        """Process all roles in a collection"""
        self.log_section("PROCESSING ANSIBLE COLLECTION")

        if not self.is_collection_root(collection_path):
            self.log_error(
                f"Error: {collection_path} does not appear to be an Ansible collection root",
                False,
            )
            self.log_error(
                "Expected to find a 'roles/' directory and preferably 'galaxy.yml'",
                False,
            )
            sys.exit(1)

        roles = self.find_roles(collection_path)
        if not roles:
            self.log_error(f"No roles found in {collection_path}/roles/", False)
            sys.exit(1)

        self.log_info(
            f"Found {len(roles)} roles in collection: {', '.join(roles)}", False
        )

        for role_name in roles:
            self.current_role = role_name
            self.log_info(f"Processing role: {role_name}", False)
            role_path = Path(collection_path) / "roles" / role_name

            try:
                self.process_single_role(str(role_path), role_name)
                self.processed_roles.append(role_name)
                self.stats["roles_processed"] += 1
            except Exception as e:
                self.log_error(f"Error processing role {role_name}: {e}", False)
                self.stats["roles_failed"] += 1
                continue

        self.current_role = ""
        self.log_summary()

    def _merge_included_variables(
        self,
        entry_point: EntryPointSpec,
        entry_point_name: str,
        analysis: Dict[str, Any],
        existing_options: Dict[str, Dict[str, Any]] = None,
    ):
        """Merge variables from included task files into the entry point"""
        if existing_options is None:
            existing_options = {}

        included_files = analysis.get("file_includes_map", {}).get(
            entry_point_name, set()
        )
        file_variables = analysis.get("file_variables", {})

        # Get variables from directly included files
        for included_file in included_files:
            if included_file in file_variables:
                file_vars = file_variables[included_file]
                for var_name in file_vars:
                    if var_name not in entry_point.options:
                        # Use existing description and version info if available
                        existing_opt = existing_options.get(var_name, {})
                        existing_desc = existing_opt.get("description")
                        existing_version = existing_opt.get("version_added")
                        is_existing = existing_opt.get("_existing", False)

                        if existing_desc:
                            description = existing_desc
                            self.log_trace(f"Using existing description for {var_name}")
                        else:
                            description = f"Variable used in included task file: {included_file}.yml"

                        # Determine version_added for task file variables
                        version_added = None
                        if existing_version:
                            version_added = existing_version
                            self.log_trace(
                                f"Using existing version_added for {var_name}: {version_added}"
                            )
                        elif is_existing:
                            self.log_trace(
                                f"Variable {var_name} existed in argument specs - not adding version_added"
                            )
                        else:
                            version_added = analysis["version"]
                            self.log_trace(
                                f"Adding version_added for new task variable {var_name}: {version_added}"
                            )

                        # Create a basic argument spec for variables found in task files
                        # Variables from task files without defaults are likely required
                        # But check if they have defaults in the analysis
                        has_default = var_name in analysis.get(
                            "defaults", {}
                        ) or var_name in analysis.get("vars", {})

                        arg_spec = ArgumentSpec(
                            name=var_name,
                            type="str",  # Default to string, could be improved with type inference
                            description=description,
                            version_added=version_added,
                            required=not has_default,  # Only required if no default value exists
                        )
                        entry_point.options[var_name] = arg_spec
                        req_status = "required" if not has_default else "optional"
                        self.log_debug(
                            f"Added {req_status} variable from {included_file}.yml: {var_name}"
                        )

        # Recursively get variables from files included by the included files
        def collect_recursive_variables(file_name: str, visited: set):
            if file_name in visited:
                return
            visited.add(file_name)

            # Get variables from this file
            if file_name in file_variables:
                file_vars = file_variables[file_name]
                for var_name in file_vars:
                    if var_name not in entry_point.options:
                        # Use existing description and version info if available
                        existing_opt = existing_options.get(var_name, {})
                        existing_desc = existing_opt.get("description")
                        existing_version = existing_opt.get("version_added")
                        is_existing = existing_opt.get("_existing", False)

                        if existing_desc:
                            description = existing_desc
                            self.log_trace(f"Using existing description for {var_name}")
                        else:
                            description = (
                                f"Variable used in included task file: {file_name}.yml"
                            )

                        # Determine version_added for recursive task file variables
                        version_added = None
                        if existing_version:
                            version_added = existing_version
                            self.log_trace(
                                f"Using existing version_added for {var_name}: {version_added}"
                            )
                        elif is_existing:
                            self.log_trace(
                                f"Variable {var_name} existed in argument specs - not adding version_added"
                            )
                        else:
                            version_added = analysis["version"]
                            self.log_trace(
                                f"Adding version_added for new task variable {var_name}: {version_added}"
                            )

                        # Variables from task files without defaults are likely required
                        # But check if they have defaults in the analysis
                        has_default = var_name in analysis.get(
                            "defaults", {}
                        ) or var_name in analysis.get("vars", {})

                        arg_spec = ArgumentSpec(
                            name=var_name,
                            type="str",
                            description=description,
                            version_added=version_added,
                            required=not has_default,  # Only required if no default value exists
                        )
                        entry_point.options[var_name] = arg_spec
                        req_status = "required" if not has_default else "optional"
                        self.log_debug(
                            f"Added {req_status} variable from {file_name}.yml (recursive): {var_name}"
                        )

            # Recursively check files that this file includes
            sub_includes = analysis.get("file_includes_map", {}).get(file_name, set())
            for sub_included in sub_includes:
                collect_recursive_variables(sub_included, visited)

        # Collect variables recursively for all included files
        visited_files = set()
        for included_file in included_files:
            collect_recursive_variables(included_file, visited_files)

    def _add_entry_point_variables(
        self,
        entry_point: EntryPointSpec,
        entry_point_name: str,
        analysis: Dict[str, Any],
        existing_options: Dict[str, Dict[str, Any]] = None,
    ):
        """Add variables found directly in the entry point file itself"""
        if existing_options is None:
            existing_options = {}

        file_variables = analysis.get("file_variables", {})
        entry_point_vars = file_variables.get(entry_point_name, set())

        self.log_debug(
            f"Adding variables from entry point file '{entry_point_name}': {sorted(entry_point_vars) if entry_point_vars else 'none'}"
        )

        for var_name in entry_point_vars:
            if var_name not in entry_point.options:
                # Use existing description and version info if available
                existing_opt = existing_options.get(var_name, {})
                existing_desc = existing_opt.get("description")
                existing_version = existing_opt.get("version_added")
                is_existing = existing_opt.get("_existing", False)

                if existing_desc:
                    description = existing_desc
                    self.log_trace(f"Using existing description for {var_name}")
                else:
                    description = f"Variable used in {entry_point_name} entry point"

                # Determine version_added for entry point variables
                version_added = None
                if existing_version:
                    version_added = existing_version
                    self.log_trace(
                        f"Using existing version_added for {var_name}: {version_added}"
                    )
                elif is_existing:
                    self.log_trace(
                        f"Variable {var_name} existed in argument specs - not adding version_added"
                    )
                else:
                    version_added = analysis["version"]
                    self.log_trace(
                        f"Adding version_added for new entry point variable {var_name}: {version_added}"
                    )

                # Check if variable has defaults to determine if it's required
                has_default = var_name in analysis.get(
                    "defaults", {}
                ) or var_name in analysis.get("vars", {})

                arg_spec = ArgumentSpec(
                    name=var_name,
                    type="str",  # Default to string, could be improved with type inference
                    description=description,
                    version_added=version_added,
                    required=not has_default,  # Only required if no default value exists
                )
                entry_point.options[var_name] = arg_spec
                req_status = "required" if not has_default else "optional"
                self.log_debug(
                    f"Added {req_status} variable from entry point {entry_point_name}: {var_name}"
                )

    def _create_entry_point_spec(
        self,
        entry_point_name: str,
        role_name: str,
        analysis: Dict[str, Any],
        existing_specs: Dict[str, Any] = None,
    ) -> EntryPointSpec:
        """Create an entry point specification from analysis data, preserving existing descriptions"""
        if existing_specs is None:
            existing_specs = {}

        self.log_debug(f"Creating specs for entry point: {entry_point_name}")

        # Use description with priority: existing specs > meta/main.yml > generated
        if "description" in existing_specs:
            description = existing_specs["description"]
            self.log_trace(f"Using existing description for entry point")
        elif analysis.get("meta_description"):
            description = analysis["meta_description"]
            self.log_trace(f"Using description from meta/main.yml")
        else:
            # Build description including information about included files
            description_lines = [
                f"Automatically generated argument specification for the {role_name} role.",
                f"Entry point: {entry_point_name}",
            ]

            # Add information about included files
            included_files = analysis.get("file_includes_map", {}).get(
                entry_point_name, set()
            )
            if included_files:
                description_lines.append(
                    f"Includes task files: {', '.join(sorted(included_files))}"
                )

            description = description_lines

        # Use short_description with priority: existing specs > meta/main.yml > generated
        if "short_description" in existing_specs:
            short_description = existing_specs["short_description"]
            self.log_trace(f"Using existing short_description for entry point")
        elif analysis.get("meta_short_description"):
            short_description = analysis["meta_short_description"]
            self.log_trace(f"Using short_description from meta/main.yml")
        else:
            short_description = f"Auto-generated specs for {role_name} role - {entry_point_name} entry point"

        # Use existing author if available, otherwise use from meta/main.yml
        if "author" in existing_specs:
            author = existing_specs["author"]
            self.log_trace(f"Using existing author for entry point")
        else:
            author = analysis.get("authors", [])
            if author:
                self.log_trace(
                    f"Using author(s) from meta/main.yml: {', '.join(author)}"
                )

        entry_point = EntryPointSpec(
            name=entry_point_name,
            short_description=short_description,
            description=description,
            author=author,
        )

        existing_options = existing_specs.get("options", {})
        if existing_options:
            self.log_trace(
                f"Existing options for entry point: {list(existing_options.keys())}"
            )

        # Add arguments from defaults
        for var_name, var_value in analysis["defaults"].items():
            existing_opt = existing_options.get(var_name, {})
            existing_desc = existing_opt.get("description")
            existing_version = existing_opt.get("version_added")
            is_existing = existing_opt.get("_existing", False)

            self.log_trace(
                f"Processing default variable '{var_name}': existing={is_existing}, has_existing_version={bool(existing_version)}"
            )

            arg_spec = self._infer_argument_spec(
                var_name,
                var_value,
                existing_desc,
                existing_version,
                is_existing,
                analysis["version"],
            )
            entry_point.options[var_name] = arg_spec

        # Add arguments from vars (but mark them as having defaults in vars)
        for var_name, var_value in analysis["vars"].items():
            if var_name not in entry_point.options:
                existing_opt = existing_options.get(var_name, {})
                existing_desc = existing_opt.get("description")
                existing_version = existing_opt.get("version_added")
                is_existing = existing_opt.get("_existing", False)
                arg_spec = self._infer_argument_spec(
                    var_name,
                    var_value,
                    existing_desc,
                    existing_version,
                    is_existing,
                    analysis["version"],
                )
                if (
                    not existing_desc
                ):  # Only modify description if not from existing specs
                    arg_spec.description = f"{arg_spec.description} (defined in vars)"
                entry_point.options[var_name] = arg_spec

        # Add variables from the entry point file itself (e.g., main.yml)
        self._add_entry_point_variables(
            entry_point, entry_point_name, analysis, existing_options
        )

        # Add variables from task files that this entry point includes
        self._merge_included_variables(
            entry_point, entry_point_name, analysis, existing_options
        )

        return entry_point

    def process_single_role(self, role_path: str, role_name: str = None):
        """Process a single role to generate argument specs"""
        role_dir = Path(role_path)
        if role_name is None:
            role_name = role_dir.name

        if not self.current_role:
            self.current_role = role_name

        self.log_verbose("Analyzing role structure...")
        # Clear variable context for this role to avoid mixing contexts
        self.variable_context.clear()

        analysis = self.analyze_role_structure(role_path)

        # Load existing specs to preserve manual descriptions
        existing_specs = self.load_existing_specs(role_path)
        if existing_specs:
            self.log_verbose(
                f"Loaded existing specs for entry points: {list(existing_specs.keys())}"
            )
        else:
            self.log_verbose("No existing argument specs found")

        # Clear any existing entry points for this role
        self.entry_points.clear()

        # Generate specs for each entry point
        for entry_point_name in analysis["entry_points"]:
            existing_entry_specs = existing_specs.get(entry_point_name, {})
            if existing_entry_specs:
                existing_options_count = len(existing_entry_specs.get("options", {}))
                self.log_debug(
                    f"Entry point '{entry_point_name}' has {existing_options_count} existing options"
                )
            else:
                self.log_debug(
                    f"Entry point '{entry_point_name}' has no existing specs"
                )

            entry_point = self._create_entry_point_spec(
                entry_point_name, role_name, analysis, existing_entry_specs
            )
            self.add_entry_point(entry_point)
            self.stats["entry_points_created"] += 1

        self.log_verbose(
            f"Generated specs for {len(analysis['entry_points'])} entry point(s)"
        )
        self.log_verbose(f"Found {len(analysis['defaults'])} default variables")

        # Show summary of version_added assignments for this role's entry points
        total_vars = 0
        new_vars = 0
        existing_vars = 0

        # Only count variables from entry points created for this role
        for entry_point_name in analysis["entry_points"]:
            if entry_point_name in self.entry_points:
                entry_point = self.entry_points[entry_point_name]
                for var_name, arg_spec in entry_point.options.items():
                    total_vars += 1
                    if arg_spec.version_added:
                        if arg_spec.version_added == analysis["version"]:
                            new_vars += 1
                        else:
                            existing_vars += 1
                    else:
                        existing_vars += 1

        # Update global stats
        self.stats["total_variables"] += total_vars
        self.stats["new_variables"] += new_vars
        self.stats["existing_variables"] += existing_vars

        if total_vars > 0:
            self.log_verbose(
                f"Version tracking: {new_vars} new variables, {existing_vars} existing variables"
            )

        # In collection mode, save specs immediately for this role
        if self.collection_mode:
            self.save_role_specs(role_path, role_name)

    def load_existing_specs(self, role_path: str) -> Dict[str, Dict[str, Any]]:
        """Load existing argument specs to preserve manual descriptions"""
        specs_file = Path(role_path) / "meta" / "argument_specs.yml"
        existing_specs = {}

        if not specs_file.exists():
            return existing_specs

        try:
            with open(specs_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if not content:
                    return existing_specs

                specs_data = yaml.safe_load(content)
                if (
                    not isinstance(specs_data, dict)
                    or "argument_specs" not in specs_data
                ):
                    return existing_specs

                # Extract existing descriptions for each entry point and option
                for entry_name, entry_data in specs_data["argument_specs"].items():
                    if not isinstance(entry_data, dict):
                        continue

                    entry_specs = {}

                    # Preserve entry point descriptions and author
                    if "description" in entry_data:
                        entry_specs["description"] = entry_data["description"]
                    if "short_description" in entry_data:
                        entry_specs["short_description"] = entry_data[
                            "short_description"
                        ]
                    if "author" in entry_data:
                        entry_specs["author"] = entry_data["author"]

                    # Preserve option descriptions and track existing variables
                    if "options" in entry_data and isinstance(
                        entry_data["options"], dict
                    ):
                        entry_specs["options"] = {}
                        for opt_name, opt_data in entry_data["options"].items():
                            # Mark ALL variables found in existing specs as existing
                            opt_spec = {"_existing": True}

                            if isinstance(opt_data, dict):
                                if "description" in opt_data:
                                    opt_spec["description"] = opt_data["description"]
                                if "version_added" in opt_data:
                                    opt_spec["version_added"] = opt_data[
                                        "version_added"
                                    ]
                            else:
                                # Even if it's not a dict, it was still in the existing specs
                                self.log_debug(
                                    f"Warning: Variable '{opt_name}' in existing specs is not properly structured"
                                )

                            entry_specs["options"][opt_name] = opt_spec

                    existing_specs[entry_name] = entry_specs

                self.log_debug(
                    f"Loaded existing specs with {len(existing_specs)} entry point(s)"
                )
                for ep_name, ep_data in existing_specs.items():
                    options_count = len(ep_data.get("options", {}))
                    self.log_trace(
                        f"Entry point '{ep_name}': {options_count} existing options"
                    )

        except yaml.YAMLError as e:
            self.log_verbose(
                f"Warning: Could not parse existing specs file {specs_file}: {e}"
            )
        except Exception as e:
            self.log_verbose(
                f"Warning: Could not load existing specs file {specs_file}: {e}"
            )

        return existing_specs

    def save_role_specs(self, role_path: str, role_name: str = None):
        """Save argument specs for a single role"""
        if role_name is None:
            role_name = Path(role_path).name

        if not self.entry_points:
            self.log_verbose(f"No entry points found for role {role_name}")
            return

        output_file = Path(role_path) / "meta" / "argument_specs.yml"

        try:
            # Ensure meta directory exists
            output_file.parent.mkdir(parents=True, exist_ok=True)

            # Use the generate_yaml method to get proper YAML with markers
            yaml_content = self.generate_yaml()

            with open(output_file, "w") as f:
                f.write(yaml_content)

            self.log_verbose(f"Saved argument specs to: {output_file}")

        except Exception as e:
            self.log_error(f"Error saving specs for role {role_name}: {e}")

    def generate_yaml(self) -> str:
        """Generate YAML content for argument specs"""
        if not self.entry_points:
            return "---\nargument_specs: {}\n...\n"

        # Build the specs dictionary
        specs = {"argument_specs": {}}

        for entry_point_name, entry_point in self.entry_points.items():
            specs["argument_specs"][entry_point_name] = entry_point.to_dict()

        # Custom YAML dumper for better formatting
        class CustomDumper(yaml.SafeDumper):
            def write_line_break(self, data=None):
                super().write_line_break(data)
                if len(self.indents) == 1:
                    super().write_line_break()

            def increase_indent(self, flow=False, indentless=False):
                return super().increase_indent(flow, False)

            def ignore_aliases(self, data):
                # Disable YAML references/anchors (like *id001) for cleaner output
                return True

            def ignore_aliases(self, data):
                # Disable YAML references/anchors (like *id001) for cleaner output
                return True

        # Configure YAML output for better readability
        yaml_content = yaml.dump(
            specs,
            Dumper=CustomDumper,
            default_flow_style=False,
            sort_keys=False,
            indent=2,
            width=120,  # Prevent overly long lines
            allow_unicode=True,
        )

        # Add document markers
        return f"---\n{yaml_content}...\n"

    def save_all_role_specs(self, collection_path: str = "."):
        """Save argument specs for all processed roles"""
        # In collection mode, we process roles individually and save immediately
        # This method is called to save any remaining specs
        for role_name in self.processed_roles:
            role_path = Path(collection_path) / "roles" / role_name
            self.save_role_specs(str(role_path), role_name)

    def interactive_mode(self):
        """Interactive mode to create argument specs"""
        print("=== Ansible Argument Specs Generator ===")
        print("Interactive mode - Press Ctrl+C to exit\n")

        try:
            # Get entry point information
            entry_name = input("Entry point name (default: main): ").strip() or "main"
            short_desc = input("Short description: ").strip()

            print("\nLong description (press Enter on empty line to finish):")
            description = []
            while True:
                line = input()
                if not line:
                    break
                description.append(line)

            print("\nAuthor(s) (press Enter on empty line to finish):")
            authors = []
            while True:
                line = input().strip()
                if not line:
                    break
                authors.append(line)

            entry_point = EntryPointSpec(
                name=entry_name,
                short_description=short_desc,
                description=description,
                author=authors,
            )

            # Add arguments
            print(f"\n=== Adding arguments for '{entry_name}' entry point ===")
            while True:
                print("\nAdd new argument? (y/n): ", end="")
                if input().lower() != "y":
                    break

                arg_spec = self._get_argument_interactive()
                if arg_spec:
                    entry_point.options[arg_spec.name] = arg_spec

            # Add conditional requirements
            self._get_conditionals_interactive(entry_point)

            self.add_entry_point(entry_point)

        except KeyboardInterrupt:
            print("\nExiting...")
            sys.exit(0)

    def _get_argument_interactive(self) -> Optional[ArgumentSpec]:
        """Get argument specification interactively"""
        try:
            name = input("Argument name: ").strip()
            if not name:
                return None

            print(f"Available types: {', '.join([t.value for t in ArgumentType])}")
            arg_type = input("Type (default: str): ").strip() or "str"

            description = input("Description: ").strip()

            required = input("Required? (y/n, default: n): ").lower().startswith("y")

            default_val = input("Default value (press Enter for none): ").strip()
            if default_val:
                # Try to parse as appropriate type
                if arg_type == "bool":
                    default_val = default_val.lower() in ["true", "yes", "1", "on"]
                elif arg_type == "int":
                    default_val = int(default_val)
                elif arg_type == "float":
                    default_val = float(default_val)
                elif arg_type in ["list", "dict"]:
                    try:
                        default_val = json.loads(default_val)
                    except json.JSONDecodeError:
                        print("Warning: Could not parse as JSON, using as string")
            else:
                default_val = None

            choices = None
            if input("Has choices? (y/n): ").lower().startswith("y"):
                choices_str = input("Choices (comma-separated): ")
                choices = [c.strip() for c in choices_str.split(",") if c.strip()]

            elements = None
            if arg_type in ["list", "dict"]:
                elements = (
                    input(f"Element type for {arg_type} (default: str): ").strip()
                    or "str"
                )

            version_added = None
            version_input = input("Version added (press Enter for none): ").strip()
            if version_input:
                version_added = version_input

            return ArgumentSpec(
                name=name,
                type=arg_type,
                required=required,
                default=default_val,
                choices=choices,
                description=description,
                elements=elements,
                version_added=version_added,
            )

        except (ValueError, KeyboardInterrupt) as e:
            print(f"Error: {e}")
            return None

    def _get_conditionals_interactive(self, entry_point: EntryPointSpec):
        """Get conditional requirements interactively"""
        print("\n=== Conditional Requirements ===")

        # required_if
        if input("Add required_if conditions? (y/n): ").lower().startswith("y"):
            entry_point.required_if = []
            while True:
                condition = input("Condition (param,value,required_params): ").strip()
                if not condition:
                    break
                try:
                    parts = [p.strip() for p in condition.split(",")]
                    if len(parts) >= 3:
                        param = parts[0]
                        value = parts[1]
                        required = parts[2:]
                        entry_point.required_if.append([param, value, required])
                except Exception as e:
                    print(f"Error parsing condition: {e}")

        # required_one_of
        if input("Add required_one_of groups? (y/n): ").lower().startswith("y"):
            entry_point.required_one_of = []
            while True:
                group = input("Group (comma-separated params): ").strip()
                if not group:
                    break
                params = [p.strip() for p in group.split(",")]
                entry_point.required_one_of.append(params)

        # mutually_exclusive
        if input("Add mutually_exclusive groups? (y/n): ").lower().startswith("y"):
            entry_point.mutually_exclusive = []
            while True:
                group = input("Group (comma-separated params): ").strip()
                if not group:
                    break
                params = [p.strip() for p in group.split(",")]
                entry_point.mutually_exclusive.append(params)

        # required_together
        if input("Add required_together groups? (y/n): ").lower().startswith("y"):
            entry_point.required_together = []
            while True:
                group = input("Group (comma-separated params): ").strip()
                if not group:
                    break
                params = [p.strip() for p in group.split(",")]
                entry_point.required_together.append(params)

    def from_defaults_file(self, defaults_file: str, entry_name: str = "main"):
        """Generate specs from a defaults/main.yml file"""
        if not os.path.exists(defaults_file):
            print(f"Error: Defaults file not found: {defaults_file}")
            sys.exit(1)

        try:
            with open(defaults_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if not content:
                    print(f"Warning: Defaults file is empty: {defaults_file}")
                    defaults = {}
                else:
                    defaults = yaml.safe_load(content)
                    if not isinstance(defaults, dict):
                        print(
                            f"Warning: Defaults file does not contain a dictionary: {defaults_file}"
                        )
                        defaults = {}

            entry_point = EntryPointSpec(
                name=entry_name,
                short_description=f"Auto-generated from {defaults_file}",
            )

            for var_name, var_value in defaults.items():
                arg_spec = self._infer_argument_spec(
                    var_name, var_value, None, None, False, "1.0.0"
                )
                entry_point.options[var_name] = arg_spec

            self.add_entry_point(entry_point)
            print(
                f"Generated specs for {len(entry_point.options)} variables from {defaults_file}"
            )

        except yaml.YAMLError as e:
            print(f"Error: Invalid YAML in defaults file {defaults_file}: {e}")
            sys.exit(1)
        except (OSError, IOError) as e:
            print(f"Error: Could not read defaults file {defaults_file}: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"Error reading defaults file: {e}")
            sys.exit(1)

    def _infer_argument_spec(
        self,
        name: str,
        value: Any,
        existing_description: str = None,
        existing_version_added: str = None,
        is_existing: bool = False,
        current_version: str = "1.0.0",
    ) -> ArgumentSpec:
        """Infer argument specification from a default value, preserving existing description and version info"""
        elements = None
        choices = None

        # Enhanced type inference with more intelligent detection
        if isinstance(value, bool):
            arg_type = "bool"
        elif isinstance(value, int):
            arg_type = "int"
        elif isinstance(value, float):
            arg_type = "float"
        elif isinstance(value, list):
            arg_type = "list"
            # Enhanced element type inference
            elements = self._infer_list_element_type(value)
        elif isinstance(value, dict):
            arg_type = "dict"
        elif isinstance(value, str):
            # Enhanced string type inference
            arg_type = self._infer_string_type(name, value)
        else:
            arg_type = "str"

        # Use existing description if provided, otherwise generate new one
        if existing_description:
            description = existing_description
            self.log_trace(f"Using existing description for {name}")
        else:
            # Enhanced description generation
            description = self._generate_smart_description(name, value, arg_type)

        # Determine version_added based on whether this is a new or existing variable
        version_added = None
        if existing_version_added:
            # Use existing version_added if it exists
            version_added = existing_version_added
            self.log_trace(f"Using existing version_added for {name}: {version_added}")
        elif is_existing:
            # Variable existed in specs but had no version_added - don't add one
            self.log_trace(
                f"Variable {name} existed in argument specs - not adding version_added"
            )
        else:
            # This is a truly new variable, add current version
            version_added = current_version
            self.log_trace(
                f"Adding version_added for new variable {name}: {version_added}"
            )

        return ArgumentSpec(
            name=name,
            type=arg_type,
            default=value,
            description=description,
            elements=elements,
            choices=choices,
            version_added=version_added,
        )

    def _infer_list_element_type(self, value: list) -> str:
        """Infer the element type for a list"""
        if not value:
            return "str"  # Default for empty lists

        # Check all elements to find the most common type
        type_counts = {}
        for elem in value:
            if isinstance(elem, dict):
                elem_type = "dict"
            elif isinstance(elem, int):
                elem_type = "int"
            elif isinstance(elem, bool):
                elem_type = "bool"
            elif isinstance(elem, float):
                elem_type = "float"
            else:
                elem_type = "str"

            type_counts[elem_type] = type_counts.get(elem_type, 0) + 1

        # Return the most common type
        return max(type_counts.items(), key=lambda x: x[1])[0]

    def _infer_string_type(self, name: str, value: str) -> str:
        """Infer more specific types for string values based on name and content"""
        name_lower = name.lower()

        # Path detection
        if any(
            keyword in name_lower
            for keyword in ["path", "dir", "directory", "file", "location"]
        ):
            if (
                value.startswith("/")
                or "\\" in value
                or any(
                    path_part in value
                    for path_part in ["home", "tmp", "var", "etc", "usr"]
                )
            ):
                return "path"

        # URL detection
        if any(
            keyword in name_lower for keyword in ["url", "uri", "endpoint", "host"]
        ) or value.startswith(("http://", "https://", "ftp://")):
            return "str"  # Could be 'url' if that was a supported type

        # State-like variables
        if name_lower in ["state", "status", "mode", "action"] and value in [
            "present",
            "absent",
            "enabled",
            "disabled",
            "started",
            "stopped",
        ]:
            return "str"  # These often have choices

        return "str"

    def _generate_smart_description(self, name: str, value: Any, arg_type: str) -> str:
        """Generate intelligent descriptions based on variable name, value, and usage context"""
        name_lower = name.lower()

        # First check if we have context information from task analysis
        if name in self.variable_context:
            context_info = self.variable_context[name]
            # Use the most relevant context (prefer specific modules over generic)
            best_context = None
            for context_key, context in context_info.items():
                if not best_context or "unknown" not in context_key:
                    best_context = context

            if best_context and "context" in best_context:
                context_desc = best_context["context"]
                module_info = f" (used in {best_context.get('module', 'task')})"
                base_desc = f"{context_desc}{module_info}"
                return self._format_description_by_type(base_desc, value, arg_type)

        # Enhanced naming patterns with more specific descriptions
        description_patterns = {
            # Authentication & Security
            "enable": "Enable or disable functionality",
            "disable": "Disable functionality",
            "auth": "Authentication configuration",
            "token": "Authentication token",
            "key": "Authentication or encryption key",
            "secret": "Secret value for authentication",
            "cert": "SSL/TLS certificate",
            "ssl": "SSL/TLS configuration",
            "tls": "TLS configuration",
            "password": "Password for authentication",
            "user": "Username for authentication",
            "admin": "Administrator user or settings",
            # Network & Connectivity
            "port": "Port number for network connection",
            "host": "Hostname or IP address",
            "address": "Network address",
            "url": "URL or web address",
            "endpoint": "API endpoint URL",
            "proxy": "Proxy server configuration",
            "dns": "DNS server or configuration",
            "interface": "Network interface",
            "bind": "Binding address or interface",
            # File System & Paths
            "path": "File or directory path",
            "file": "File path",
            "dir": "Directory path",
            "directory": "Directory path",
            "folder": "Directory path",
            "location": "File or directory location",
            "home": "Home directory path",
            "root": "Root directory path",
            "base": "Base directory path",
            "log": "Log file path or configuration",
            "backup": "Backup file or directory path",
            "archive": "Archive file path",
            "temp": "Temporary directory path",
            "cache": "Cache directory path",
            # System & Process Control
            "service": "Service name or configuration",
            "daemon": "Daemon process configuration",
            "process": "Process configuration",
            "pid": "Process ID or PID file path",
            "uid": "User ID",
            "gid": "Group ID",
            "owner": "File or resource owner",
            "group": "Group name or ID",
            "mode": "File permissions or operating mode",
            "permission": "Access permissions",
            # Configuration & Settings
            "config": "Configuration settings",
            "setting": "Configuration setting",
            "option": "Configuration option",
            "param": "Parameter value",
            "variable": "Variable value",
            "value": "Configuration value",
            "default": "Default value",
            "override": "Override value",
            # State & Control
            "state": "Desired state of the resource",
            "status": "Current status",
            "action": "Action to perform",
            "operation": "Operation to execute",
            "command": "Command to execute",
            "script": "Script path or content",
            "start": "Start the service or process",
            "stop": "Stop the service or process",
            "restart": "Restart the service or process",
            "reload": "Reload configuration",
            # Timing & Control Flow
            "timeout": "Timeout value in seconds",
            "retries": "Number of retry attempts",
            "delay": "Delay between operations in seconds",
            "interval": "Interval between operations",
            "frequency": "Frequency of execution",
            "schedule": "Schedule configuration",
            "cron": "Cron schedule expression",
            # Development & Debugging
            "debug": "Enable debug mode or output",
            "verbose": "Enable verbose output",
            "trace": "Enable trace logging",
            "force": "Force the operation even if it might cause issues",
            "check": "Perform check or validation",
            "validate": "Validate configuration or input",
            "test": "Test mode or configuration",
            "dry_run": "Perform dry run without making changes",
            # Package & Version Management
            "version": "Version specification",
            "package": "Package name or list",
            "repo": "Repository configuration",
            "repository": "Repository configuration",
            "branch": "Git branch name",
            "tag": "Version tag",
            "release": "Release version",
            # Data & Content
            "data": "Data content or configuration",
            "content": "File or resource content",
            "template": "Template file or content",
            "source": "Source file or URL",
            "destination": "Destination path",
            "target": "Target location or value",
            "output": "Output file or directory",
            "input": "Input file or data",
            # Identification
            "name": "Name identifier",
            "id": "Unique identifier",
            "uuid": "Unique identifier (UUID)",
            "label": "Label or tag",
            "tag": "Tag or label",
            "description": "Description text",
        }

        # Check for exact matches first
        if name_lower in description_patterns:
            base_desc = description_patterns[name_lower]
            return self._format_description_by_type(base_desc, value, arg_type)

        # Check for pattern matches (partial)
        for pattern, desc in description_patterns.items():
            if pattern in name_lower:
                base_desc = desc
                return self._format_description_by_type(base_desc, value, arg_type)

        # Enhanced fallback descriptions with better context
        return self._generate_fallback_description(name, value, arg_type)

    def _format_description_by_type(
        self, base_desc: str, value: Any, arg_type: str
    ) -> str:
        """Format description based on value type and content"""
        if isinstance(value, bool):
            if value:
                return f"{base_desc} (enabled by default)"
            else:
                return f"{base_desc} (disabled by default)"
        elif isinstance(value, (int, float)):
            if arg_type in ["int", "float"]:
                return f"{base_desc} (default: {value})"
            else:
                return f"{base_desc}"
        elif isinstance(value, list):
            if len(value) == 0:
                return f"{base_desc} (list, empty by default)"
            elif len(value) == 1:
                return f"{base_desc} (list with default item)"
            else:
                return f"{base_desc} (list with {len(value)} default items)"
        elif isinstance(value, dict):
            if len(value) == 0:
                return f"{base_desc} (dictionary, empty by default)"
            else:
                return f"{base_desc} (dictionary with default configuration)"
        elif isinstance(value, str):
            if value == "":
                return f"{base_desc} (empty by default)"
            elif len(value) > 50:
                return f"{base_desc} (configured with default value)"
            else:
                return f"{base_desc} (default: '{value}')"
        else:
            return base_desc

    def _generate_fallback_description(
        self, name: str, value: Any, arg_type: str
    ) -> str:
        """Generate fallback descriptions with better context"""
        name_parts = name.lower().replace("-", "_").split("_")
        cleaned_name = name.replace("_", " ").replace("-", " ")

        # Analyze name structure for better descriptions
        if len(name_parts) > 1:
            # Multi-part variable names
            if name_parts[-1] in ["list", "items", "array"]:
                return f"List of {' '.join(name_parts[:-1])}"
            elif name_parts[-1] in ["config", "conf", "cfg"]:
                return f"Configuration settings for {' '.join(name_parts[:-1])}"
            elif name_parts[-1] in ["enabled", "enable"]:
                return f"Enable {' '.join(name_parts[:-1])} functionality"
            elif name_parts[-1] in ["disabled", "disable"]:
                return f"Disable {' '.join(name_parts[:-1])} functionality"
            elif name_parts[0] in ["is", "has", "should", "can"]:
                return f"Whether to {' '.join(name_parts[1:])}"

        # Type-specific fallback descriptions
        if arg_type == "bool":
            return f"Boolean flag to control {cleaned_name}"
        elif arg_type in ["int", "float"]:
            return f"Numeric value for {cleaned_name}"
        elif arg_type == "list":
            return f"List of {cleaned_name} items"
        elif arg_type == "dict":
            return f"Configuration dictionary for {cleaned_name}"
        elif arg_type == "path":
            return f"File system path for {cleaned_name}"
        else:
            return f"Configuration value for {cleaned_name}"

    def from_config_file(self, config_file: str):
        """Generate specs from a configuration file (JSON or YAML)"""
        if not os.path.exists(config_file):
            print(f"Error: Config file not found: {config_file}")
            sys.exit(1)

        try:
            with open(config_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if not content:
                    print(f"Error: Config file is empty: {config_file}")
                    sys.exit(1)

                if config_file.endswith(".json"):
                    config = json.loads(content)
                else:
                    config = yaml.safe_load(content)

            if not isinstance(config, dict):
                print(f"Error: Config file must contain a dictionary: {config_file}")
                sys.exit(1)

            if "entry_points" not in config:
                print(
                    f"Error: Config file must contain 'entry_points' section: {config_file}"
                )
                sys.exit(1)

            for entry_name, entry_config in config.get("entry_points", {}).items():
                if not isinstance(entry_config, dict):
                    print(
                        f"Warning: Skipping invalid entry point '{entry_name}' - must be a dictionary"
                    )
                    continue

                entry_point = EntryPointSpec(
                    name=entry_name,
                    short_description=entry_config.get("short_description", ""),
                    description=entry_config.get("description", []),
                    author=entry_config.get("author", []),
                )

                for arg_name, arg_config in entry_config.get("arguments", {}).items():
                    if not isinstance(arg_config, dict):
                        print(
                            f"Warning: Skipping invalid argument '{arg_name}' - must be a dictionary"
                        )
                        continue

                    arg_spec = ArgumentSpec(
                        name=arg_name,
                        type=arg_config.get("type", "str"),
                        required=arg_config.get("required", False),
                        default=arg_config.get("default"),
                        choices=arg_config.get("choices"),
                        description=arg_config.get("description"),
                        elements=arg_config.get("elements"),
                        version_added=arg_config.get("version_added"),
                    )
                    entry_point.options[arg_name] = arg_spec

                # Add conditionals
                entry_point.required_if = entry_config.get("required_if", [])
                entry_point.required_one_of = entry_config.get("required_one_of", [])
                entry_point.mutually_exclusive = entry_config.get(
                    "mutually_exclusive", []
                )
                entry_point.required_together = entry_config.get(
                    "required_together", []
                )

                self.add_entry_point(entry_point)

            print(f"Generated specs from config file: {config_file}")

        except json.JSONDecodeError as e:
            print(f"Error: Invalid JSON in config file {config_file}: {e}")
            sys.exit(1)
        except yaml.YAMLError as e:
            print(f"Error: Invalid YAML in config file {config_file}: {e}")
            sys.exit(1)
        except (OSError, IOError) as e:
            print(f"Error: Could not read config file {config_file}: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"Error reading config file: {e}")
            sys.exit(1)

    def validate_specs(self) -> bool:
        """Validate the generated argument specs"""
        valid = True

        for entry_name, entry_point in self.entry_points.items():
            self.log_info(f"Validating entry point: {entry_name}")

            # Check for required fields
            if not entry_point.short_description:
                self.log_verbose(f"No short_description for {entry_name}")

            # Validate argument types
            for arg_name, arg_spec in entry_point.options.items():
                if arg_spec.type not in [t.value for t in ArgumentType]:
                    self.log_error(
                        f"Invalid type '{arg_spec.type}' for argument '{arg_name}'"
                    )
                    valid = False

                # Check list/dict element types
                if arg_spec.type in ["list", "dict"] and not arg_spec.elements:
                    self.log_verbose(
                        f"No elements type specified for {arg_spec.type} argument '{arg_name}'"
                    )

            # Validate conditionals reference existing arguments
            all_args = set(entry_point.options.keys())

            for condition_type, conditions in [
                ("required_if", entry_point.required_if or []),
                ("required_one_of", entry_point.required_one_of or []),
                ("mutually_exclusive", entry_point.mutually_exclusive or []),
                ("required_together", entry_point.required_together or []),
            ]:
                for condition in conditions:
                    if condition_type == "required_if":
                        # Format: [param, value, [required_params]]
                        if len(condition) >= 3:
                            param = condition[0]
                            required_params = (
                                condition[2]
                                if isinstance(condition[2], list)
                                else [condition[2]]
                            )

                            if param not in all_args:
                                self.log_error(
                                    f"{condition_type} references unknown argument '{param}'"
                                )
                                valid = False

                            for req_param in required_params:
                                if req_param not in all_args:
                                    self.log_error(
                                        f"{condition_type} references unknown argument '{req_param}'"
                                    )
                                    valid = False
                    else:
                        # Format: [param1, param2, ...]
                        for param in condition:
                            if param not in all_args:
                                self.log_error(
                                    f"{condition_type} references unknown argument '{param}'"
                                )
                                valid = False

        return valid

    def generate_yaml(self) -> str:
        """Generate the YAML content for argument_specs.yml"""
        if not self.entry_points:
            raise ValueError("No entry points defined")

        specs = {
            "argument_specs": {
                name: entry_point.to_dict()
                for name, entry_point in self.entry_points.items()
            }
        }

        # Custom YAML dumper for better formatting with proper sequence indentation
        class CustomDumper(yaml.SafeDumper):
            def write_line_break(self, data=None):
                super().write_line_break(data)
                if len(self.indents) == 1:
                    super().write_line_break()

            def increase_indent(self, flow=False, indentless=False):
                return super().increase_indent(flow, False)

            def ignore_aliases(self, data):
                # Disable YAML references/anchors (like *id001) for cleaner output
                return True

        # Configure YAML output for better readability
        yaml_content = yaml.dump(
            specs,
            Dumper=CustomDumper,
            default_flow_style=False,
            sort_keys=False,
            indent=2,
            width=120,  # Prevent overly long lines
            allow_unicode=True,
        )

        # Add YAML document markers and ensure proper formatting
        return f"---\n{yaml_content}...\n"

    def save_to_file(self, output_file: str):
        """Save the generated specs to a file"""
        try:
            yaml_content = self.generate_yaml()

            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(output_file), exist_ok=True)

            with open(output_file, "w") as f:
                f.write(yaml_content)

            print(f"Argument specs saved to: {output_file}")

        except Exception as e:
            print(f"Error saving file: {e}")
            sys.exit(1)


def create_example_config():
    """Create an example configuration file"""
    example_config = {
        "entry_points": {
            "main": {
                "short_description": "Main entry point for the role",
                "description": [
                    "This is the main entry point that combines all functionality.",
                    "It handles the primary workflow of the role.",
                ],
                "author": [
                    "Your Name <your.email@example.com>",
                    "Another Author <author@example.com>",
                ],
                "arguments": {
                    "state": {
                        "type": "str",
                        "required": False,
                        "default": "present",
                        "choices": ["present", "absent"],
                        "description": "Desired state of the resource",
                    },
                    "name": {
                        "type": "str",
                        "required": True,
                        "description": "Name of the resource to manage",
                    },
                    "config": {
                        "type": "dict",
                        "required": False,
                        "description": "Configuration dictionary for the resource",
                        "version_added": "1.1.0",
                    },
                    "items": {
                        "type": "list",
                        "elements": "str",
                        "default": [],
                        "description": "List of items to process",
                        "version_added": "1.2.0",
                    },
                },
                "mutually_exclusive": [["config", "items"]],
                "required_if": [["state", "present", ["name"]]],
            }
        }
    }

    yaml_content = yaml.dump(
        example_config, default_flow_style=False, sort_keys=False, indent=2
    )

    # Remove any YAML anchor/reference lines that may have slipped through
    lines = yaml_content.split("\n")
    cleaned_lines = []
    for line in lines:
        # Skip lines that are just YAML anchors (&id001) or references (*id001)
        stripped = line.strip()
        if not (stripped.startswith("&") and len(stripped.split()) == 1) and not (
            stripped.startswith("*") and len(stripped.split()) == 1
        ):
            cleaned_lines.append(line)

    return "\n".join(cleaned_lines)


def main():
    parser = argparse.ArgumentParser(
        description="Generate Ansible argument_specs.yml files for collections or individual roles",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Collection mode (default) - process all roles in collection
  python generate_argument_specs.py

  # Single role mode - interactive mode for one role
  python generate_argument_specs.py --single-role

  # Single role mode - generate from defaults file
  python generate_argument_specs.py --single-role --from-defaults defaults/main.yml

  # Collection mode - specify collection path
  python generate_argument_specs.py --collection-path /path/to/collection

  # Generate from config file (single role)
  python generate_argument_specs.py --single-role --from-config config.yml

  # Create example config
  python generate_argument_specs.py --create-example-config

  # Use verbosity for debugging
  python generate_argument_specs.py -vv  # Show variable processing details
        """,
    )

    # Mode selection
    parser.add_argument(
        "--single-role",
        action="store_true",
        help="Run in single role mode (default: collection mode)",
    )

    parser.add_argument(
        "--collection-path",
        default=".",
        help="Path to collection root (default: current directory)",
    )

    # Input sources
    parser.add_argument(
        "--from-defaults",
        metavar="FILE",
        help="Generate specs from a defaults/main.yml file (single role mode only)",
    )

    parser.add_argument(
        "--from-config",
        metavar="FILE",
        help="Generate specs from a configuration file (single role mode only)",
    )

    parser.add_argument(
        "--entry-point",
        default="main",
        help="Entry point name when using --from-defaults (default: main)",
    )

    # Output options
    parser.add_argument(
        "--output",
        "-o",
        help="Output file path (default: meta/argument_specs.yml for single role, auto for collection)",
    )

    # Special operations
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="Only validate existing specs, don't generate new ones",
    )

    parser.add_argument(
        "--create-example-config",
        action="store_true",
        help="Create an example configuration file",
    )

    parser.add_argument(
        "--list-roles",
        action="store_true",
        help="List roles found in collection and exit",
    )

    parser.add_argument(
        "--role", help="Process only the specified role in collection mode"
    )

    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help="""Increase verbosity levels:
  (none): Only show final summary
  -v: Show basic processing info for each role  
  -vv: Show detailed processing information
  -vvv: Show full trace and debug information""",
    )

    args = parser.parse_args()

    if args.create_example_config:
        example_content = create_example_config()
        with open("example_config.yml", "w") as f:
            f.write(example_content)
        print("Example configuration saved to: example_config.yml")
        return

    # Determine mode
    collection_mode = not args.single_role

    # Validate mode-specific arguments
    if not collection_mode:
        if args.list_roles:
            parser.error("--list-roles is only available in collection mode")
        if args.role:
            parser.error("--role is only available in collection mode")
    else:
        if args.from_defaults:
            parser.error("--from-defaults is only available in single role mode")
        if args.from_config:
            parser.error("--from-config is only available in single role mode")

    generator = ArgumentSpecsGenerator(
        collection_mode=collection_mode, verbosity=args.verbose
    )

    if collection_mode:
        # Collection mode
        if args.list_roles:
            if generator.is_collection_root(args.collection_path):
                roles = generator.find_roles(args.collection_path)
                if roles:
                    print(f"Found {len(roles)} roles in collection:")
                    for role in roles:
                        print(f"  - {role}")
                else:
                    print("No roles found in collection")
            else:
                print(f"Error: {args.collection_path} is not a collection root")
                sys.exit(1)
            return

        if args.validate_only:
            # Validate all existing specs in collection
            roles = generator.find_roles(args.collection_path)
            for role_name in roles:
                role_path = Path(args.collection_path) / "roles" / role_name
                specs_file = role_path / "meta" / "argument_specs.yml"
                if specs_file.exists():
                    print(f"Validating {role_name}...")
                    # Load and validate each role's specs
                    try:
                        with open(specs_file, "r") as f:
                            existing_specs = yaml.safe_load(f)

                        generator.entry_points.clear()
                        for entry_name, entry_data in existing_specs.get(
                            "argument_specs", {}
                        ).items():
                            entry_point = EntryPointSpec(name=entry_name)
                            entry_point.short_description = entry_data.get(
                                "short_description", ""
                            )
                            entry_point.description = entry_data.get("description", [])

                            for arg_name, arg_data in entry_data.get(
                                "options", {}
                            ).items():
                                arg_spec = ArgumentSpec(
                                    name=arg_name,
                                    type=arg_data.get("type", "str"),
                                    required=arg_data.get("required", False),
                                    default=arg_data.get("default"),
                                    choices=arg_data.get("choices"),
                                    description=arg_data.get("description"),
                                    elements=arg_data.get("elements"),
                                )
                                entry_point.options[arg_name] = arg_spec

                            generator.add_entry_point(entry_point)

                        if not generator.validate_specs():
                            print(f" Validation failed for {role_name}")
                            sys.exit(1)
                        else:
                            print(f" {role_name} specs are valid")

                    except Exception as e:
                        print(f"Error validating {role_name}: {e}")
                        sys.exit(1)

            print(" All collection specs are valid")
            return

        # Process collection
        if args.role:
            # Process only specified role
            role_path = Path(args.collection_path) / "roles" / args.role
            if not role_path.exists():
                print(f"Error: Role '{args.role}' not found in collection")
                sys.exit(1)

            generator.process_single_role(str(role_path), args.role)
            generator.processed_roles.append(args.role)
        else:
            # Process all roles
            generator.process_collection(args.collection_path)

        print(f"\n Successfully processed {len(generator.processed_roles)} role(s)")

    else:
        # Single role mode
        if args.validate_only:
            output_file = args.output or "meta/argument_specs.yml"
            if os.path.exists(output_file):
                try:
                    with open(output_file, "r") as f:
                        existing_specs = yaml.safe_load(f)

                    for entry_name, entry_data in existing_specs.get(
                        "argument_specs", {}
                    ).items():
                        entry_point = EntryPointSpec(name=entry_name)
                        entry_point.short_description = entry_data.get(
                            "short_description", ""
                        )
                        entry_point.description = entry_data.get("description", [])

                        for arg_name, arg_data in entry_data.get("options", {}).items():
                            arg_spec = ArgumentSpec(
                                name=arg_name,
                                type=arg_data.get("type", "str"),
                                required=arg_data.get("required", False),
                                default=arg_data.get("default"),
                                choices=arg_data.get("choices"),
                                description=arg_data.get("description"),
                                elements=arg_data.get("elements"),
                            )
                            entry_point.options[arg_name] = arg_spec

                        generator.add_entry_point(entry_point)

                except Exception as e:
                    print(f"Error loading existing specs: {e}")
                    sys.exit(1)
            else:
                print(f"No existing specs found at {output_file}")
                sys.exit(1)
        else:
            # Generate new specs
            if args.from_defaults:
                generator.from_defaults_file(args.from_defaults, args.entry_point)
            elif args.from_config:
                generator.from_config_file(args.from_config)
            else:
                generator.interactive_mode()

        # Validate and save
        if generator.entry_points:
            if generator.validate_specs():
                print(" All specs are valid")

                if not args.validate_only:
                    output_file = args.output or "meta/argument_specs.yml"
                    generator.save_to_file(output_file)
                    print(
                        f"\nGenerated argument_specs.yml with {len(generator.entry_points)} entry point(s)"
                    )
            else:
                print(" Validation failed")
                sys.exit(1)
        else:
            print("No entry points generated")


if __name__ == "__main__":
    main()
