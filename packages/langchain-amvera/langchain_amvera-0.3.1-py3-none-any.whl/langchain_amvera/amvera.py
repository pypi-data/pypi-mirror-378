"""Amvera LLM integration for LangChain."""

from __future__ import annotations

import json
import logging
from typing import Any, Dict, List, Optional, Sequence

import httpx
from dotenv import load_dotenv
from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    ChatMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.tools import BaseTool
from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
from pydantic import ConfigDict, Field, SecretStr, model_validator

load_dotenv()

logger = logging.getLogger(__name__)


class AmveraLLM(BaseChatModel):
    """
    Amvera LLM –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ llama8b, llama70b, gpt-4.1 –∏ gpt-5.

    Example:
        .. code-block:: python

            from langchain_amvera import AmveraLLM

            llm = AmveraLLM(
                model="llama70b",  # –∏–ª–∏ "gpt-4.1", "gpt-5"
                temperature=0.7,
                max_tokens=1000,
                api_token="your-token"  # –∏–ª–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é AMVERA_API_TOKEN
            )

            # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            response = llm.invoke("–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç")
            print(response.content)

            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            response = await llm.ainvoke("–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç")
            print(response.content)
    """

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        populate_by_name=True,
    )

    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    api_token: SecretStr = Field(alias="amvera_api_token")
    """API —Ç–æ–∫–µ–Ω –¥–ª—è Amvera."""

    model: str = Field(default="llama70b")
    """ID –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –î–æ—Å—Ç—É–ø–Ω–æ: llama8b, llama70b, gpt-4.1, gpt-5"""

    base_url: str = Field(default="https://kong-proxy.yc.amvera.ru/api/v1")
    """–ë–∞–∑–æ–≤—ã–π URL –¥–ª—è Amvera API"""

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    """–ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –æ—Ç 0 –¥–æ 2"""

    max_tokens: Optional[int] = Field(default=None, ge=1)
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""

    timeout: int = Field(default=60, ge=1)
    """–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö"""

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    json_mode: bool = Field(default=False)
    """–í–∫–ª—é—á–∏—Ç—å JSON —Ä–µ–∂–∏–º –æ—Ç–≤–µ—Ç–∞"""

    json_schema: Optional[Dict[str, Any]] = Field(default=None)
    """JSON —Å—Ö–µ–º–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –≤ JSON —Ä–µ–∂–∏–º–µ"""


    verbose: bool = Field(default=False)
    """–í–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""

    # –ü—Ä–∏–≤–∞—Ç–Ω–æ–µ –ø–æ–ª–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å bind_tools)
    bound_tools: Optional[List[Dict[str, Any]]] = Field(default=None, exclude=True)

    @model_validator(mode="before")
    @classmethod
    def validate_environment(cls, data: Any) -> Any:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è."""
        if isinstance(data, dict):
            api_token = get_from_dict_or_env(
                data, "api_token", "AMVERA_API_TOKEN", default=""
            )
            if api_token:
                data["api_token"] = convert_to_secret_str(api_token)
            else:
                raise ValueError(
                    "API —Ç–æ–∫–µ–Ω Amvera –Ω–µ –Ω–∞–π–¥–µ–Ω. "
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è AMVERA_API_TOKEN "
                    "–∏–ª–∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ api_token –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏."
                )
        return data

    def __init__(self, **kwargs: Any):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Amvera."""
        super().__init__(**kwargs)
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç—ã –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–ª–∞—Å—Å–∞
        self._setup_clients()

    def _setup_clients(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP –∫–ª–∏–µ–Ω—Ç–æ–≤."""
        headers = {
            "accept": "application/json",
            "Content-Type": "application/json",
            "X-Auth-Token": f"Bearer {self.api_token.get_secret_value()}",
        }

        # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
        self._sync_client = httpx.Client(
            headers=headers, timeout=self.timeout, base_url=self.base_url
        )

        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
        self._async_client = None

    def _get_async_client(self) -> httpx.AsyncClient:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞."""
        if self._async_client is None:
            headers = {
                "accept": "application/json",
                "Content-Type": "application/json",
                "X-Auth-Token": f"Bearer {self.api_token.get_secret_value()}",
            }

            self._async_client = httpx.AsyncClient(
                headers=headers, timeout=self.timeout, base_url=self.base_url
            )
        return self._async_client

    @property
    def _llm_type(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏."""
        return "amvera"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏."""
        return {
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "base_url": self.base_url,
        }

    def _convert_messages_to_amvera_format(
        self, messages: List[BaseMessage]
    ) -> List[Dict[str, Any]]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π LangChain –≤ —Ñ–æ—Ä–º–∞—Ç Amvera API."""
        amvera_messages = []

        for message in messages:
            if isinstance(message, HumanMessage):
                role = "user"
            elif isinstance(message, AIMessage):
                role = "assistant"
            elif isinstance(message, SystemMessage):
                role = "system"
            elif isinstance(message, ToolMessage):
                role = "user"  # –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            elif isinstance(message, ChatMessage):
                role = message.role
            else:
                logger.warning(
                    f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å–æ–æ–±—â–µ–Ω–∏—è {type(message)}, –∏—Å–ø–æ–ª—å–∑—É–µ–º 'user'"
                )
                role = "user"

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º content –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
            content = message.content
            if content is None:
                content = ""
            elif not isinstance(content, str):
                content = str(content)

            # –û—Å–æ–±–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è ToolMessage
            if isinstance(message, ToolMessage):
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
                content = f"–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {content}"

            amvera_message: Dict[str, Any] = {
                "role": role,
                "text": content,
            }

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ tool calls –¥–ª—è AI —Å–æ–æ–±—â–µ–Ω–∏–π
            if isinstance(message, AIMessage) and hasattr(message, "tool_calls"):
                if message.tool_calls:
                    amvera_message["toolCallList"] = {
                        "toolCalls": [
                            {
                                "functionCall": {
                                    "name": tool_call.get("name", ""),
                                    "arguments": tool_call.get("args", {}),
                                }
                            }
                            for tool_call in message.tool_calls
                        ]
                    }

            amvera_messages.append(amvera_message)

        return amvera_messages

    def _create_payload(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ payload –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ API."""
        payload: Dict[str, Any] = {
            "model": self.model,
            "messages": self._convert_messages_to_amvera_format(messages),
        }

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        if self.temperature is not None:
            # GPT –º–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —Ç–æ–ª—å–∫–æ temperature = 1 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            if not self.model.startswith("gpt"):
                payload["temperature"] = self.temperature
        if self.max_tokens is not None:
            # GPT –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç max_completion_tokens –≤–º–µ—Å—Ç–æ max_tokens
            if self.model.startswith("gpt"):
                payload["max_completion_tokens"] = self.max_tokens
            else:
                payload["max_tokens"] = self.max_tokens
        if stop:
            payload["stop"] = stop

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
        if self.bound_tools:
            payload["tools"] = self.bound_tools
        if self.json_mode:
            payload["jsonObject"] = True
            if self.json_schema:
                payload["jsonSchema"] = {"schema": self.json_schema}

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ kwargs
        payload.update(kwargs)
        return payload

    def _parse_response(
        self, response_data: Dict[str, Any]
    ) -> tuple[str, Dict[str, Any], Optional[List[Dict[str, Any]]]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –æ—Ç Amvera API."""
        content = ""
        generation_info = {}
        tool_calls = None

        # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ Amvera (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏ –¥–ª—è llama –∏ –¥–ª—è gpt)
        if "result" in response_data and "alternatives" in response_data["result"]:
            alternatives = response_data["result"]["alternatives"]

            if alternatives:
                # –ò—â–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É
                for alt in alternatives:
                    if alt.get("status") == "ALTERNATIVE_STATUS_FINAL":
                        message = alt.get("message", {})
                        content = message.get("text", "")

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º tool calls –µ—Å–ª–∏ –µ—Å—Ç—å
                        if "toolCallList" in message and "toolCalls" in message["toolCallList"]:
                            tool_calls = []
                            for tool_call in message["toolCallList"]["toolCalls"]:
                                if "functionCall" in tool_call:
                                    func_call = tool_call["functionCall"]
                                    tool_calls.append({
                                        "name": func_call.get("name", ""),
                                        "args": func_call.get("arguments", {}),
                                        "id": tool_call.get("id", f"call_{len(tool_calls)}")
                                    })
                        break

                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω—É—é, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é
                if not content and alternatives:
                    message = alternatives[0].get("message", {})
                    content = message.get("text", "")

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º tool calls –µ—Å–ª–∏ –µ—Å—Ç—å
                    if "toolCallList" in message and "toolCalls" in message["toolCallList"]:
                        tool_calls = []
                        for tool_call in message["toolCallList"]["toolCalls"]:
                            if "functionCall" in tool_call:
                                func_call = tool_call["functionCall"]
                                tool_calls.append({
                                    "name": func_call.get("name", ""),
                                    "args": func_call.get("arguments", {}),
                                    "id": tool_call.get("id", f"call_{len(tool_calls)}")
                                })

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
            if "usage" in response_data["result"]:
                generation_info["usage"] = response_data["result"]["usage"]
            if "modelVersion" in response_data["result"]:
                generation_info["model_version"] = response_data["result"][
                    "modelVersion"
                ]

        # Fallback –¥–ª—è OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (–µ—Å–ª–∏ –≤–¥—Ä—É–≥ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è)
        elif "choices" in response_data and response_data["choices"]:
            choice = response_data["choices"][0]
            message = choice.get("message", {})
            content = message.get("content", "")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º tool_calls –≤ OpenAI —Ñ–æ—Ä–º–∞—Ç–µ
            if "tool_calls" in message:
                tool_calls = []
                for tool_call in message["tool_calls"]:
                    if tool_call.get("type") == "function" and "function" in tool_call:
                        func_call = tool_call["function"]
                        args = func_call.get("arguments", "{}")
                        if isinstance(args, str):
                            try:
                                args = json.loads(args)
                            except json.JSONDecodeError:
                                args = {}
                        tool_calls.append({
                            "name": func_call.get("name", ""),
                            "args": args,
                            "id": tool_call.get("id", f"call_{len(tool_calls)}")
                        })

        # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞  
        elif "response" in response_data:
            content = response_data["response"]
        else:
            content = str(response_data)

        return content, generation_info, tool_calls

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞."""
        payload = self._create_payload(messages, stop, **kwargs)

        if self.verbose:
            logger.info(
                f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Amvera API: {json.dumps(payload, ensure_ascii=False, indent=2)}"
            )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º endpoint –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
        endpoint = "/models/gpt" if self.model.startswith("gpt") else "/models/llama"

        try:
            response = self._sync_client.post(endpoint, json=payload)
            response.raise_for_status()
            response_data = response.json()

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 401:
                raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π API —Ç–æ–∫–µ–Ω Amvera") from e
            elif e.response.status_code == 429:
                raise ValueError("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Amvera API") from e
            else:
                raise ValueError(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}: {e}") from e
        except httpx.TimeoutException as e:
            raise ValueError("–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Amvera API") from e
        except httpx.RequestError as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Amvera API: {e}") from e
        except json.JSONDecodeError as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}") from e

        if self.verbose:
            logger.info(
                f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Amvera API: {json.dumps(response_data, ensure_ascii=False, indent=2)}"
            )

        content, generation_info, tool_calls = self._parse_response(response_data)

        message = AIMessage(
            content=content,
            response_metadata=generation_info,
            tool_calls=tool_calls or [],
        )

        generation = ChatGeneration(
            message=message,
            generation_info=generation_info,
        )

        return ChatResult(generations=[generation])

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞."""
        payload = self._create_payload(messages, stop, **kwargs)

        if self.verbose:
            logger.info(
                f"[ASYNC] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Amvera API: {json.dumps(payload, ensure_ascii=False, indent=2)}"
            )

        client = self._get_async_client()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º endpoint –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
        endpoint = "/models/gpt" if self.model.startswith("gpt") else "/models/llama"

        try:
            response = await client.post(endpoint, json=payload)
            response.raise_for_status()
            response_data = response.json()

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 401:
                raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π API —Ç–æ–∫–µ–Ω Amvera") from e
            elif e.response.status_code == 429:
                raise ValueError("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Amvera API") from e
            else:
                raise ValueError(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}: {e}") from e
        except httpx.TimeoutException as e:
            raise ValueError("–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Amvera API") from e
        except httpx.RequestError as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Amvera API: {e}") from e
        except json.JSONDecodeError as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}") from e

        if self.verbose:
            logger.info(
                f"[ASYNC] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Amvera API: {json.dumps(response_data, ensure_ascii=False, indent=2)}"
            )

        content, generation_info, tool_calls = self._parse_response(response_data)

        message = AIMessage(
            content=content,
            response_metadata=generation_info,
            tool_calls=tool_calls or [],
        )

        generation = ChatGeneration(
            message=message,
            generation_info=generation_info,
        )

        return ChatResult(generations=[generation])

    def get_token_usage(self, result: AIMessage) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ AIMessage."""
        if hasattr(result, "response_metadata") and result.response_metadata:
            return result.response_metadata.get("usage")
        return None

    def get_model_version(self, result: AIMessage) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ –∏–∑ AIMessage."""
        if hasattr(result, "response_metadata") and result.response_metadata:
            return result.response_metadata.get("model_version")
        return None

    @property
    def lc_secrets(self) -> Dict[str, str]:
        """–°–µ–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""
        return {"api_token": "AMVERA_API_TOKEN"}

    @classmethod
    def get_lc_namespace(cls) -> List[str]:
        """–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω –¥–ª—è LangChain."""
        return ["langchain", "chat_models", "amvera"]

    def bind_tools(self, tools: Sequence[BaseTool], **kwargs: Any) -> "AmveraLLM":
        """
        –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∫ –º–æ–¥–µ–ª–∏ –¥–ª—è Function Calling.

        Args:
            tools: –°–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ LangChain –¥–ª—è –ø—Ä–∏–≤—è–∑–∫–∏
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

        Returns:
            –ù–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä AmveraLLM —Å –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º LangChain tools –≤ —Ñ–æ—Ä–º–∞—Ç Amvera API
        amvera_tools = []
        for tool in tools:
            tool_def = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                }
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if hasattr(tool, 'args_schema') and tool.args_schema:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º JSON —Å—Ö–µ–º—É –∏–∑ pydantic –º–æ–¥–µ–ª–∏
                    schema = tool.args_schema.model_json_schema()
                    tool_def["function"]["parameters"] = schema
                except Exception:
                    # Fallback: –±–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞
                    tool_def["function"]["parameters"] = {
                        "type": "object",
                        "properties": {},
                        "required": []
                    }

            amvera_tools.append(tool_def)

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Å –Ω–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
        new_kwargs = self.dict()
        new_kwargs.update(kwargs)
        new_kwargs["bound_tools"] = amvera_tools

        return self.__class__(**new_kwargs)

    def __del__(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞."""
        try:
            if hasattr(self, "_sync_client") and self._sync_client:
                self._sync_client.close()
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏

    async def aclose(self) -> None:
        """–Ø–≤–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞."""
        if hasattr(self, "_async_client") and self._async_client:
            await self._async_client.aclose()
            self._async_client = None


def create_amvera_chat_model(
    model: str = "llama70b",
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    api_token: Optional[str] = None,
    **kwargs: Any,
) -> AmveraLLM:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Amvera chat model.

    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (llama8b, llama70b, gpt-4.1, gpt-5)
        temperature: –ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞ (0.0-2.0)
        max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        api_token: API —Ç–æ–∫–µ–Ω
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä AmveraLLM
    """
    params = {"model": model, "temperature": temperature, **kwargs}

    if max_tokens is not None:
        params["max_tokens"] = max_tokens
    if api_token is not None:
        params["api_token"] = api_token

    return AmveraLLM(**params)


# –ê–ª–∏–∞—Å—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
AmveraChatModel = AmveraLLM
ChatAmvera = AmveraLLM


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import asyncio

    from langchain_core.messages import HumanMessage, SystemMessage

    async def main():
        try:
            llm = create_amvera_chat_model(
                model="llama70b", temperature=0.7, verbose=True
            )

            print(f"–°–æ–∑–¥–∞–Ω {llm._llm_type} —Å –º–æ–¥–µ–ª—å—é {llm.model}")

            messages = [
                SystemMessage(content="–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫."),
                HumanMessage(content="–†–∞—Å—Å–∫–∞–∂–∏ –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–µ–∫–¥–æ—Ç –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤"),
            ]

            print("\nü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
            result = await llm.ainvoke(messages)

            print(f"\n‚úÖ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n{result.content}")

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö
            usage = llm.get_token_usage(result)
            if usage:
                print(f"\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: {usage}")

            version = llm.get_model_version(result)
            if version:
                print(f"üì¶ –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {version}")

            # –ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤
            await llm.aclose()

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()

    asyncio.run(main())

