#!/usr/bin/env python3
"""–ü—Ä–∏–º–µ—Ä –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è langchain-amvera."""

import asyncio
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_amvera import AmveraLLM


async def single_request_example():
    """–ü—Ä–∏–º–µ—Ä –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
    print("üìù –ü—Ä–∏–º–µ—Ä 1: –û–¥–∏–Ω–æ—á–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
    
    llm = AmveraLLM(
        model="llama70b",
        temperature=0.8,
        max_tokens=150
    )
    
    messages = [
        SystemMessage(content="–¢—ã –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –ø–∏—Å–∞—Ç–µ–ª—å."),
        HumanMessage(content="–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ —Ä–æ–±–æ—Ç–∞-—Å–∞–¥–æ–≤–Ω–∏–∫–∞")
    ]
    
    try:
        print("üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
        result = await llm.ainvoke(messages)
        
        print(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω:\n{result.content}")
        
        usage = llm.get_token_usage(result)
        if usage:
            print(f"üìä –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {usage.get('totalTokens', 'N/A')}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    finally:
        # –í–∞–∂–Ω–æ –∑–∞–∫—Ä—ã—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
        await llm.aclose()


async def multiple_requests_example():
    """–ü—Ä–∏–º–µ—Ä –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
    print("\nüìù –ü—Ä–∏–º–µ—Ä 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
    
    llm = AmveraLLM(
        model="llama8b",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å
        temperature=0.5,
        max_tokens=100
    )
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤
    questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
        "–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π",
        "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Ä–∞–∑–ª–∏—á–∏—è—Ö –º–µ–∂–¥—É –ò–ò –∏ ML"
    ]
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    tasks = []
    for i, question in enumerate(questions):
        messages = [
            SystemMessage(content="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ."),
            HumanMessage(content=question)
        ]
        task = llm.ainvoke(messages)
        tasks.append((i + 1, question, task))
    
    try:
        print("üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*[task for _, _, task in tasks])
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for (num, question, _), result in zip(tasks, results):
            print(f"\n‚ùì –í–æ–ø—Ä–æ—Å {num}: {question}")
            print(f"ü§ñ –û—Ç–≤–µ—Ç: {result.content}")
            
            usage = llm.get_token_usage(result)
            if usage:
                print(f"üìä –¢–æ–∫–µ–Ω–æ–≤: {usage.get('totalTokens', 'N/A')}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö: {e}")
    
    finally:
        await llm.aclose()


async def streaming_like_example():
    """–ü—Ä–∏–º–µ—Ä –∏–º–∏—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —Å –ø–æ–º–æ—â—å—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±—ã—Å—Ç—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
    print("\nüìù –ü—Ä–∏–º–µ—Ä 3: –ò–º–∏—Ç–∞—Ü–∏—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞")
    
    llm = AmveraLLM(
        model="llama8b",
        temperature=0.3,
        max_tokens=50  # –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–∞—Å—Ç–µ–π
    )
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —á–∞—Å—Ç–∏
    story_parts = [
        "–ù–∞—á–Ω–∏ –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∞",
        "–ü—Ä–æ–¥–æ–ª–∂–∏ —ç—Ç—É –∏—Å—Ç–æ—Ä–∏—é - —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –¥–∞–ª—å—à–µ?",
        "–ó–∞–≤–µ—Ä—à–∏ –∏—Å—Ç–æ—Ä–∏—é –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π –∫–æ–Ω—Ü–æ–≤–∫–æ–π"
    ]
    
    try:
        print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ —á–∞—Å—Ç—è–º...")
        story = ""
        
        for i, part in enumerate(story_parts, 1):
            messages = [
                SystemMessage(content="–¢—ã —Ä–∞—Å—Å–∫–∞–∑—á–∏–∫. –ü–∏—à–∏ —Å–≤—è–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –∏—Å—Ç–æ—Ä–∏–∏."),
                HumanMessage(content=f"{part}. –ü—Ä–µ–¥—ã–¥—É—â–∞—è —á–∞—Å—Ç—å: {story}")
            ]
            
            print(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–∞—Å—Ç–∏ {i}/3...")
            result = await llm.ainvoke(messages)
            
            new_part = result.content
            story += f" {new_part}"
            
            print(f"‚ú® –ß–∞—Å—Ç—å {i}: {new_part}")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            await asyncio.sleep(0.5)
        
        print(f"\nüìñ –ü–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è:\n{story.strip()}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
    
    finally:
        await llm.aclose()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏."""
    print("üîÑ –ü—Ä–∏–º–µ—Ä—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Amvera LLM\n")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    await single_request_example()
    await multiple_requests_example() 
    await streaming_like_example()
    
    print("\n‚úÖ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")


if __name__ == "__main__":
    asyncio.run(main())