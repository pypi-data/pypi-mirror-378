{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Imports necessary libraries\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import suite2p"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Figure Style settings for the notebook.\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"axes.spines.left\": False,\n",
    "        \"axes.spines.bottom\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"legend.frameon\": False,\n",
    "        \"figure.subplot.wspace\": 0.01,\n",
    "        \"figure.subplot.hspace\": 0.01,\n",
    "        \"figure.figsize\": (18, 13),\n",
    "        \"ytick.major.left\": False,\n",
    "    }\n",
    ")\n",
    "jet = mpl.colormaps.get_cmap(\"jet\")\n",
    "jet.set_bad(color=\"k\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the multi-day suite2p pipeline on example data\n",
    "\n",
    "This notebook will guide you through the various stages and outputs of the suite2p multi-day pipeline by running it on\n",
    "your own dataset. This is an advanced tutorial that depends on the familiarity with the\n",
    "single-day pipeline and requires at least two recording sessions processed with the\n",
    "single-day pipeline to work as expected. This demonstration is written using the data featured in the\n",
    "[OSM manuscript](https://www.nature.com/articles/s41586-024-08548-w).\n",
    "\n",
    "**Note!** Unlike the single-day suite2p pipeline, the multi-day pipeline is not found in the 'original' suite2p\n",
    "implementation. Instead, this pipeline has been developed by the authors of the OSM manuscript and\n",
    "the reference implementation is available [here](https://github.com/sprustonlab/multiday-suite2p-public)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the runtime parameters\n",
    "\n",
    "Similar to single-day runtime parameters, all parameters for the multi-day pipeline are stored in a\n",
    "MultiDayS2PConfiguration dataclass instance. To generate a class instance with default configuration parameters, use the\n",
    " `generate_default_multiday_ops` function. To see the descriptions of available multi-day configuration parameters, see\n",
    " our repository and API documentation [here](https://github.com/Sun-Lab-NBB/suite2p)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ops = suite2p.generate_default_multiday_ops(as_dict=False)\n",
    "print(ops)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding specific configuration parameters\n",
    "\n",
    "The interface for the multi-day parameters functions the same way as the interface for the single-day parameters, but\n",
    "uses different section names. Most multi-day parameters can be left at their default values, and shared between\n",
    "runtimes. The most important parameter to change for each runtime is the list of `session_fodlers` paths to process.\n",
    "\n",
    "**Note!** Each path in the input session folder list has to satisfy two conditions. First, it must point to a directory\n",
    "containing exactly one `combined` plane folder. The 'combined' folder can be stored in the directory pointed by the\n",
    "path or any of its subdirectories. Second, all input paths must have a unique path component, typically the session\n",
    "ID, that is not present in any other input path. In other words, all sessions must be stored in uniquely named folders.\n",
    "\n",
    "***Important!*** When running this demonstration on your dataset, adjust the parameters below to match your dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Provides the paths to the two sessions processed as part of the single-day demonstration notebook's runtime\n",
    "ops.io.session_directories = [\n",
    "    \"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/2022-02-02-17-16-40-000000\",\n",
    "    \"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/2022-02-03-17-23-54-000000\",\n",
    "]\n",
    "\n",
    "# Provides the path to the root directory under which to generate the output folder. This is the same as the\n",
    "# save_path0 parameter of the single-day pipeline.\n",
    "ops.io.multiday_save_path = \"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/\"\n",
    "\n",
    "# Defines the name of the output folder. This is the same as the save_folder parameter of the single-day pipeline.\n",
    "ops.io.multiday_save_folder = \"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/s2p_multiday\"\n",
    "\n",
    "# Since the processed data is a mesoscope recording, provides the x-coordinates of the ROI stripe borders.\n",
    "# This is an important parameter for mesoscope recordings that ensures ROIs close to striped borders are not processed\n",
    "# multiple times (one for each neighboring stripe).\n",
    "ops.cell_selection.mesoscope_stripe_borders = [462, 924]\n",
    "\n",
    "# This is the only single-day processing parameter that is also explicitly present in the multi-day configuration class.\n",
    "# During multi-day registration, this determines how many sessions are processed in parallel. During multi-day trace\n",
    "# extraction (roi processing), this determines how many CPU cores are used to process each session either sequentially\n",
    "# or in parallel.\n",
    "ops.main.parallel_workers = 10\n",
    "\n",
    "# Converts the configuration class to the dictionary format expected by downstream functions.\n",
    "ops = ops.to_ops()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding single-day pipeline parameters\n",
    "\n",
    "**Critical!** The multi-day pipeline acts as an extension of the single-day pipeline. It requires the data to be\n",
    "pre-processed with the single-day pipeline and **reuses some single-day configuration parameters stored in the\n",
    "ops.npy file generated as part of single-day processing**. Each multi-day runtime merges the parameters from the\n",
    "MultiDayS2PConfiguration instance with the parameters loaded from the single-day ops.npy file for each of the\n",
    "processed sessions.\n",
    "\n",
    "The MultiDayS2PConfiguration provides an explicit way for overriding most 'shared' parameters, such as those that\n",
    "control signal extraction and spike deconvolution processing steps. However, some 'implicit' parameters, such as the\n",
    "dimensions of each processed plane, are loaded from the single-day ops.npy file and are NOT modified as part of\n",
    "resolving the multi-day ops.npy file for each session.\n",
    "\n",
    "However, it is possible to override these parameters by specifying them either as part of the `ops` or, as demonstrated\n",
    "here, the `db` dictionary passed to the `resolve_multiday_ops` function (see the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The multi-day pipeline maintains a similar interface for generating the 'ops.npy' file as the single-day pipeline,\n",
    "# taking both 'ops' and 'db' dictionaries. We recommend using the 'db' dictionary to override single-day parameters\n",
    "# during multi-day runtimes.\n",
    "db = {\n",
    "    # While it is uncommon to run multiple multi-day pipelines in parallel via the same terminal, if you do, set\n",
    "    # this to 'False' to prevent the terminal from being overwhelmed with progress bar updates.\n",
    "    \"progress_bars\": True,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the ops.npy file\n",
    "To integrate the parameters from the `db` dictionary into the `ops` dictionary, use the `resolve_multiday_ops` function.\n",
    "This function works similar to the single-day one, but carries out some additional processing steps.\n",
    "\n",
    "First, it merges the multi-day configuration parameters with the single-day configuration parameters stored\n",
    "inside the single-day ops.npy configuration file of one of the sessions to be processed. Second, it generates the\n",
    "output data hierarchy using the 'multiday_save_path' and 'multiday_save_folder' values. In that hierarchy, for each\n",
    "processed session, it generates a single_day and a multi_day subfolder. Finally, it copies the output of the 'combined'\n",
    "folder for each processed session into the single_day output folder, so that single_day and multi_day outputs are always\n",
    "stored together. Note, however, that it does NOT copy the binary files.\n",
    "\n",
    "**Note!** The multi-day ops.npy file stored at the path returned by the `resolve_multiday_ops` is stored under\n",
    "the root output folder ('multiday_save_folder'). Like the single-day pipeline, the resolve_multiday_ops () function also\n",
    "generates a .yaml representation of the resolved multi-day configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Constructs the runtime configuration dictionary and dumps it as an .npy file\n",
    "ops_path = suite2p.multi_day.resolve_multiday_ops(ops=ops, db=db)\n",
    "print(ops_path)\n",
    "\n",
    "# Loads and visualizes the final state of the configuration dictionary:\n",
    "final_ops = np.load(file=ops_path, allow_pickle=True).item()\n",
    "\n",
    "# Notice that the dictionary contains both the multi-day and the single-day keys.\n",
    "print(final_ops.keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the multi-day suite2p pipeline\n",
    "The default way to run the multi-day pipeline is by calling the `run_s2p_multiday` function from the `multi_day` module.\n",
    "This function executes all steps of the multi-day processing pipeline in a sequence, using the processing parameters\n",
    "from the multi-day ops.npy file created via the `resolve_multiday_ops` function.\n",
    "\n",
    "**Note!** The function will use up to 'parallel_workers' CPU cores when processing each session and may require a\n",
    "significant amount of memory (RAM), depending on the input dataset size. However, compared to the single-day pipeline,\n",
    "the multi-day pipeline is overall less memory and CPU intensive.\n",
    "\n",
    "**Advanced users with access to powerful compute machines or distributed clusters should consult the sections at the\n",
    "end of this notebook to learn about running pipeline steps in parallel.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "suite2p.multi_day.run_s2p_multiday(ops_path=ops_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-day pipeline outputs\n",
    "This section goes over the outputs generated by the multi-day pipeline and briefly mentions how each can be used when\n",
    "working with your own datasets.\n",
    "\n",
    "**Note!** We highly recommend consulting the [OSM manuscript](https://www.nature.com/articles/s41586-024-08548-w)\n",
    "to understand the principles behind the multi-day processing pipeline and outputs discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration (images)\n",
    "The multi-day registration works similar to the single-day registration, but instead of frames, it works with sessions.\n",
    "Specifically, it tries to counteract the across-day session imaging field drift by shifting (deforming) the session\n",
    "imaging fields to align them in the same deformed space, using 'DiffeomorphicDemonsRegistration' adapted from\n",
    "[pirt](https://github.com/almarklein/pirt) library.\n",
    "\n",
    "After aligning all sessions in this 'deformed' (registered) space, the algorithm clusters the cells from different\n",
    "sessions based on their distance in the deformed space to track them across sessions. This relies on the assumption\n",
    "that a cell would appear in roughly similar location in the deformed visual space across all sessions. The discovered\n",
    "cell clusters are then used to generate a set of 'template' masks, one for each of the cells tracked across sessions.\n",
    "\n",
    "Finally, the algorithm transforms the 'template' masks back to the original (non-deformed) visual space of each session,\n",
    "so that they can be used to extract the fluorescence of the tracked cells for each session during the `roi processing`\n",
    "pipeline step.\n",
    "\n",
    "Unlike with the single-day pipeline, we currently do not offer a way of quantifying across-day registration metrics.\n",
    "Instead, you can use the interactive GUI below to visually inspect how the registration pipeline transforms\n",
    "the (registered) reference images and single-day detected cell masks from each session during the 'registration' step.\n",
    "\n",
    "**Note!** Any image with 'transformed_', in its name is an image in the deformed (registered) visual space, while any\n",
    "image without is in the original visual space of that session. There are 4 sets of cell masks that can be rendered:\n",
    "'unregistered' (single-day pipeline output), 'registered' (single-day masks converted to deformed visual space),\n",
    "'shared_multiday' masks (all template masks in the deformed space) and 'session_multiday' masks\n",
    "(all template masks in the original session visual space)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "suite2p.show_images_with_masks(ops_path=ops_path);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Extraction\n",
    "The second (and final) step of the multi-day pipeline extracts the fluorescence traces for cells tracked across days\n",
    "from each session. This generates the same set of files as the single-day pipeline, stored inside the 'multi_day' output\n",
    "folder of each session: the raw activity data for cells (F.npy), the surrounding neuropil (Fneu.npy), and the\n",
    "deconvolved cell activity (spks).\n",
    "\n",
    "The graphs below visualize the activity of a single chosen cell (ROI) across all processed sessions. The multi-day\n",
    "pipeline allows tracking the same cells even during the sessions when that particular cell is silent (not firing), so\n",
    "it is possible for the visualized ROI to not have any meaningful spike activity for some sessions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clears the widget figure from the previous code cell\n",
    "plt.close(\"all\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Selects the index of the cell (ROI) to be visualized. Adjust this to visualize different multi-day tracked cells.\n",
    "roi_index = 1500\n",
    "\n",
    "# Precreates temporary storage lists for fluorescence data\n",
    "f_cells_sessions = []\n",
    "f_neuropils_sessions = []\n",
    "spks_sessions = []\n",
    "\n",
    "# Reconstructs the path to the output folder from 'ops' parameters\n",
    "output_folder = Path(ops[\"multiday_save_path\"]).joinpath(ops[\"multiday_save_folder\"])\n",
    "\n",
    "# The output folder contains .npy and .yaml files and directories named after each processed session ID.\n",
    "# This re-generates the list of session IDs from the directories stored in the output folder.\n",
    "session_ids = [folder.stem for folder in output_folder.glob(\"*\") if folder.is_dir()]\n",
    "\n",
    "# Sorts session IDs for consistency\n",
    "session_ids = natsorted(session_ids)\n",
    "\n",
    "# Resolves paths to the multi-day output for each session\n",
    "session_directories = [output_folder.joinpath(session_id) for session_id in session_ids]\n",
    "\n",
    "# Loops over processed sessions and loads the data to be visualized below\n",
    "for session in session_directories:\n",
    "    f_cells_sessions.append(np.load(session.joinpath(\"F.npy\")))\n",
    "    f_neuropils_sessions.append(np.load(session.joinpath(\"Fneu.npy\")))\n",
    "    spks_sessions.append(np.load(session.joinpath(\"spks.npy\")))\n",
    "\n",
    "plt.figure(figsize=(20.0, 20.0))\n",
    "plt.suptitle(f\"Fluorescence and Deconvolved Traces for ROI {roi_index} Across Sessions\", y=0.92)\n",
    "\n",
    "# Assigns distinct color to visualized traces\n",
    "colors = [\"#1f77b4\", \"#2ca02c\", \"#d62728\"]  # Blue, Green, Red\n",
    "\n",
    "# Loops over sessions, extracts the fluorescence data for the specified cell from each session, and plots it\n",
    "# on the canvas.\n",
    "for i, (f_cells, f_neuropils, spks, session_name) in enumerate(\n",
    "    zip(f_cells_sessions, f_neuropils_sessions, spks_sessions, session_ids)\n",
    "):\n",
    "    plt.subplot(len(f_cells_sessions), 1, i + 1)\n",
    "\n",
    "    # Extracts data for the specific ROI from this session\n",
    "    f = f_cells[roi_index]\n",
    "    f_neu = f_neuropils[roi_index]\n",
    "    sp = spks[roi_index]\n",
    "\n",
    "    # Adjust range to match fluorescence traces\n",
    "    fmax = np.maximum(f.max(), f_neu.max())\n",
    "    fmin = np.minimum(f.min(), f_neu.min())\n",
    "    frange = fmax - fmin\n",
    "\n",
    "    # Normalizes spikes\n",
    "    if sp.max() > 0:\n",
    "        sp = sp / sp.max() * frange + fmin\n",
    "    else:\n",
    "        sp = np.zeros_like(sp) + fmin\n",
    "\n",
    "    plt.plot(f, color=colors[0], label=\"Cell Fluorescence\")\n",
    "    plt.plot(f_neu, color=colors[1], label=\"Neuropil Fluorescence\")\n",
    "    plt.plot(sp, color=colors[2], label=\"Deconvolved\")\n",
    "\n",
    "    plt.xticks(np.arange(0, f.shape[0], f.shape[0] // 10))\n",
    "\n",
    "    # Add title for session name above each plot\n",
    "    plt.title(f\"Session {session_name}\")\n",
    "\n",
    "    # Add y-axis label for fluorescence/pixel intensity\n",
    "    plt.ylabel(\"fluorescence\")\n",
    "\n",
    "    plt.xlabel(\"frame\")\n",
    "    plt.grid(True, linestyle=\":\", alpha=0.6)\n",
    "\n",
    "    if i == 0:\n",
    "        plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing multi-day pipeline steps\n",
    "This section demonstrates a more advanced workflow which consists of manually calling the two functions that jointly\n",
    "form the multi-day pipeline. This is the preferred way for running the pipeline on large datasets and powerful\n",
    "machines, offering more advanced users a flexible way to fine-tune the processing to match their specific needs.\n",
    "\n",
    "**Note!** In contrast to the single-day pipeline, only the second step of the multi-day pipeline can be explicitly\n",
    "parallelized across sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "In this demonstration, we will reuse the same parameters as used during the 'sequential' pipeline execution\n",
    "demonstrated above. Since the multi-day pipeline always overwrites any existing pipeline outputs, we can\n",
    "reuse the `ops_path` generated above to re-run all processing steps.\n",
    "\n",
    "**Tip!** If you want to generate multiple partially overlapping multi-day datasets (using different combinations of\n",
    "sessions from the same pool), change the 'multiday_save_folder' configuration parameter in the 'ops' configuration file\n",
    "for each unique set. All sets can be processed in parallel, as the processing does not modify the single-day pipeline\n",
    "outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tracking cells across sessions (days)\n",
    "First, all processed sessions need to be registered to the same visual space to track the cells active across sessions\n",
    "(days). See the discussion of multi-day pipeline outputs (above) for details on specific transformations carried out at\n",
    "this processing step.\n",
    "\n",
    "**Tip.** This step necessarily works with all sessions processed as part of the multi-day runtime. It automatically\n",
    "parallelizes session processing according to the 'parallel_workers' parameter loaded from the 'ops' configuration file.\n",
    "It is generally possible and safe to further parallelize this step by running multiple multi-day runtimes, as it\n",
    "requires comparably minimal memory (RAM) resources."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "suite2p.multi_day.discover_multiday_cells(ops_path=ops_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extracting across-day tracked cell fluorescence from each session\n",
    "While the default approach is to process all sessions sequentially, powerful machines can run ROI signal extraction\n",
    "in parallel to reduce the overall processing time. Note that each parallel session will use the number of CPU cores\n",
    "specified in the configuration file ('parallel_workers' field) to parallelize certain computations. Therefore, the\n",
    "total number of CPU cores used at the same time will be equal to\n",
    "`number_of_parallel_sessions * number_of_parallel_workers`.\n",
    "\n",
    "**Tip.** This is the most computationally- and memory-intensive step of the multi-day pipeline. Since this step uses a\n",
    "combination of multi-thread and single-thread operations, increasing the number of parallel session workers is\n",
    "detrimental to the processing speed beyond a certain threshold. We recommend using no more than **20** threads per\n",
    "session and advise testing lower thread numbers to determine the optimal number of parallel workers. However, compared\n",
    "to the single-day pipeline, this step uses less RAM, so it is safe to run more sessions in parallel than single-day\n",
    "planes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extracts the list of session IDs from the 'ops' configuration file to run ROI data extraction for all sessions in\n",
    "# parallel\n",
    "ops = np.load(ops_path, allow_pickle=True).item()\n",
    "session_ids = ops[\"session_ids\"]\n",
    "\n",
    "# Extracts fluorescence from across-day tracked cells for each processed session in parallel\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [\n",
    "        executor.submit(suite2p.multi_day.extract_multiday_fluorescence, ops_path, session)\n",
    "        for session in ops[\"session_ids\"]\n",
    "    ]\n",
    "    for future in futures:\n",
    "        future.result()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running suite2p via terminal\n",
    "All suite2p functions used in the demonstration above can be called via terminal Command-Line-Interface (CLI) installed\n",
    "into the python environment together with sl-suite2p package. Specifically, use `ss2p-config` command to generate the\n",
    "human-readable pipeline configuration .yaml files (another way of specifying 'ops' parameters). Use `ss2p-run` to\n",
    "execute the pipeline either as a sequence of all steps or as specific pipeline step(s). See our\n",
    "[repository](https://github.com/Sun-Lab-NBB/suite2p) for more details and the API documentation for these CLI commands.\n",
    "\n",
    "**Note!** The CLI commands are especially beneficial for running code on remote compute servers using shell scripts\n",
    "or similar methods. Complex suite2p runtimes can be entirely coded and executed via shell scripts using the CLI\n",
    "interface and precreated configuration files. For more details about running sl-suite2p on remote compute servers,\n",
    "see the [sl-forgery](https://github.com/Sun-Lab-NBB/sl-forgery) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
