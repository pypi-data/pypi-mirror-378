{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt\n",
    "%matplotlib inline\n",
    "# Imports necessary libraries\n",
    "import json\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import suite2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure Style settings for the notebook.\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"axes.spines.left\": False,\n",
    "        \"axes.spines.bottom\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"legend.frameon\": False,\n",
    "        \"figure.subplot.wspace\": 0.01,\n",
    "        \"figure.subplot.hspace\": 0.01,\n",
    "        \"figure.figsize\": (18, 13),\n",
    "        \"ytick.major.left\": False,\n",
    "    }\n",
    ")\n",
    "jet = mpl.colormaps.get_cmap(\"jet\")\n",
    "jet.set_bad(color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the single-day suite2p pipeline on example data\n",
    "\n",
    "This notebook will guide you through the various stages and outputs of the suite2p single-day pipeline. It is intended\n",
    "to run on your own dataset, but, if you do not have a dataset to run this pipeline, you can explore dataset generation\n",
    "options from the [original suite2p repository](https://github.com/MouseLand/suite2p/tree/main). This demonstration is\n",
    "written using the data featured in the [OSM manuscript](https://www.nature.com/articles/s41586-024-08548-w).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the runtime parameters\n",
    "\n",
    "All single-day runtime parameters are stored in a SingleDayS2PConfiguration dataclass instance. To generate an instance\n",
    "filled with default parameters, use the `generate_default_ops` function. You can find an explanation of most single-day\n",
    "parameters in the original suite2p documentation [here](https://suite2p.readthedocs.io/en/latest/settings.html).\n",
    "Alternatively, you can check our repository and API documentation [here](https://github.com/Sun-Lab-NBB/suite2p).\n",
    "Note, the default single-day parameters are preconfigured for mesoscope-acquired data used in the Sun lab and will\n",
    "likely need tuning to work in other contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = suite2p.generate_default_ops(as_dict=False)  # If 'as_dict' is true, this returns the 'ops' dictionary.\n",
    "print(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding specific configuration parameters\n",
    "\n",
    "It is common to keep most processing parameters the same between different datasets and projects. However, some\n",
    "processing parameters need to be adjusted for each processed dataset (recording session). Most of these parameters are\n",
    "addressable via the configuration class.\n",
    "\n",
    "***Important!*** When running this demonstration on your dataset, adjust the parameters below to match your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input directory stores the data to be processed. Typically, this would be a folder filled with .tif or .tiff files.\n",
    "# If you are working with mesoscope data, this folder would also typically contain the ops.json file generated from\n",
    "# ScanImage TIFs by the helper script\n",
    "ops.file_io.data_path = [\"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/2022-02-02-17-16-40-000000/data\"]\n",
    "\n",
    "# The output directory determines whether the processed data is saved.\n",
    "ops.file_io.save_path0 = \"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/2022-02-02-17-16-40-000000/\"\n",
    "\n",
    "# The type of the input dataset. In this demonstration, the processed data is a Mesoscope scan.\n",
    "ops.file_io.mesoscan = True\n",
    "\n",
    "# Limits the number of parallel workers to 10 per each plane\n",
    "ops.main.parallel_workers = 10\n",
    "\n",
    "# Enables registration metrics computation.\n",
    "ops.registration.compute_registration_metrics = True\n",
    "\n",
    "# This improves terminal output during sequential processing, but breaks during parallel processing.\n",
    "ops.main.progress_bars = True\n",
    "\n",
    "# Converts the configuration class to the dictionary format expected by downstream functions.\n",
    "ops = ops.to_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters not addressable through the configuration class\n",
    "\n",
    "In addition to the configuration-class-addressable parameters, there are other suite2p parameters typically resolved\n",
    "automatically during runtime. However, it is also possible to provide these parameters manually. To do so, we define an\n",
    "additional shallow dictionary `db` that stores these parameters and their values.\n",
    "\n",
    "**Note!** Most of the additional parameters for mesoscope recordings are resolved through helper scripts, such as the\n",
    "one listed in the original suite2p repository or the one used in our\n",
    "[sl-experiment](https://github.com/Sun-Lab-NBB/sl-experiment) library. For example, when working with mesoscope\n",
    "recordings, the suite2p automatically searches for the ops.json file(s) inside all data folders and resolves\n",
    "these additional parameters as necessary. Here, we load this data manually to demonstrate how to write the `db`\n",
    "dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually loads the ops.json file as a dictionary\n",
    "ops_path = Path(ops[\"data_path\"][0]).joinpath(\"ops.json\")\n",
    "with open(ops_path) as f:\n",
    "    precursor: dict[str, Any] = json.load(f)\n",
    "\n",
    "# Generates 'db' using a subset of data from the loaded JSON file\n",
    "db = {\n",
    "    \"fs\": precursor[\"fs\"],\n",
    "    \"nplanes\": precursor[\"nplanes\"],\n",
    "    \"nchannels\": precursor.get(\"nchannels\", 1),  # If 'nchannels' not in ops.json, sets it to 1\n",
    "    \"nrois\": precursor[\"nrois\"],\n",
    "    \"dx\": precursor[\"dx\"],\n",
    "    \"dy\": precursor[\"dy\"],\n",
    "    \"lines\": precursor[\"lines\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the ops.npy file\n",
    "To integrate the parameters from the `db` dictionary into the `ops` dictionary, use the `resolve_ops` function.\n",
    "This function also translates the resultant dictionary into the ops.npy file and returns the path to this file.\n",
    "\n",
    "**Tip** For specifying runtime parameters `ops` and `db` are functionally equivalent. The only difference is that any\n",
    "parameters from `db` overwrite parameters from `ops` and parameters from `ops` override default configuration\n",
    "parameters. If your use case does not require one or both of these dictionaries, set them to {} (empty) when calling\n",
    "`resolve_ops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs the runtime configuration dictionary and dumps it as an .npy file\n",
    "ops_path = suite2p.single_day.resolve_ops(ops=ops, db=db)\n",
    "print(ops_path)\n",
    "\n",
    "# Loads and visualizes the final state of the configuration dictionary:\n",
    "final_ops = np.load(file=ops_path, allow_pickle=True).item()\n",
    "print(final_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note!** In sl-suite2p versions 2.0.0+, the resolve_ops() function now also generates a '.yaml' snapshot of the\n",
    "resolved configuration parameters. Since .yaml files are human-readable and do not rely on unsafe pickling, saving the\n",
    "final parameters as a 'yaml' file is primarily intended as an additional safety feature. It prevents data loss if you\n",
    "lose the ability to load the 'ops.npy' file, for example, due to silent changes to the underlying pickling protocol.\n",
    "The snapshot is saved to the same folder as the 'ops.npy' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the single-day suite2p pipeline\n",
    "The default way to run the single-day pipeline is by calling the `run_s2p` function from the `single_day` module.\n",
    "This function executes all steps of the single-day processing pipeline in a sequence, using the processing parameters\n",
    "from the ops.npy file created via the `resolve_ops` function.\n",
    "\n",
    "**Note!** The function will use up to 'parallel_workers' CPU cores when processing each plane and may require a\n",
    "significant amount of memory (RAM), depending on the input dataset size. However, it will only process a single plane\n",
    "at a time, keeping both RAM and CPU requirements generally manageable for most use cases.\n",
    "\n",
    "**Advanced users with access to powerful compute machines or distributed clusters should consult the sections at the\n",
    "end of this notebook to learn about running pipeline steps in parallel.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite2p.single_day.run_s2p(ops_path=ops_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-day pipeline outputs\n",
    "This section goes over the outputs generated by the single-day pipeline and briefly mentions how each can be used when\n",
    "working with your own datasets.\n",
    "\n",
    "**Note!** We highly recommend consulting the original suite2p documentation and / or watching this\n",
    "[video](https://www.youtube.com/watch?v=HpL5XNtC5wU) to understand the principles behind suite2p outputs discussed\n",
    "below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ops dictionaries\n",
    "During processing, the pipeline caches various output data inside the `ops.npy` file for each processed plane. If\n",
    "planes are combined during post-processing, the suite2p also generates a 'combined' `ops.npy` file. Each output ops file\n",
    "contains all keys that went into the pipeline, plus new keys that contain additional metrics/outputs calculated during\n",
    "the pipeline runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the 'ops.npy' file from plane 0\n",
    "combined_path = Path(ops_path).parent.joinpath(\"plane0\", \"ops.npy\")\n",
    "output_ops = np.load(combined_path, allow_pickle=True).item()\n",
    "print(set(output_ops.keys()).difference(ops.keys()))  # Shows keys added during runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration (images)\n",
    "During registration, the pipeline tries to register every frame of the processed plane movie to the 'reference image.'\n",
    "A good registration result would therefore generate a 'mean image' that is visually similar to the 'reference image.'\n",
    "\n",
    "**Important!** The best way to verify registration results is to use the suite2p GUI to render and evaluate the data\n",
    "generated during registration metrics computation (see below). For well-registered data, the rendered video should have\n",
    "no significant shift of cells in the X and Y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(\n",
    "    output_ops[\"refImg\"],\n",
    "    cmap=\"gray\",\n",
    ")\n",
    "plt.title(\"Reference Image for Registration\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(output_ops[\"max_proj\"], cmap=\"gray\")\n",
    "plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(output_ops[\"meanImg\"], cmap=\"gray\")\n",
    "plt.title(\"Mean registered image\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(output_ops[\"meanImgE\"], cmap=\"gray\")\n",
    "plt.title(\"High-pass filtered Mean registered image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Registration (metrics)\n",
    "Additionally, the pipeline (optionally) computes the registration quality metrics. The metrics quantify the rigid and\n",
    "non-rigid offsets (shifts) that need to be applied to register the top and bottom of processed moviesâ€™ n\n",
    "Principal Components (here, we use 10) to each-other. Note, the PCs are extracted from a subset of frames evenly sampled\n",
    "across the entire movie. For well-registered movies, the shifts should be close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the offsets\n",
    "offsets = output_ops[\"regDX\"]\n",
    "mean_offsets = np.mean(offsets, axis=0)\n",
    "max_offsets = np.max(offsets, axis=0)\n",
    "\n",
    "# Formats the offsets to match the original suite2p registration metric script output.\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Avg_Rigid: {mean_offsets[0]:.6f} \\tAvg_Average NR: {mean_offsets[1]:.6f} \\tAvg_Max NR: {mean_offsets[2]:.6f}\n",
    "    Max_Rigid: {max_offsets[0]:.6f} \\tMax_Average NR: {max_offsets[1]:.6f} \\tMax_Max NR: {max_offsets[2]:.6f}\n",
    "    \"\"\".replace(\"            \", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Detection\n",
    "During ROI detection, the pipeline first discovers cell ROI candidates and then passes them through a classifier to\n",
    "determine whether these ROIs are valid cells. The output of the classifier is stored in the `iscell.npy` file and the\n",
    "ROI statistics, including cell masks, are stored in the `stat.npy` file. The data stored in these files always has\n",
    "the same dimension.\n",
    "\n",
    "**Note!** While this demonstration uses a simple built-in classifier, suite2p can be configured to use the\n",
    "[cellpose](https://www.cellpose.org/) classifier to further augment ROI detection and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file = Path(output_ops[\"save_path\"]).joinpath(\"stat.npy\")\n",
    "iscell = np.load(Path(output_ops[\"save_path\"]).joinpath(\"iscell.npy\"), allow_pickle=True)[:, 0].astype(bool)\n",
    "stats = np.load(stats_file, allow_pickle=True)\n",
    "stats.shape, iscell.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the ROI detection (extraction) step is to identify the cells from which to extract the fluorescence\n",
    "signals (see below). A well-configured ROI extraction step should discover most cells observable in the registered image\n",
    "and discard most outlier objects a human rater would not consider a cell. The accuracy of the ROI extraction can be\n",
    "assessed by comparing the discovered and rejected ROI to the registered maximum projection image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts ROI statistics stored as a list of dictionaries to a renderable NumPy matrix\n",
    "im = suite2p.ROI.stats_dicts_to_3d_array(stats, Ly=output_ops[\"Ly\"], Lx=output_ops[\"Lx\"], label_id=True)\n",
    "# Replaces zero-values with NaNs\n",
    "im[im == 0] = np.nan\n",
    "\n",
    "# Suppresses RuntimeWarnings generated by np.nanmax() calls when they receive all-NaN slices.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(output_ops[\"max_proj\"], cmap=\"gray\")\n",
    "    plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(np.nanmax(im, axis=0), cmap=\"jet\")\n",
    "    plt.title(\"All ROIs Found\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(np.nanmax(im[~iscell], axis=0), cmap=\"jet\")\n",
    "    plt.title(\"All Non-Cell ROIs\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(np.nanmax(im[iscell], axis=0), cmap=\"jet\")\n",
    "    plt.title(\"All Cell ROIs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Extraction\n",
    "As a final step, suite2p extracts the fluorescence traces from each cell ROI and generates multiple output files.\n",
    "Available files contain the raw activity data for cells (F.npy), the surrounding neuropil (Fneu.npy), and the\n",
    "deconvolved cell activity (spks). All extracted data have the same shape of cell_number x movie frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cells = np.load(Path(output_ops[\"save_path\"]).joinpath(\"F.npy\"))\n",
    "f_neuropils = np.load(Path(output_ops[\"save_path\"]).joinpath(\"Fneu.npy\"))\n",
    "spks = np.load(Path(output_ops[\"save_path\"]).joinpath(\"spks.npy\"))\n",
    "f_cells.shape, f_neuropils.shape, spks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs below visualize the activity of some ROIs over the course of the processed plane movie. For this\n",
    "demonstration we use a small sample of all cell ROIs, but the entire dataset is available for further use in\n",
    "analysis by loading the same .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20.0, 20.0))\n",
    "plt.suptitle(\"Fluorescence and Deconvolved Traces for Different ROIs\", y=0.92)\n",
    "\n",
    "# Evenly samples the required number of ROIs. Adjusting the step rate here allows controlling the number of\n",
    "# rendered ROIs\n",
    "rois = np.arange(len(f_cells))[::500]\n",
    "\n",
    "# Assigns distinct color to visualized traces\n",
    "colors = [\"#1f77b4\", \"#2ca02c\", \"#d62728\"]  # Blue, Green, Red\n",
    "\n",
    "for i, roi in enumerate(rois):\n",
    "    plt.subplot(len(rois), 1, i + 1)\n",
    "    f = f_cells[roi]\n",
    "    f_neu = f_neuropils[roi]\n",
    "    sp = spks[roi]\n",
    "\n",
    "    # Adjust range to match fluorescence traces\n",
    "    fmax = np.maximum(f.max(), f_neu.max())\n",
    "    fmin = np.minimum(f.min(), f_neu.min())\n",
    "    frange = fmax - fmin\n",
    "\n",
    "    # Normalizes spikes\n",
    "    if sp.max() > 0:\n",
    "        sp = sp / sp.max() * frange + fmin\n",
    "    else:\n",
    "        sp = np.zeros_like(sp) + fmin\n",
    "\n",
    "    plt.plot(f, color=colors[0], label=\"Cell Fluorescence\")\n",
    "    plt.plot(f_neu, color=colors[1], label=\"Neuropil Fluorescence\")\n",
    "    plt.plot(sp, color=colors[2], label=\"Deconvolved\")\n",
    "\n",
    "    plt.xticks(np.arange(0, f_cells.shape[1], f_cells.shape[1] // 10))\n",
    "    plt.ylabel(f\"ROI {roi}\", rotation=0, labelpad=10)\n",
    "    plt.xlabel(\"frame\")\n",
    "    plt.grid(True, linestyle=\":\", alpha=0.6)  # Add grid for easier comparison\n",
    "\n",
    "    if i == 0:\n",
    "        plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing single-day pipeline steps\n",
    "This section demonstrates a more advanced workflow which consists of manually calling the three functions that jointly\n",
    "form the single-day pipeline. This is the preferred way for running the pipeline on large datasets and powerful\n",
    "machines, offering more advanced users a flexible way to fine-tune the processing to match their specific needs.\n",
    "\n",
    "**Note!** Each of the functions discussed below can be executed in parallel for multiple datasets (sessions) to increase\n",
    "the overall processing speed. Additionally, the 'process_plane' function can be called in parallel for each plane\n",
    "(of every session), resulting in an even faster processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "The steps to prepare for a parallel runtime are the same as for the sequential runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the processing in parallel, we will use a different session and tweak some important parameters.\n",
    "ops = suite2p.generate_default_ops(as_dict=False)\n",
    "\n",
    "# Different session path\n",
    "ops.file_io.data_path = [\"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/2022-02-03-17-23-54-000000/data\"]\n",
    "ops.file_io.save_path0 = \"/home/cyberaxolotl/Desktop/sl_suite2p_demos/Tyche/A7/2022-02-03-17-23-54-000000/\"\n",
    "\n",
    "# This is also a mesoscope session\n",
    "ops.file_io.mesoscan = True\n",
    "\n",
    "# Critical! Limits the number of workers to 10 per plane. When planes run in parallel, this will be multiplied by the\n",
    "# number of planes. Adjust this value to work for your system.\n",
    "ops.main.parallel_workers = 10\n",
    "\n",
    "# Disables progress bars, as they cannot be properly displayed when running processing in parallel.\n",
    "ops.main.progress_bars = False\n",
    "\n",
    "# Converts ops to dictionary\n",
    "ops = ops.to_ops()\n",
    "\n",
    "# Resolves 'db', works the same way as in the example above\n",
    "ops_path = Path(ops[\"data_path\"][0]).joinpath(\"ops.json\")\n",
    "with open(ops_path) as f:\n",
    "    precursor: dict[str, Any] = json.load(f)\n",
    "db = {\n",
    "    \"fs\": precursor[\"fs\"],\n",
    "    \"nplanes\": precursor[\"nplanes\"],\n",
    "    \"nchannels\": precursor.get(\"nchannels\", 1),\n",
    "    \"nrois\": precursor[\"nrois\"],\n",
    "    \"dx\": precursor[\"dx\"],\n",
    "    \"dy\": precursor[\"dy\"],\n",
    "    \"lines\": precursor[\"lines\"],\n",
    "}\n",
    "\n",
    "# Generates the ops.npy file\n",
    "ops_path = suite2p.single_day.resolve_ops(ops=ops, db=db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generating plane binary files and folders\n",
    "First, the processed dataset needs to be converted into binary (.bin) files. Regardless of the input raw data format,\n",
    "this step generates a separate output folder for each imaging plane in the dataset. Then, it writes the data.bin\n",
    "(plane data) and ops.npy (plane processing parameters and intermediate pipeline outputs) files to the folder.\n",
    "\n",
    "**Tip.** Since this step only uses a single core and memory mapping, it is safe to parallelize it for many sessions\n",
    "without limiting the number of parallel workers or assigning significant memory allocations. We typically use 1 or 2\n",
    "cores and 5 or 10 GB RAM per parallel session when running this on a compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite2p.single_day.resolve_binaries(ops_path=ops_path)\n",
    "\n",
    "# Collects the paths to individual plane folders to be used below\n",
    "planes = [path for path in ops_path.parent.glob(\"plane*\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Processing planes\n",
    "While the default approach is to process all planes sequentially, powerful machines can run plane processing in parallel\n",
    "to reduce the overall processing time. Note that each parallel plane will use the number of CPU cores specified in the\n",
    "configuration file ('parallel_workers' field) to parallelize certain computations. Therefore, the total number of CPU\n",
    "cores used at the same time will be equal to `number_of_parallel_planes * number_of_parallel_workers`.\n",
    "\n",
    "**Tip.** This is the most computationally- and memory-intensive step of the single-day pipeline. Since this step uses a\n",
    "combination of multi-thread and single-thread operations, increasing the number of parallel plane workers is detrimental\n",
    "to the processing speed beyond a certain threshold. We recommend using no more than **20** threads per plane and advise\n",
    "testing lower thread numbers to determine the optimal number of parallel workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although using 'range' would have worked here too, extracts the indices using plane folder names\n",
    "# for demonstration and added safety\n",
    "plane_indices = [int(plane.name[-1]) for plane in planes]\n",
    "\n",
    "# Sets up a ProcessPool executor and processes each plane in parallel. This demonstrates parallelizing single-day\n",
    "# processing steps and can be applied to all steps across sessions (datasets).\n",
    "with ProcessPoolExecutor(max_workers=len(plane_indices)) as executor:\n",
    "    # ops_path is the same for all processing steps. Each plane has a unique index.\n",
    "    futures = [executor.submit(suite2p.single_day.process_plane, ops_path, index) for index in plane_indices]\n",
    "\n",
    "    # Waits for the processing to complete\n",
    "    for future in futures:\n",
    "        _ = future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Post-processing the data\n",
    "\n",
    "This is a comparatively minor step that optionally assembles a unified output dataset. Specifically, it allows\n",
    "assembling all processed data into an .nwb file or a `combined` folder. The latter type of processing is required if\n",
    "you intend to also process the session with the multi-day pipeline. It is safe to 'combine' sessions even if they only\n",
    "have a single plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite2p.single_day.combine_planes(ops_path=ops_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running suite2p via terminal\n",
    "All suite2p functions used in the demonstration above can be called via terminal Command-Line-Interface (CLI) installed\n",
    "into the python environment together with sl-suite2p package. Specifically, use `ss2p-config` command to generate the\n",
    "human-readable pipeline configuration .yaml files (another way of specifying 'ops' parameters). Use `ss2p-run` to\n",
    "execute the pipeline either as a sequence of all steps or as specific pipeline step(s). See our\n",
    "[repository](https://github.com/Sun-Lab-NBB/suite2p) for more details and the API documentation for these CLI commands.\n",
    "\n",
    "**Note!** The CLI commands are especially beneficial for running code on remote compute servers using shell scripts\n",
    "or similar methods. Complex suite2p runtimes can be entirely coded and executed via shell scripts using the CLI\n",
    "interface and precreated configuration files. For more details about running sl-suite2p on remote compute servers,\n",
    "see the [sl-forgery](https://github.com/Sun-Lab-NBB/sl-forgery) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
