Metadata-Version: 2.4
Name: mimo-llm
Version: 0.1.3
Summary: A language model fine-tuned for code and conversation.
Home-page: https://github.com/eurocybersecurite/Mimo-llm
Author: ABDESSEMED Mohamed
Author-email: mohamed.abdessemed@eurocybersecurite.fr
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: transformers>=4.35
Requires-Dist: datasets>=2.14
Requires-Dist: accelerate>=0.22
Requires-Dist: bitsandbytes>=0.41
Requires-Dist: peft>=0.4.0
Requires-Dist: torch>=2.1
Requires-Dist: git-lfs
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ğŸš€ Mimo Language Model

Mimo est un modÃ¨le de langage AI pour exceller Ã  la fois en **gÃ©nÃ©ration de code** et en **conversations naturelles**.  
Il est issu d'un mÃ©lange de datasets puissants.

![Mimo](https://raw.githubusercontent.com/eurocybersecurite/Mimo-llm/main/assets/mimo.png)


---

## âœ¨ Points forts de Mimo

- ğŸ”§ **OptimisÃ© pour le code** : gÃ©nÃ©ration fiable de scripts Python, JS, etc.  
- ğŸ’¬ **Excellente conversation** : rÃ©ponses naturelles et contextualisÃ©es.  
- âš¡ **CompatibilitÃ© multiplateforme** : fonctionne sur Mac, PC et VSCode.  
- ğŸ“¦ **PrÃªt pour la quantification** (GGUF) â†’ utilisable avec LM Studio ou Ollama.  

---

## ğŸ“¦ Installation

Clonez le dÃ©pÃ´t et installez les dÃ©pendances dans un environnement virtuel :

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/eurocybersecurite/Mimo-llm.git
cd Mimo-llm

# CrÃ©er et activer un environnement virtuel (recommandÃ©)
python3 -m venv .venv
source .venv/bin/activate  # Sur Linux/macOS
# Ou sur Windows : .\.venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

âš ï¸ Assurez-vous dâ€™avoir `git-lfs` installÃ© pour gÃ©rer les poids du modÃ¨le.

---

## ğŸ”‘ Configuration

Avant toute utilisation, configurez votre **Hugging Face Token** :

```bash
export HF_TOKEN="votre_token_hugging_face"
```
(Remplacez `"votre_token_hugging_face"` par votre vÃ©ritable token.)

---

## ğŸ‹ï¸ Fine-tuning

Lancez le fine-tuning avec :

```bash
python fine_tune_mimo.py
```

**IMPORTANT :** Remplacez `example.jsonl` par votre propre fichier de dataset avant d'exÃ©cuter ce script. Le fichier `example.jsonl` contient quelques exemples fictifs Ã  des fins de dÃ©monstration.

- Utilise vos donnÃ©es perso (`example.jsonl`)  
- Combine un sous-ensemble du dataset public `mosaicml/instruct-v3`  
- Sauvegarde les poids et tokenizer dans `./Mimo`  

---

## ğŸ§‘â€ğŸ’» Exemples dâ€™utilisation

### GÃ©nÃ©ration de code

```python
# Assurez-vous que le modÃ¨le et le tokenizer sont chargÃ©s correctement
# Exemple d'infÃ©rence pour la gÃ©nÃ©ration de code
prompt_code = "Ã‰cris une fonction Python pour calculer la somme des Ã©lÃ©ments d'une liste."
inputs_code = tokenizer(prompt_code, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs_code = model.generate(
        **inputs_code,
        max_new_tokens=100,
        pad_token_id=tokenizer.eos_token_id
    )
generated_code = tokenizer.decode(outputs_code[0], skip_special_tokens=True)
print("--- GÃ©nÃ©ration de Code ---")
print(generated_code)
```

### Conversation

```python
# Exemple d'infÃ©rence pour la conversation
prompt_conversation = "Quelle est la meilleure faÃ§on d'apprendre une nouvelle langue ?"
inputs_conversation = tokenizer(prompt_conversation, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs_conversation = model.generate(
        **inputs_conversation,
        max_new_tokens=50,
        pad_token_id=tokenizer.eos_token_id
    )
generated_conversation = tokenizer.decode(outputs_conversation[0], skip_special_tokens=True)
print("\n--- GÃ©nÃ©ration de Conversation ---")
print(generated_conversation)
```

---

## ğŸ“Š Performances comparatives

| ModÃ¨le                          | Code (Python) | Conversation | MÃ©moire requise |
|---------------------------------|---------------|--------------|-----------------|
| GPT-Neo 1.3B                    | â­â­            | â­â­           | ~12 Go          |
| DeepSeek-Qwen-1.5B (base)       | â­â­â­           | â­â­â­          | ~10 Go          |
| **Mimo-1.5B (fine-tuned)**      | â­â­â­â­          | â­â­â­â­         | ~8 Go (quantisÃ©) |

â¡ï¸ **Mimo surpasse la version de base** sur les benchmarks internes (code + QA).

![Mimo Performance](https://raw.githubusercontent.com/eurocybersecurite/Mimo-llm/main/assets/mimo_conv_code.png)

---

## ğŸ“‚ Structure du dÃ©pÃ´t

```
Mimo/
â”œâ”€â”€ README.md
â”œâ”€â”€ assets/mimo.png
â”œâ”€â”€ mohamed.jsonl
â”œâ”€â”€ fine_tune_mimo.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ IntÃ©gration dans VSCode

1. Clonez le dÃ©pÃ´t :  
   ```bash
   git clone https://github.com/votre-utilisateur/mimo-llm.git
   cd mimo-llm
   ```
2. Installez les dÃ©pendances :  
   ```bash
   pip install -r requirements.txt
   ```
3. ExÃ©cutez soit :  
   - `fine_tune_mimo.py` â†’ pour lâ€™entraÃ®nement  
   - un script dâ€™infÃ©rence personnalisÃ©  

âš¡ Vous pouvez aussi utiliser Mimo dans **LM Studio** en important la version quantisÃ©e GGUF ou autre Format.

---

## ğŸ“§ Auteur

- **Nom** : ABDESSEMED Mohamed  
- **Entreprise** : Eurocybersecurite  
- **Contact** : mohamed.abdessemed@eurocybersecurite.fr
