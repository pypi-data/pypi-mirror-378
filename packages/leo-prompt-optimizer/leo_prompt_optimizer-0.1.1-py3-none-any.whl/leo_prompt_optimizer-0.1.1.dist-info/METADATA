Metadata-Version: 2.4
Name: leo-prompt-optimizer
Version: 0.1.1
Summary: A Python library to optimize prompt drafts using LLMs
Author: LÃ©onard Baesen-Wagner
Author-email: Leonard Baesen-Wagner <lr.baesen@gmail.com>
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: groq
Requires-Dist: python-dotenv
Dynamic: author
Dynamic: requires-python

# ğŸ§  leo-prompt-optimizer

**leo-prompt-optimizer** is a Python library that helps developers **optimize raw prompt drafts** into structured, high-performance prompts for large language models (LLMs).

It leverages **open-source models via [Groq API](https://console.groq.com/)** (like LLaMA 3 or Mixtral), making it fast, affordable, and production-ready.

---

## ğŸš€ Features

- ğŸ› ï¸ Refines messy or vague prompts into structured, effective ones
- ğŸ§  Follows a 9-step prompt engineering framework
- ğŸ“¦ Supports contextual optimization (with user input & LLM output)
- âš¡ Uses blazing-fast open-source LLMs via Groq
- ğŸ” Secure API key handling with `.env`

---

## ğŸ“¦ Installation

```bash
pip install leo-prompt-optimizer
````

---

## ğŸ”§ Setup

1. Create a `.env` file at the root of your project:

```env
GROQ_API_KEY=your_groq_api_key_here
```

2. Install dependencies:

```bash
pip install leo-prompt-optimizer
```

---

## âœï¸ Usage Example

```python
from leo_prompt_optimizer.optimizer import optimize_prompt

draft = "I want to understand user feedback better. Can you help me ask the right questions?"

user_input = "Users say the interface feels confusing. I want to understand what they mean exactly."

llm_output = "You could ask: 'Which parts feel confusing?' or 'How could it be more intuitive?'"

optimized = optimize_prompt(draft, user_input, llm_output)

print(optimized)
```

---

## ğŸ“˜ Output Format

The optimized prompt follows a structured format like:

```text
Role:
[Define the LLM's persona]

Task:
[Clearly state the specific objective]

Instructions:
* Step-by-step subtasks

Context:
[Any relevant background, constraints, domain]

Output Format:
[e.g., bullet list, JSON, summary]

User Input:
[Original user input or example]
```

---

## ğŸ’¡ Why Use It?

Prompt quality is critical when building with LLMs.
**leo-prompt-optimizer** helps you:

* Make prompts explicit and usable across apps
* Reduce hallucination
* Increase repeatability and reliability

---

## ğŸ“„ License

MIT Â© 2025 \[Leonard Baesen]

````
