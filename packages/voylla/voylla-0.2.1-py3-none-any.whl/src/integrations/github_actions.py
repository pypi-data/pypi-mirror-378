"""
GitHub Actions Integration - Automated test suggestions on PRs
"""
import os
import json
import requests
from typing import Dict, List, Any, Optional
from pathlib import Path

class GitHubActionsIntegration:
    """Integration with GitHub Actions for automated test suggestions."""
    
    def __init__(self, github_token: Optional[str] = None):
        self.github_token = github_token or os.getenv('GITHUB_TOKEN')
        self.base_url = "https://api.github.com"
        self.headers = {
            'Authorization': f'token {self.github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
    
    def generate_workflow_file(self, output_path: str = ".github/workflows/test-generator.yml"):
        """Generate GitHub Actions workflow file for automated test generation."""
        
        workflow_content = """name: Test Case Generator

on:
  pull_request:
    types: [opened, synchronize]
    paths:
      - '**.py'
      - '**.js'
      - '**.ts'
      - '**.java'

jobs:
  generate-tests:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Get changed files
      id: changed-files
      uses: tj-actions/changed-files@v35
      with:
        files: |
          **.py
          **.js
          **.ts
          **.java
    
    - name: Generate test suggestions
      if: steps.changed-files.outputs.any_changed == 'true'
      run: |
        echo "Changed files: ${{ steps.changed-files.outputs.all_changed_files }}"
        
        # Create output directory
        mkdir -p suggested-tests
        
        # Generate tests for each changed file
        for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
          echo "Generating tests for $file"
          python src/main.py --file "$file" --output suggested-tests/ --coverage
        done
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
    
    - name: Create PR comment with test suggestions
      if: steps.changed-files.outputs.any_changed == 'true'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read generated test files
          const suggestedTestsDir = 'suggested-tests';
          let comment = '## 🧪 Generated Test Suggestions\\n\\n';
          comment += 'The Test Case Generator Bot has analyzed your code changes and suggests the following test cases:\\n\\n';
          
          if (fs.existsSync(suggestedTestsDir)) {
            const testFiles = fs.readdirSync(suggestedTestsDir);
            
            for (const testFile of testFiles) {
              const testPath = path.join(suggestedTestsDir, testFile);
              const testContent = fs.readFileSync(testPath, 'utf8');
              
              comment += `### ${testFile}\\n\\n`;
              comment += '```' + (testFile.endsWith('.py') ? 'python' : 
                                testFile.endsWith('.js') ? 'javascript' : 
                                testFile.endsWith('.java') ? 'java' : '') + '\\n';
              comment += testContent;
              comment += '\\n```\\n\\n';
            }
          } else {
            comment += 'No test suggestions generated for the changed files.\\n';
          }
          
          comment += '---\\n';
          comment += '*Generated by Test Case Generator Bot* 🤖';
          
          // Post comment on PR
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
    
    - name: Upload test artifacts
      if: steps.changed-files.outputs.any_changed == 'true'
      uses: actions/upload-artifact@v3
      with:
        name: suggested-tests
        path: suggested-tests/
        retention-days: 30
"""
        
        # Create .github/workflows directory
        workflow_path = Path(output_path)
        workflow_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Write workflow file
        with open(workflow_path, 'w') as f:
            f.write(workflow_content)
        
        return str(workflow_path)
    
    def analyze_pr_changes(self, repo_owner: str, repo_name: str, pr_number: int) -> List[str]:
        """Analyze PR changes and return list of modified code files."""
        
        url = f"{self.base_url}/repos/{repo_owner}/{repo_name}/pulls/{pr_number}/files"
        
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            
            files_data = response.json()
            code_files = []
            
            for file_info in files_data:
                filename = file_info['filename']
                status = file_info['status']
                
                # Only process added or modified code files
                if status in ['added', 'modified'] and self._is_code_file(filename):
                    code_files.append(filename)
            
            return code_files
            
        except requests.RequestException as e:
            print(f"Error fetching PR files: {e}")
            return []
    
    def post_test_suggestions(self, repo_owner: str, repo_name: str, pr_number: int, 
                            test_suggestions: Dict[str, List[str]]):
        """Post test suggestions as a PR comment."""
        
        comment_body = self._format_test_suggestions_comment(test_suggestions)
        
        url = f"{self.base_url}/repos/{repo_owner}/{repo_name}/issues/{pr_number}/comments"
        
        payload = {
            'body': comment_body
        }
        
        try:
            response = requests.post(url, headers=self.headers, json=payload)
            response.raise_for_status()
            print(f"Posted test suggestions to PR #{pr_number}")
            
        except requests.RequestException as e:
            print(f"Error posting PR comment: {e}")
    
    def _is_code_file(self, filename: str) -> bool:
        """Check if file is a supported code file."""
        code_extensions = ['.py', '.js', '.ts', '.java']
        return any(filename.endswith(ext) for ext in code_extensions)
    
    def _format_test_suggestions_comment(self, test_suggestions: Dict[str, List[str]]) -> str:
        """Format test suggestions as a markdown comment."""
        
        comment = "## 🧪 Test Case Generator Bot Suggestions\\n\\n"
        comment += "I've analyzed your code changes and generated the following test suggestions:\\n\\n"
        
        for filename, suggestions in test_suggestions.items():
            comment += f"### {filename}\\n\\n"
            
            for i, suggestion in enumerate(suggestions, 1):
                comment += f"{i}. **{suggestion['name']}** ({suggestion['type']})\\n"
                comment += f"   - {suggestion['description']}\\n"
                comment += f"   ```{self._get_language_from_filename(filename)}\\n"
                comment += f"   {suggestion['code']}\\n"
                comment += f"   ```\\n\\n"
        
        comment += "---\\n"
        comment += "*These suggestions are generated automatically. Please review and adapt them as needed.*\\n"
        comment += "*Generated by Test Case Generator Bot* 🤖"
        
        return comment
    
    def _get_language_from_filename(self, filename: str) -> str:
        """Get language identifier for syntax highlighting."""
        if filename.endswith('.py'):
            return 'python'
        elif filename.endswith('.js'):
            return 'javascript'
        elif filename.endswith('.ts'):
            return 'typescript'
        elif filename.endswith('.java'):
            return 'java'
        return ''
    
    def create_pr_check(self, repo_owner: str, repo_name: str, commit_sha: str, 
                       test_results: Dict[str, Any]):
        """Create a GitHub check run with test generation results."""
        
        url = f"{self.base_url}/repos/{repo_owner}/{repo_name}/check-runs"
        
        # Determine check status
        if test_results.get('coverage_percentage', 0) >= 80:
            conclusion = 'success'
            summary = f"✅ Generated {test_results['test_count']} test cases with {test_results['coverage_percentage']:.1f}% coverage"
        elif test_results.get('coverage_percentage', 0) >= 60:
            conclusion = 'neutral'
            summary = f"⚠️ Generated {test_results['test_count']} test cases with {test_results['coverage_percentage']:.1f}% coverage (consider adding more tests)"
        else:
            conclusion = 'failure'
            summary = f"❌ Low test coverage: {test_results['coverage_percentage']:.1f}% (minimum 60% recommended)"
        
        payload = {
            'name': 'Test Case Generator',
            'head_sha': commit_sha,
            'status': 'completed',
            'conclusion': conclusion,
            'output': {
                'title': 'Test Generation Results',
                'summary': summary,
                'text': self._format_check_details(test_results)
            }
        }
        
        try:
            response = requests.post(url, headers=self.headers, json=payload)
            response.raise_for_status()
            print(f"Created check run for commit {commit_sha}")
            
        except requests.RequestException as e:
            print(f"Error creating check run: {e}")
    
    def _format_check_details(self, test_results: Dict[str, Any]) -> str:
        """Format detailed test results for check run."""
        
        details = f"## Test Generation Summary\\n\\n"
        details += f"- **Total Test Cases Generated:** {test_results['test_count']}\\n"
        details += f"- **Estimated Coverage:** {test_results['coverage_percentage']:.1f}%\\n"
        details += f"- **Language:** {test_results['analysis'].language}\\n"
        details += f"- **Functions Analyzed:** {len(test_results['analysis'].functions)}\\n\\n"
        
        if test_results['analysis'].edge_cases:
            details += "### Edge Cases Detected\\n"
            for edge_case in test_results['analysis'].edge_cases:
                details += f"- {edge_case}\\n"
            details += "\\n"
        
        if test_results['analysis'].performance_risks:
            details += "### Performance Risks Identified\\n"
            for risk in test_results['analysis'].performance_risks:
                details += f"- {risk}\\n"
            details += "\\n"
        
        details += "### Test Types Generated\\n"
        test_types = {}
        for test in test_results['tests']:
            test_types[test.test_type.value] = test_types.get(test.test_type.value, 0) + 1
        
        for test_type, count in test_types.items():
            details += f"- **{test_type.title()}:** {count} tests\\n"
        
        return details