Metadata-Version: 2.4
Name: steadytext
Version: 2025.9.21
Summary: Deterministic text generation and embedding with zero configuration
Project-URL: Homepage, https://github.com/julep-ai/steadytext
Project-URL: Bug Tracker, https://github.com/julep-ai/steadytext/issues
Author-email: SteadyText Community <singh@diwank.name>
License: MIT License
        
        Copyright (c) 2025 SteadyText Community
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
License-File: LICENSE
License-File: LICENSE-GEMMA.txt
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Typing :: Typed
Requires-Python: <3.14,>=3.10
Requires-Dist: chonkie>=0.2.1
Requires-Dist: click>=8.0.0
Requires-Dist: faiss-cpu>=1.7.0
Requires-Dist: httpx>=0.27.2
Requires-Dist: huggingface-hub>=0.32.0
Requires-Dist: llama-cpp-python>=0.3.16
Requires-Dist: numpy>=1.21.0
Requires-Dist: openai>=1.102.0
Requires-Dist: pydantic>=2.11.7
Requires-Dist: pyzmq>=22.0.0
Requires-Dist: requests>=2.32.5
Requires-Dist: tomli-w>=1.2.0
Requires-Dist: tomli>=2.2.0
Requires-Dist: tqdm>=4
Requires-Dist: voyageai>=0.3.4
Provides-Extra: benchmark
Requires-Dist: matplotlib; extra == 'benchmark'
Requires-Dist: pandas; extra == 'benchmark'
Requires-Dist: psutil; extra == 'benchmark'
Requires-Dist: tabulate; extra == 'benchmark'
Provides-Extra: test
Requires-Dist: pytest; extra == 'test'
Requires-Dist: pytest-cov; extra == 'test'
Requires-Dist: pytest-xdist; extra == 'test'
Description-Content-Type: text/markdown

<p align="center">
    <img src="https://github.com/user-attachments/assets/735141f8-56ff-40ce-8a4e-013dbecfe299" alt="SteadyText Logo" height=320 width=480 />
</p>

# SteadyText

*Deterministic text generation and embeddings with zero configuration*

[![](https://img.shields.io/pypi/v/steadytext.svg)](https://pypi.org/project/steadytext/)
[![](https://img.shields.io/pypi/pyversions/steadytext.svg)](https://pypi.org/project/steadytext/)
[![](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

> [!IMPORTANT]
> **Version 2025.8.17 - Date-Based Versioning**
> 
> SteadyText has transitioned from semantic versioning to **date-based versioning** (yyyy.mm.dd format).
> 
> **Why this change?** The rapid pace of AI model improvements and feature additions made traditional semantic versioning impractical. With models evolving weekly and new capabilities being added frequently, date-based versioning provides clearer insight into release recency and better aligns with our continuous improvement philosophy.
> 
> This applies to both the Python package and the PostgreSQL extension (pg_steadytext). Existing installations can upgrade normally using pip or PostgreSQL extension commands.

**Same input ‚Üí same output. Every time.**
No more flaky tests, unpredictable CLI tools, or inconsistent docs. SteadyText makes AI outputs as reliable as hash functions.

Ever had an AI test fail randomly? Or a CLI tool give different answers each run? SteadyText makes AI outputs reproducible - perfect for testing, tooling, and anywhere you need consistent results.

> [!TIP]
> ‚ú® _Powered by open-source AI workflows from [**Julep**](https://julep.ai)._ ‚ú®

---

## üöÄ Quick Start

### Installing with UV (Recommended - 10-100x faster)

```bash
# Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install SteadyText
uv add steadytext
```

### Installing with pip

```bash
pip install steadytext
```

### Installing from Source (for development or llama-cpp-python build issues)

Due to the specific build requirements for the inference-sh fork of llama-cpp-python, you may need to install from source:

```bash
# Clone the repository
git clone https://github.com/julep-ai/steadytext.git
cd steadytext

# Set required environment variables
export FORCE_CMAKE=1
export CMAKE_ARGS="-DLLAVA_BUILD=OFF -DGGML_ACCELERATE=OFF -DGGML_BLAS=OFF -DGGML_CUDA=OFF -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF"

# Install with UV (recommended)
uv sync

# Or install with pip
pip install -e .
```

```python
import steadytext

# Deterministic text generation (uses daemon by default)
code = steadytext.generate("implement binary search in Python")
assert "def binary_search" in code  # Always passes!

# Choose model size (small=4B, large=30B)
quick = steadytext.generate("summarize this", size="small")  # Fast
detailed = steadytext.generate("explain in detail", size="large")  # Higher quality

# Generate with controlled randomness (temperature > 0)
creative = steadytext.generate("write a poem", temperature=0.8)
more_creative = steadytext.generate("write a poem", temperature=1.2)

# Streaming (also deterministic)
for token in steadytext.generate_iter("explain quantum computing"):
    print(token, end="", flush=True)

# Deterministic embeddings (uses Jina v4 by default)
vec = steadytext.embed("What is machine learning?")  # 1024-dimensional embedding
# Note: Jina v4 outputs 2048 dimensions but we truncate to 1024 for backward compatibility

# Explicit daemon usage (ensures connection)
from steadytext.daemon import use_daemon
with use_daemon():
    code = steadytext.generate("implement quicksort")
    embedding = steadytext.embed("machine learning")

# Model switching (v2.0.0+)
fast_response = steadytext.generate("Quick task", size="small")  # Qwen3-4B (default)
quality_response = steadytext.generate("Complex analysis", size="large")  # Qwen3-30B

# Size-based selection (v2025.8.16+)
small = steadytext.generate("Simple task", size="small")      # Qwen3-4B (default)
large = steadytext.generate("Complex task", size="large")    # Qwen3-30B
```

_Or,_

```bash
echo "hello" | uvx steadytext
```

---

## üìú License Notice

The default generation models (Qwen3 family) are MIT licensed and open source. These models provide high-quality text generation with full commercial usage rights.

**Note:** The library supports various models including Gemma models which have their own [Terms of Use](https://ai.google.dev/gemma/terms).

---

## üêò PostgreSQL Extension

Transform your PostgreSQL database into an AI-powered system with **pg_steadytext** - the production-ready PostgreSQL extension that brings deterministic AI directly to your SQL queries.

### Key Features

- **Native SQL Functions**: Generate text and embeddings using simple SQL commands
- **Async Processing**: Non-blocking AI operations with queue-based background workers  
- **AI Summarization**: Enhanced aggregate functions (`steadytext_summarize`) with remote model support and TimescaleDB compatibility (v2025.8.17)
- **Structured Generation**: Generate JSON, regex-constrained text, and multiple-choice outputs
- **pgvector Integration**: Seamless compatibility for similarity search and vector operations
- **Built-in Caching**: PostgreSQL-based frecency cache that mirrors SteadyText's performance
- **Schema Qualification**: Full support for TimescaleDB continuous aggregates and custom schemas (v2025.8.17)

### Quick Example

```sql
-- Generate text
SELECT steadytext_generate('Write a product description for wireless headphones');

-- Create embeddings for similarity search
SELECT steadytext_embed('machine learning') <-> steadytext_embed('artificial intelligence');

-- AI-powered summarization (v2025.8.17+)
SELECT steadytext_summarize(content) AS summary
FROM documents
WHERE created_at > NOW() - INTERVAL '1 day'
GROUP BY category;

-- Or use short alias
SELECT st_summarize(content, 
    jsonb_build_object('max_facts', 10, 'model', 'openai:gpt-4o-mini', 'unsafe_mode', true)) 
FROM documents;

-- Structured JSON generation
SELECT steadytext_generate_json(
    'Create a user profile',
    '{"type": "object", "properties": {"name": {"type": "string"}, "age": {"type": "integer"}}}'::jsonb
);
```

üìö **[Full PostgreSQL Extension Documentation ‚Üí](pg_steadytext/)**

---

## üîß How It Works

SteadyText achieves determinism via:

* **Customizable seeds:** Control determinism with a `seed` parameter, while still defaulting to `42`.
* **Greedy decoding:** Always chooses highest-probability token
* **Frecency cache:** LRU cache with frequency counting‚Äîpopular prompts stay cached longer
* **Quantized models:** 8-bit quantization ensures identical results across platforms
* **Model switching:** Dynamically switch between models while maintaining determinism (v1.0.0+)
* **Daemon architecture:** Persistent model serving eliminates loading overhead (v1.2.0+)

This means `generate("hello")` returns the exact same 512 tokens on any machine, every single time.

## üåê Ecosystem

SteadyText is more than just a library. It's a full ecosystem for deterministic AI:

- **Python Library**: The core `steadytext` library for programmatic use in your applications.
- **Command-Line Interface (CLI)**: A powerful `st` command to use SteadyText from your shell for scripting and automation.
- **PostgreSQL Extension (pg_steadytext)**: Production-ready extension with async processing, AI summarization, and structured generation for SQL-native AI operations.
- **Zsh Plugin**: Supercharge your shell with AI-powered command suggestions and history search.
- **Cloudflare Worker**: Deploy SteadyText to the edge with a Cloudflare Worker for distributed, low-latency applications.

### ‚ö° Daemon Architecture (Optional but Recommended)

SteadyText includes a daemon mode that provides significantly better performance:

* **Persistent serving:** Models stay loaded in memory between requests
* **Zero loading overhead:** Skip the 2-3 second model loading time on each call
* **Explicit startup required:** Start with `st daemon start` for best performance
* **Automatic fallback:** Gracefully falls back to direct model loading if daemon not running
* **Centralized caching:** Consistent cache behavior between daemon and direct access

```bash
# Start the daemon for better performance
st daemon start

# Now all operations will use the daemon
echo "Hello world" | st  # Much faster with daemon running!
```

```python
# Python automatically uses daemon if available
text = steadytext.generate("Hello world")  # Fast if daemon is running

# Explicit daemon usage (ensures connection)
from steadytext.daemon import use_daemon
with use_daemon():
    text = steadytext.generate("Hello world")
    embedding = steadytext.embed("Some text")

# Disable daemon globally
import os
os.environ["STEADYTEXT_DISABLE_DAEMON"] = "1"
text = steadytext.generate("Hello world")  # Direct model loading
```

---

## Fun Goofy Sample You Can Try

They will (literally) **always** do this...

```bash
‚ùØ st generate --verbose --size large --eos-string STOP "DON'T SAY STOP (ALL CAPS) NO MATTER WHAT"
Understood. I will not use the word "%
```

> [!NOTE]
> This is by far the single best explanation of recursion in existence.

```bash
$> echo "explain recursion in pig latin" | st --verbose --size large

Okay, let's explain recursion in Pig Latin!  It's a bit tricky to do *in* Pig Latin, but I'll try to explain the concept and then give a Pig Latin-flavored analogy.

**What is Recursion? (In English)**

Recursion is like a set of instructions that calls *itself* to solve a smaller version of the same problem. Think of it like Russian nesting dolls (Matryoshka dolls). Each doll contains a smaller version of itself.

Here's the breakdown:

1. **Base Case:**  This is the *stopping point*.  It's the simplest version of the problem that you can solve directly, *without* calling the function again.  Without a base case, the recursion would go on forever (like an infinite loop!).

2. **Recursive Step:** This is where the function calls *itself*, but with a slightly modified (smaller) version of the original problem.  Each call gets closer to the base case.

**Example (in English):**

Let's say you want to calculate the factorial of a number (e.g., 5! = 5 * 4 * 3 * 2 * 1 = 120).  You can do this recursively:

* **Base Case:** If the number is 1, the factorial is 1.
* **Recursive Step:**  If the number is greater than 1, the factorial is the number multiplied by the factorial of the number minus 1.

**Pig Latin Analogy (Trying to explain it *using* Pig Latin):**

Okay, this is where it gets fun (and a little silly)!  Let's say we want to count the number of "ay" sounds in a word.

Here's how we could *imagine* a recursive Pig Latin function to do this:

\```piglatin
"Ehay-ay"  ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-ay-%
```

---

## ‚ú® Structured Generation (v2.4.1+)

SteadyText now supports structured generation using llama.cpp's native grammar support, allowing you to force the model's output to conform to a specific format.

- **JSON Generation**: Generate JSON that validates against a schema or Pydantic model.
- **Regex Matching**: Constrain output to a regular expression.
- **Multiple Choice**: Force the output to be one of a list of choices.

### Python API

```python
import steadytext
from pydantic import BaseModel

# JSON generation with a Pydantic model
class User(BaseModel):
    name: str
    email: str

user_json = steadytext.generate(
    "Create a user: name John Doe, email john.doe@example.com",
    schema=User,
    temperature=0.0  # Fully deterministic (default)
# Output contains: <json-output>{"name": "John Doe", "email": "john.doe@example.com"}</json-output>

# Regex-constrained generation
phone = steadytext.generate("My number is ", regex=r"\(\d{3}\) \d{3}-\d{4}")
# Output: (123) 456-7890

# Multiple choice
response = steadytext.generate("Is this useful?", choices=["Yes", "No"])
# Output: Yes
```

### CLI Support

```bash
# JSON generation with schema
echo "Create a person" | st --schema '{"type": "object", "properties": {"name": {"type": "string"}, "age": {"type": "integer"}}}' --wait

# JSON from schema file
echo "Generate user data" | st --schema user_schema.json --wait

# Regex pattern matching
echo "My phone is" | st --regex '\d{3}-\d{3}-\d{4}' --wait

# Multiple choice selection
echo "Is Python good?" | st --choices "yes,no,maybe" --wait
```

üìö **[Learn more in the Structured Generation Guide](docs/structured-generation.md)**

---

## ‚ö†Ô∏è Unsafe Mode: Remote Models (Experimental)

SteadyText now supports remote AI models (OpenAI, Cerebras, VoyageAI, Jina) with **best-effort determinism** via seed parameters. This feature is explicitly marked as "unsafe" because remote models cannot guarantee reproducibility.

**New in v2025.8.17+:**
- **GPT-5 Series Support**: Full support for OpenAI's GPT-5 reasoning models (gpt-5-mini, gpt-5-pro, etc.) with automatic temperature adjustment
- **Custom Provider Options**: Pass provider-specific parameters directly to remote APIs via the `options` parameter

### Why Use Unsafe Mode?

- Access to larger, more capable models
- Prototype before switching to local models
- Use when true determinism isn't critical

### Quick Example

```bash
# Install OpenAI client (required for OpenAI models)
pip install openai
# or
pip install steadytext[unsafe]

# Enable unsafe mode
export STEADYTEXT_UNSAFE_MODE=true

# Use OpenAI for generation
echo "Explain quantum computing" | st --unsafe-mode --model openai:gpt-4o-mini

# Use GPT-5 reasoning models (temperature automatically set to 1.0)
echo "Solve this problem" | st --unsafe-mode --model openai:gpt-5-mini
echo "Complex reasoning" | st --unsafe-mode --model openai:gpt-5-pro

# Pass custom options to providers (v2025.8.17+)
echo "Creative writing" | st --unsafe-mode --model openai:gpt-4o-mini --options '{"top_p": 0.95, "presence_penalty": 0.5}'

# Use Cerebras for fast generation
echo "Write Python code" | st --unsafe-mode --model cerebras:llama3.1-8b

# Use VoyageAI for embeddings
echo "Advanced search query" | st embed --unsafe-mode --model voyageai:voyage-3

# Use Jina for multilingual embeddings
echo "‰Ω†Â•Ω‰∏ñÁïå" | st embed --unsafe-mode --model jina:jina-embeddings-v3

# List available remote models
st unsafe list-models
```

### Python API

```python
import os
import steadytext

# Enable unsafe mode
os.environ["STEADYTEXT_UNSAFE_MODE"] = "true"

# Generate with OpenAI (requires OPENAI_API_KEY)
text = steadytext.generate(
    "Explain AI", 
    model="openai:gpt-4o-mini",
    seed=42  # Best-effort determinism only
)

# Use GPT-5 reasoning models (temperature automatically adjusted to 1.0)
text = steadytext.generate(
    "Complex reasoning task",
    model="openai:gpt-5-mini",
    unsafe_mode=True
)

# Pass custom provider options (v2025.8.17+)
text = steadytext.generate(
    "Creative writing",
    model="openai:gpt-4o-mini",
    unsafe_mode=True,
    options={"top_p": 0.95, "presence_penalty": 0.5}
)
```

### Supported Remote Providers

| Provider | Generation | Embeddings | Determinism | API Key |
|----------|------------|------------|-------------|---------|
| **OpenAI** | ‚úÖ gpt-4o, gpt-4o-mini<br>‚úÖ gpt-5-mini, gpt-5-pro (reasoning) | ‚úÖ text-embedding-3-* | Best-effort (seed param) | `OPENAI_API_KEY` |
| **Cerebras** | ‚úÖ Llama models | ‚ùå | Best-effort (seed param) | `CEREBRAS_API_KEY` |
| **VoyageAI** | ‚ùå | ‚úÖ voyage-3, voyage-large-2 | No seed support | `VOYAGE_API_KEY` |
| **Jina AI** | ‚ùå | ‚úÖ jina-embeddings-v3/v2 | No seed support | `JINA_API_KEY` |

‚ö†Ô∏è **WARNING**: Remote models may produce different outputs despite using the same seed. Use local GGUF models for guaranteed determinism.

üìö **[Learn more in the Unsafe Mode Guide](docs/unsafe-mode.md)**

---

## üì¶ Installation & Models

Install stable release:

```bash
pip install steadytext
```

#### Models

**Default models (v2025.8.17+)**:

* Generation (Small): `Qwen3-4B-Instruct` (3.9GB) - High-quality 4B parameter model
* Generation (Large): `Qwen3-30B-A3B-Instruct` (12GB) - Advanced 30B parameter model with A3B architecture  
* Embeddings: `Jina-v4-Text-Retrieval` (1.2GB) - State-of-the-art 2048-dim embeddings (truncated to 1024)
* Reranking: `Qwen3-Reranker-4B` (3.5GB) - Document reranking model

**Dynamic model switching (v1.0.0+):**

Switch between different models at runtime:

```python
# Use built-in model registry
text = steadytext.generate("Hello", size="large")  # Uses Qwen3-30B

# Use size parameter for model selection
text = steadytext.generate("Hello", size="small")  # Uses Qwen3-4B (default)
text = steadytext.generate("Hello", size="large")  # Uses Qwen3-30B

# Or specify custom models
text = steadytext.generate(
    "Hello",
    model_repo="unsloth/Qwen3-4B-Instruct-2507-GGUF",
    model_filename="Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf"
)
```

Available models: Qwen3 models in 4B and 30B variants

Size shortcuts: `mini` (270M, CI/testing), `small` (4B, default), `large` (30B)

> Each model produces deterministic outputs. The default model remains fixed per major version.

## üöÄ Mini Models for CI/Testing

SteadyText includes support for "mini" models - extremely small models (~10x smaller) designed for fast CI testing and development environments where speed matters more than quality.

**üéØ Tests use mini models by default!** Running `poe test` or `pytest` via the project's configuration automatically uses mini models for faster testing.

### Mini Model Sizes
- **Generation**: Gemma-3-270M (~97MB) - Tiny but functional text generation (mini mode only)
- **Embedding**: BGE-large-en-v1.5 (~130MB) - Produces compatible 1024-dim embeddings
- **Reranking**: BGE-reranker-base (~300MB) - Basic reranking capabilities

### Usage in Testing

**Default test commands (use mini models automatically):**
```bash
# These commands use mini models by default (configured in pyproject.toml)
uv run poe test           # Run tests with mini models
uv run poe test-cov       # Run tests with coverage using mini models

# To test with full models instead:
uv run poe test-full      # Run tests with regular models
```

### Usage in CI

**Environment Variable (Recommended for CI):**
```bash
# Enable mini models globally
export STEADYTEXT_USE_MINI_MODELS=true
export STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true

# All operations will now use mini models
st "Generate some text"  # Uses gemma-mini-270m
st embed "Create embedding"  # Uses bge-embedding-mini
st rerank "query" "doc1" "doc2"  # Uses bge-reranker-mini
```

**CLI with --size flag:**
```bash
# Use mini models for specific commands
st --size mini "Quick test generation"
st embed --size mini "Test embedding"
st rerank --size mini "query" "document"

# Start daemon with mini models
st daemon start --size mini
```

**Python API:**
```python
# Use mini models programmatically
text = steadytext.generate("Test prompt", size="mini")
```

### CI Workflow Example

```yaml
name: Test with Mini Models
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: pip install -e .
      
      - name: Run tests with mini models
        env:
          STEADYTEXT_USE_MINI_MODELS: true
          STEADYTEXT_ALLOW_MODEL_DOWNLOADS: true
        run: |
          # Quick smoke test
          echo "Test" | st --size mini
          
          # Run full test suite
          pytest tests/
```

### Benefits
- ‚úÖ ~10x faster model downloads (530MB total vs 5GB+)
- ‚úÖ Faster test execution
- ‚úÖ Lower memory usage
- ‚úÖ Full API compatibility
- ‚úÖ Deterministic outputs maintained

> **Note**: Mini models trade quality for speed. Use them for testing and CI, not production.

## Version History

| Version | Key Features                                                                                                                            | Default Generation Model                               | Default Embedding Model                                | Default Reranking Model | Python Versions |
| :------ | :-------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------- | :----------------------------------------------------- | :---------------------- | :-------------- |
| **2025.8.x** | - **Date-Based Versioning**: Switched to yyyy.mm.dd format.<br>- **Qwen3 Models**: Using Qwen3-4B/30B for generation.<br>- **Jina v4 Embeddings**: State-of-the-art retrieval embeddings.<br>- **Enhanced PostgreSQL Extension**: AI summarization with remote model support.<br>- **Document Reranking**: Reranking functionality with `Qwen3-Reranker-4B` model. | `unsloth/Qwen3-4B-Instruct-2507-GGUF` (Qwen3-4B) / `Qwen3-30B-A3B` (large) | `jinaai/jina-embeddings-v4-text-retrieval-GGUF` (Jina v4) | `QuantFactory/Qwen3-Reranker-4B-GGUF` | `>=3.10, <3.14` |
| **1.x** | - **Model Switching**: Added support for switching models via environment variables.<br>- **Centralized Cache**: Unified cache system.<br>- **CLI Improvements**: Streaming by default, quiet output. | `Qwen/Qwen3-1.7B-GGUF` (Qwen3-1.7B-Q8_0.gguf) | `Qwen/Qwen3-Embedding-0.6B-GGUF` (Qwen3-Embedding-0.6B-Q8_0.gguf) | - | `>=3.10, <3.14` |
| **1.0-1.2** | - **Model Switching**: Added support for switching models via environment variables and a model registry.<br>- **Qwen3 Models**: Switched to `qwen3-1.7b` for generation.<br>- **Indexing**: Added support for FAISS indexing. | `Qwen/Qwen3-1.7B-GGUF` (Qwen3-1.7B-Q8_0.gguf) | `Qwen/Qwen3-Embedding-0.6B-GGUF` (Qwen3-Embedding-0.6B-Q8_0.gguf) | - | `>=3.10, <3.14` |
| **0.x** | - **Initial Release**: Deterministic text generation and embedding.                                                                      | `Qwen/Qwen1.5-0.5B-Chat-GGUF` (qwen1_5-0_5b-chat-q4_k_m.gguf) | `Qwen/Qwen1.5-0.5B-Chat-GGUF` (qwen1_5-0_5b-chat-q8_0.gguf) | - | `>=3.10`        |

### Breaking Changes in v2025.8.x

* **Date-based versioning:** Switched from semantic versioning to yyyy.mm.dd format
* **Qwen3 models:** Using Qwen3-4B and Qwen3-30B models for generation
* **Enhanced PostgreSQL functions:** Renamed ai_* functions to steadytext_* with st_* aliases
* **Thinking mode removed:** `thinking_mode` parameter and `--think` flag have been deprecated
* **Default output:** Default max tokens is 512

### Breaking Changes in v2.3.0+

* **Document Reranking:** Added reranking functionality with the Qwen3-Reranker-4B model
* **Reranking API:** New `steadytext.rerank()` function and `st rerank` CLI command

### Other Notable Changes

* **Daemon enabled by default:** Use `STEADYTEXT_DISABLE_DAEMON=1` to opt-out
* **Streaming by default:** CLI streams output by default, use `--wait` to disable
* **Quiet by default:** CLI is quiet by default, use `--verbose` for informational output
* **Centralized caching:** Cache system now shared between daemon and direct access
* **New CLI syntax:** Use `echo "prompt" | st` instead of `st generate "prompt"`

---

## ‚ö° Performance

SteadyText delivers deterministic AI with production-ready performance:

* **Text Generation**: 21.4 generations/sec (46.7ms latency)
* **Embeddings**: 104-599 embeddings/sec (single to batch-50)
* **Cache Speedup**: 48x faster for repeated prompts
* **Memory**: ~1.4GB models, 150-200MB runtime
* **100% Deterministic**: Same output every time, verified across 100+ test runs
* **Accuracy**: 69.4% similarity for related texts, correct ordering maintained

üìä **[Full benchmarks ‚Üí](docs/benchmarks.md)**

---

## üéØ Examples

Use SteadyText in tests or CLI tools for consistent, reproducible results:

```python
# Testing with reliable assertions
def test_ai_function():
    result = my_ai_function("test input")
    expected = steadytext.generate("expected output for 'test input'")
    assert result == expected  # No flakes!

# CLI tools with consistent outputs
import click

@click.command()
def ai_tool(prompt):
    print(steadytext.generate(prompt))
```

üìÇ **[More examples ‚Üí](examples/)**

---

## üñ•Ô∏è CLI Usage

### Daemon Management

```bash
# Daemon commands
st daemon start                    # Start daemon in background
st daemon start --foreground       # Run daemon in foreground
st daemon status                   # Check daemon status
st daemon status --json            # JSON status output
st daemon stop                     # Stop daemon gracefully
st daemon stop --force             # Force stop daemon
st daemon restart                  # Restart daemon

# Daemon configuration
st daemon start --host 127.0.0.1 --port 5678  # Custom host/port
```

### Text Generation

```bash
# Generate text (streams by default, uses daemon if running)
echo "write a hello world function" | st

# Generate with temperature for creativity
echo "write a poem" | st --temperature 0.8

# Disable streaming (wait for complete output)
echo "write a function" | st --wait

# Enable verbose output
echo "explain recursion" | st --verbose

# JSON output with metadata
echo "hello world" | st --json

# Get log probabilities
echo "predict next word" | st --logprobs
```

### Model Management

```bash
# List available models
st models list

# Download models
st models download --size small
st models download --model gemma-3n-4b
st models download --all

# Delete models
st models delete --size small
st models delete --model gemma-3n-4b
st models delete --all --force

# Preload models
st models preload
```

### Other Operations

```bash
# Get embeddings (local model - deterministic)
echo "machine learning" | st embed

# Remote embeddings (requires unsafe mode)
echo "machine learning" | st embed --model openai:text-embedding-3-small --unsafe-mode
echo "document text" | st embed --model voyageai:voyage-3-lite --unsafe-mode --json

# Document reranking (v2.3.0+)
st rerank "what is Python?" document1.txt document2.txt document3.txt
st rerank "search query" --file documents.txt --top-k 5 --json

# Vector operations
st vector similarity "cat" "dog"
st vector search "Python" candidate1.txt candidate2.txt candidate3.txt

# Create and search FAISS indices
st index create *.txt --output docs.faiss
st index search docs.faiss "how to install" --top-k 5

# Generate with automatic context from index
echo "what is the configuration?" | st --index-file docs.faiss

# Disable daemon for specific command
STEADYTEXT_DISABLE_DAEMON=1 echo "hello" | st

# Preload models
st models --preload
```

---

## üìã When to Use SteadyText

‚úÖ **Perfect for:**

* Testing AI features (reliable asserts)
* Deterministic CLI tooling
* Reproducible documentation & demos
* Offline/dev/staging environments
* Semantic caching and embedding search
* Vector similarity comparisons
* Document retrieval & RAG applications

‚ùå **Not ideal for:**

* Creative or conversational tasks
* Latest knowledge queries
* Large-scale chatbot deployments

---

## üîç API Overview

```python
# Text generation (uses daemon by default)
steadytext.generate(prompt: str, seed: int = 42, temperature: float = 0.0) -> str
steadytext.generate(prompt, return_logprobs=True, seed: int = 42, temperature: float = 0.0)


# Streaming generation
steadytext.generate_iter(prompt: str, seed: int = 42, temperature: float = 0.0)

# Embeddings (uses daemon by default)
steadytext.embed(text: str | List[str], seed: int = 42) -> np.ndarray

# Document reranking (v2.3.0+)
steadytext.rerank(
    query: str,
    documents: Union[str, List[str]],
    task: str = "Given a web search query, retrieve relevant passages that answer the query",
    return_scores: bool = True,
    seed: int = 42
) -> Union[List[Tuple[str, float]], List[str]]

# Daemon management
from steadytext.daemon import use_daemon
with use_daemon():  # Ensure daemon connection
    text = steadytext.generate("Hello")

# Model preloading
steadytext.preload_models(verbose=True)

# Cache management
from steadytext import get_cache_manager
cache_manager = get_cache_manager()
stats = cache_manager.get_cache_stats()
```

### Vector Operations (CLI)

```bash
# Compute similarity between texts
st vector similarity "text1" "text2" [--metric cosine|dot]

# Calculate distance between texts
st vector distance "text1" "text2" [--metric euclidean|manhattan|cosine]

# Find most similar text from candidates
st vector search "query" file1.txt file2.txt [--top-k 3]

# Average multiple text embeddings
st vector average "text1" "text2" "text3"

# Vector arithmetic
st vector arithmetic "king" - "man" + "woman"
```

### Index Management (CLI)

```bash
# Create FAISS index from documents
st index create doc1.txt doc2.txt --output my_index.faiss

# View index information
st index info my_index.faiss

# Search index
st index search my_index.faiss "query text" --top-k 5

# Use index with generation
echo "question" | st --index-file my_index.faiss
```

üìö [Full API Documentation](docs/api.md)

---

## üîß Configuration

### Cache Configuration

Control caching behavior via environment variables (affects both daemon and direct access):

```bash
# Generation cache (default: 256 entries, 50MB)
export STEADYTEXT_GENERATION_CACHE_CAPACITY=256
export STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50

# Embedding cache (default: 512 entries, 100MB)
export STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512
export STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100
```

### Daemon Configuration

```bash
# Disable daemon globally (use direct model loading)
export STEADYTEXT_DISABLE_DAEMON=1

# Daemon connection settings
export STEADYTEXT_DAEMON_HOST=127.0.0.1
export STEADYTEXT_DAEMON_PORT=5678
```

### Model Downloads

```bash
# Allow model downloads in tests
export STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true
```

---

## üìñ API Reference

### Text Generation

#### `generate(prompt: str, return_logprobs: bool = False, temperature: float = 0.0, model: Optional[str] = None, unsafe_mode: bool = False, options: Optional[Dict] = None) -> Union[str, Tuple[str, Optional[Dict]]]`

Generate text from a prompt with optional temperature control and remote model support.

```python
# Deterministic generation (default)
text = steadytext.generate("Write a haiku about Python")

# With controlled randomness
creative_text = steadytext.generate("Write a haiku about Python", temperature=0.8)

# With log probabilities
text, logprobs = steadytext.generate("Explain AI", return_logprobs=True)

# With remote models (v2025.8.17+)
text = steadytext.generate(
    "Solve this problem",
    model="openai:gpt-5-mini",
    unsafe_mode=True,
    options={"top_p": 0.9}  # Custom provider options
)
```

- **Parameters:**
  - `prompt`: Input text to generate from
  - `return_logprobs`: If True, returns tuple of (text, logprobs)
  - `temperature`: Controls randomness (0.0 = deterministic, >0 = more random)
  - `model`: Optional remote model (e.g., "openai:gpt-4o-mini", "openai:gpt-5-mini")
  - `unsafe_mode`: Enable remote models (non-deterministic)
  - `options`: Optional dict of provider-specific parameters (v2025.8.17+)
- **Returns:** Generated text string, or tuple if `return_logprobs=True`

#### `generate_iter(prompt: str, temperature: float = 0.0, model: Optional[str] = None, unsafe_mode: bool = False, options: Optional[Dict] = None) -> Iterator[str]`

Generate text iteratively, yielding tokens as they are produced.

```python
# Deterministic streaming
for token in steadytext.generate_iter("Tell me a story"):
    print(token, end="", flush=True)

# Creative streaming with temperature
for token in steadytext.generate_iter("Tell me a story", temperature=0.7):
    print(token, end="", flush=True)

# Streaming with remote models (v2025.8.17+)
for token in steadytext.generate_iter(
    "Explain reasoning",
    model="openai:gpt-5-mini",
    unsafe_mode=True
):
    print(token, end="", flush=True)
```

- **Parameters:**
  - `prompt`: Input text to generate from
  - `temperature`: Controls randomness (0.0 = deterministic, >0 = more random)
  - `model`: Optional remote model (e.g., "openai:gpt-4o-mini", "openai:gpt-5-mini")
  - `unsafe_mode`: Enable remote models (non-deterministic)
  - `options`: Optional dict of provider-specific parameters (v2025.8.17+)
- **Yields:** Text tokens/words as they are generated

### Embeddings

#### `embed(text_input: Union[str, List[str]], model: Optional[str] = None, unsafe_mode: bool = False) -> np.ndarray`

Create deterministic embeddings for text input, with optional remote provider support.

```python
# Local model (deterministic)
vec = steadytext.embed("Hello world")

# List of strings (averaged)
vecs = steadytext.embed(["Hello", "world"])

# Remote models (requires unsafe_mode)
vec = steadytext.embed(
    "Hello world",
    model="openai:text-embedding-3-small",
    unsafe_mode=True
)

vec = steadytext.embed(
    "Hello world", 
    model="voyageai:voyage-3-lite",
    unsafe_mode=True
)

vec = steadytext.embed(
    "Hello world",
    model="jina:jina-embeddings-v3",
    unsafe_mode=True
)
```

- **Parameters:**
  - `text_input`: String or list of strings to embed
  - `model`: Optional remote model (e.g., "openai:text-embedding-3-small", "voyageai:voyage-3-lite", "jina:jina-embeddings-v3")
  - `unsafe_mode`: Enable remote models (non-deterministic)
- **Returns:** 1024-dimensional L2-normalized numpy array (float32)

### Utilities

#### `preload_models(verbose: bool = False) -> None`

Preload models before first use.

```python
steadytext.preload_models()  # Silent
steadytext.preload_models(verbose=True)  # With progress
```

#### `get_model_cache_dir() -> str`

Get the path to the model cache directory.

```python
cache_dir = steadytext.get_model_cache_dir()
print(f"Models are stored in: {cache_dir}")
```

### Constants

```python
steadytext.DEFAULT_SEED  # 42
steadytext.GENERATION_MAX_NEW_TOKENS  # 512
steadytext.EMBEDDING_DIMENSION  # 1024
```

---

## ü§ù Contributing

Contributions are welcome!
See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## üìÑ License

* **Code:** MIT
* **Models:** MIT (Qwen3)

---
## üìà What's New

### Structured Generation (v2.4.1+)
- **Native llama.cpp grammar support** for JSON, regex, and choice constraints
- **PostgreSQL extension integration** - all structured generation features in SQL
- **Async structured generation** functions for high-performance applications

### PostgreSQL Extension (v1.1.0+)
- **Production-ready SQL functions** for text generation and embeddings
- **Async operations** with queue-based background processing
- **AI summarization** aggregate functions with TimescaleDB support
- **Structured generation** in SQL (JSON schemas, regex patterns, choices)
- **Docker support** for easy deployment

### Document Reranking (v2.3.0+)
- **Reranking support** using Qwen3-Reranker-4B model for query-document relevance scoring
- **Python API** - `steadytext.rerank()` function with customizable task descriptions
- **CLI command** - `st rerank` for command-line reranking operations
- **PostgreSQL functions** - SQL functions for reranking with async support (PostgreSQL extension v1.3.0+)
- **Fallback scoring** - simple word overlap when model unavailable
- **Dedicated cache** - separate frecency cache for reranking results

### Daemon Architecture (v1.2.0+)
- **Persistent model serving** with ZeroMQ for 10-100x faster repeated calls
- **Automatic fallback** to direct model loading when daemon unavailable
- **Explicit startup required** - start daemon with `st daemon start`
- **Background operation** - daemon runs silently in the background

### Centralized Cache System
- **Unified caching** - consistent behavior between daemon and direct access
- **Thread-safe SQLite backend** for reliable concurrent access
- **Shared cache files** across all access modes
- **Cache integration** with daemon server for optimal performance

### Improved CLI Experience
- **Streaming by default** - see output as it's generated
- **Quiet by default** - clean output without informational messages
- **New pipe syntax** - `echo "prompt" | st` for better unix integration
- **Daemon management** - built-in commands for daemon lifecycle

---

## üîß Troubleshooting

### Installation Issues

#### llama-cpp-python Build Errors

If you encounter build errors related to llama-cpp-python, especially with the error "Failed to load model", this is likely due to the package requiring the inference-sh fork with specific CMAKE flags:

```bash
# Set required environment variables before installation
export FORCE_CMAKE=1
export CMAKE_ARGS="-DLLAVA_BUILD=OFF -DGGML_ACCELERATE=OFF -DGGML_BLAS=OFF -DGGML_CUDA=OFF -DGGML_BUILD_TESTS=OFF -DGGML_BUILD_EXAMPLES=OFF"

# Then install
pip install steadytext

# Or install from source
git clone https://github.com/julep-ai/steadytext.git
cd steadytext
uv sync  # or pip install -e .
```

#### Model Loading Issues

If you see "Failed to load model from file" errors:

1. **Try fallback models**: Set `STEADYTEXT_USE_FALLBACK_MODEL=true`
2. **Clear model cache**: `rm -rf ~/.cache/steadytext/models/`
3. **Check disk space**: Models require ~2-4GB per model

### Common Issues

- **"No module named 'llama_cpp'"**: Reinstall with the CMAKE flags above
- **Daemon connection refused**: Check if daemon is running with `st daemon status`
- **Slow first run**: Models download on first use (~2-4GB)

---

Built with ‚ù§Ô∏è for developers tired of flaky AI tests.
