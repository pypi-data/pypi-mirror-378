{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cbf344",
   "metadata": {},
   "source": [
    "# CNN model\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e47803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook root to project root\n",
    "from helper_functions import set_project_root\n",
    "\n",
    "# Silence tensorflow, except for errors\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Run on the GTX1080 GPU - fastest single worker/small memory performance\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "set_project_root()\n",
    "\n",
    "# Standard library imports\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Local imports\n",
    "from ariel_data_preprocessing.data_preprocessing import DataProcessor\n",
    "import configuration as config\n",
    "\n",
    "# Make sure the figures directory exists\n",
    "figures_dir = f'{config.FIGURES_DIRECTORY}/model_training'\n",
    "Path(figures_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Make sure models directory exists\n",
    "Path(config.MODELS_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Best settings from ~400 Optuna optimization trials\n",
    "# (see model_training/optimize_cnn.py)\n",
    "sample_size = 372\n",
    "batch_size = 4\n",
    "steps = 431\n",
    "learning_rate = 0.0007103203515277739\n",
    "l_one = 0.9381346432258663\n",
    "l_two = 0.36282682418942663\n",
    "cnn_layers = 3\n",
    "first_filter_set = 73\n",
    "second_filter_set = 34\n",
    "third_filter_set = 48\n",
    "first_filter_size = 2\n",
    "second_filter_size = 5\n",
    "third_filter_size = 3\n",
    "dense_units = 104\n",
    "beta_one=0.72\n",
    "beta_two=0.93\n",
    "amsgrad=True\n",
    "weight_decay=0.016\n",
    "use_ema=True\n",
    "\n",
    "# Long training run\n",
    "epochs = 1000\n",
    "\n",
    "# Evaluation settings\n",
    "samples = 10   # Number of samples to draw per planet\n",
    "planets = 550  # Number of planets to evaluate\n",
    "\n",
    "# File names\n",
    "total_ksteps = int((epochs * steps) / 1000)\n",
    "model_save_file = f'{config.MODELS_DIRECTORY}/ariel-cnn-8.4M-{total_ksteps}ksteps.keras'\n",
    "training_results_save_file = f'{config.MODELS_DIRECTORY}/ariel-cnn-8.4M-{total_ksteps}ksteps.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfbc3e",
   "metadata": {},
   "source": [
    "## 1. Hyperparameter optimization results\n",
    "\n",
    "### 1.1. Load Optuna study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_study = optuna.load_study(\n",
    "    study_name='cnn_optimization',\n",
    "    storage=f'postgresql://{config.USER}:{config.PASSWD}@{config.HOST}:{config.PORT}/{config.STUDY_NAME}'\n",
    ")\n",
    "\n",
    "results_df = loaded_study.trials_dataframe()\n",
    "results_df = results_df[results_df['state'] == 'COMPLETE'] # Only keep completed trials\n",
    "results_df = results_df[results_df['value'] < 0.02] # Filter out extreme high loss values\n",
    "results_df.sort_values('value', ascending=True, inplace=True)\n",
    "\n",
    "param_columns = [\n",
    "    'params_batch_size', 'params_cnn_layers', 'params_dense_layers',\n",
    "    'params_first_dense_units', 'params_first_filter_set',\n",
    "    'params_first_filter_size', 'params_l_one', 'params_l_two',\n",
    "    'params_learning_rate', 'params_sample_size',\n",
    "    'params_second_dense_units', 'params_second_filter_set',\n",
    "    'params_second_filter_size', 'params_steps', 'params_third_dense_units',\n",
    "    'params_third_filter_set', 'params_third_filter_size'\n",
    "]\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15115fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5934067",
   "metadata": {},
   "source": [
    "### 1.2. Validation loss distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd270da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(config.STD_FIG_WIDTH/1.25, config.STD_FIG_WIDTH/1.5))\n",
    "plt.title('CNN hyperparameter optimization\\nvalidation RMSE distribution')\n",
    "plt.hist(\n",
    "    results_df['value'],\n",
    "    bins=30,\n",
    "    color='black',\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.xlabel('Validation RMSE')\n",
    "plt.ylabel('Number of Trials')\n",
    "# plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\n",
    "    f'{figures_dir}/03.2.1-validation_RMSE_distribution.jpg',\n",
    "    dpi=config.STD_FIG_DPI,\n",
    "    bbox_inches='tight'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d2327",
   "metadata": {},
   "source": [
    "### 1.3. Hyperparameter sampling distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53bcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 6, figsize=(16, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle('CNN hyperparameter distributions')\n",
    "fig.supxlabel('Hyperparameter value')\n",
    "fig.supylabel('Number of trials')\n",
    "\n",
    "for i, param in enumerate(param_columns):\n",
    "    axs[i].set_title(param.replace('params_', ''))\n",
    "    axs[i].hist(\n",
    "        results_df[param],\n",
    "        bins=20,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "axs[-1].axis('off')  # Turn off the last unused subplot\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\n",
    "    f'{figures_dir}/03.2.2-hyperparameter_distributions.jpg',\n",
    "    dpi=config.STD_FIG_DPI,\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27e0ea",
   "metadata": {},
   "source": [
    "### 1.4. Hyperparameter heatmap matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96754a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots for the heatmap matrix\n",
    "n_params = len(param_columns)\n",
    "fig, axes = plt.subplots(n_params, n_params, figsize=(10, 9))\n",
    "\n",
    "# Get the range of validation loss values for consistent color scaling\n",
    "vmin, vmax = results_df['value'].min(), results_df['value'].max()\n",
    "\n",
    "# Create the heatmap matrix\n",
    "for i, param_y in enumerate(param_columns):\n",
    "    for j, param_x in enumerate(param_columns):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if i == j:\n",
    "\n",
    "            # For diagonal elements, show parameter distribution as histogram\n",
    "            ax.hist(\n",
    "                results_df[param_x],\n",
    "                bins=20,\n",
    "                alpha=0.7,\n",
    "                color='lightblue',\n",
    "                edgecolor='black'\n",
    "            )\n",
    "            \n",
    "            # Configure tick labels and marks: only show for bottom row (x) and first column (y)\n",
    "            if j == 0:  # Left edge\n",
    "                ax.set_ylabel(param_y.replace('params_', ''), rotation='horizontal', ha='right')\n",
    "\n",
    "            if i == n_params - 1:  # Bottom edge\n",
    "                ax.set_xlabel(param_x.replace('params_', ''), rotation='vertical')\n",
    "\n",
    "            # Hide all tick labels and marks\n",
    "            ax.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False, top=False, right=False)\n",
    "\n",
    "        else:\n",
    "            # Create interpolated heatmap of validation loss\n",
    "            x_data = results_df[param_x].values\n",
    "            y_data = results_df[param_y].values\n",
    "            z_data = results_df['value'].values\n",
    "            \n",
    "            # Create a regular grid for interpolation\n",
    "            x_min, x_max = x_data.min(), x_data.max()\n",
    "            y_min, y_max = y_data.min(), y_data.max()\n",
    "            \n",
    "            # Add small margins to avoid edge effects\n",
    "            x_margin = (x_max - x_min) * 0.05\n",
    "            y_margin = (y_max - y_min) * 0.05\n",
    "            \n",
    "            x_grid = np.linspace(x_min - x_margin, x_max + x_margin, 50)\n",
    "            y_grid = np.linspace(y_min - y_margin, y_max + y_margin, 50)\n",
    "            X_grid, Y_grid = np.meshgrid(x_grid, y_grid)\n",
    "            \n",
    "            # Interpolate using griddata\n",
    "            Z_interp = griddata(\n",
    "                (x_data, y_data), z_data, (X_grid, Y_grid), \n",
    "                method='linear', fill_value=np.nan\n",
    "            )\n",
    "            \n",
    "            # Standardize the heatmap using its own min/max values\n",
    "            z_min_local = np.nanmin(Z_interp)\n",
    "            z_max_local = np.nanmax(Z_interp)\n",
    "            \n",
    "            # Create the interpolated heatmap with individual scaling\n",
    "            im = ax.imshow(Z_interp, origin='lower', aspect='auto', \n",
    "                cmap='viridis', vmin=z_min_local, vmax=z_max_local,\n",
    "            )\n",
    "            \n",
    "            # Set labels only for edge subplots to avoid clutter\n",
    "            if j == 0:  # Left edge\n",
    "                ax.set_ylabel(param_y.replace('params_', ''), rotation='horizontal', ha='right')\n",
    "\n",
    "            if i == n_params - 1:  # Bottom edge\n",
    "                ax.set_xlabel(param_x.replace('params_', ''), rotation='vertical')\n",
    "\n",
    "            # Hide all tick labels and marks\n",
    "            ax.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False, top=False, right=False)\n",
    "\n",
    "# Add a single colorbar to the left of the figure showing the global range\n",
    "# Note: Individual heatmaps are standardized to their own ranges\n",
    "cbar_ax = fig.add_axes([0.02, 0.08, 0.05, 0.87])  # [left, bottom, width, height]\n",
    "\n",
    "# Create a dummy mappable for the global colorbar\n",
    "norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "sm = cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "sm.set_array([])\n",
    "\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.ax.tick_params(labelsize=9)\n",
    "\n",
    "# Adjust layout to make room for the colorbar\n",
    "plt.subplots_adjust(left=0.27, right=0.95, top=0.95, bottom=0.08, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# Add a main title\n",
    "fig.suptitle(\n",
    "    'Validation RMSE Heatmap Matrix'\n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\n",
    "    f'{figures_dir}/03.2.3-hyperparameter_validation_RMSE_heatmaps.jpg',\n",
    "    dpi=config.STD_FIG_DPI,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc846628",
   "metadata": {},
   "source": [
    "## 2. Long training run\n",
    "\n",
    "### 2.1. Initialize data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e59990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = DataProcessor(\n",
    "    input_data_path=config.RAW_DATA_DIRECTORY,\n",
    "    output_data_path=config.PROCESSED_DATA_DIRECTORY,\n",
    "    mode='train',\n",
    ")\n",
    "\n",
    "data_preprocessor.initialize_data_generators(\n",
    "    sample_size=sample_size,\n",
    "    n_samples=samples,\n",
    "    validation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d586e99",
   "metadata": {},
   "source": [
    "### 2.2. CNN\n",
    "\n",
    "#### 2.2.1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(\n",
    "        samples: int=sample_size,\n",
    "        wavelengths: int=config.WAVELENGTHS,\n",
    "        learning_rate: float=learning_rate,\n",
    "        l1: float=l_one,\n",
    "        l2: float=l_two,\n",
    "        first_filter_set: int=first_filter_set,\n",
    "        second_filter_set: int=second_filter_set,\n",
    "        third_filter_set: int=third_filter_set,\n",
    "        first_filter_size: int=first_filter_size,\n",
    "        second_filter_size: int=second_filter_size,\n",
    "        third_filter_size: int=third_filter_size,\n",
    "        dense_units: int=dense_units,\n",
    "        beta_one: float=beta_one,\n",
    "        beta_two: float=beta_two,\n",
    "        amsgrad: bool=amsgrad,\n",
    "        weight_decay: float=weight_decay,\n",
    "        use_ema: bool=use_ema\n",
    ") -> tf.keras.Model:\n",
    "\n",
    "    '''Builds the convolutional neural network regression model'''\n",
    "\n",
    "    # Set-up the L1L2 for the dense layers\n",
    "    regularizer = tf.keras.regularizers.L1L2(l1=l1, l2=l2)\n",
    "\n",
    "    # Define the model layers in order\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input((samples,wavelengths,1)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            first_filter_set,\n",
    "            first_filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            second_filter_set,\n",
    "            second_filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            third_filter_set,\n",
    "            third_filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            dense_units,\n",
    "            kernel_regularizer=regularizer,\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.Dense(wavelengths, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=beta_one,\n",
    "        beta_2=beta_two,\n",
    "        amsgrad=amsgrad,\n",
    "        weight_decay=weight_decay,\n",
    "        use_ema=use_ema\n",
    "    )\n",
    "\n",
    "    # Compile the model, specifying the type of loss to use during training \n",
    "    # and any extra metrics to evaluate\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.MeanSquaredError(name='MSE'),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.RootMeanSquaredError(name='RMSE')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83900f61",
   "metadata": {},
   "source": [
    "#### 2.2.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(model_save_file).exists() and Path(training_results_save_file).exists():\n",
    "\n",
    "    print(f'Found existing model for {total_ksteps} ksteps, skipping training.')\n",
    "\n",
    "    # Load the existing model\n",
    "    model = tf.keras.models.load_model(model_save_file)\n",
    "\n",
    "    # Load existing training results\n",
    "    with open(training_results_save_file, 'rb') as input_file:\n",
    "        training_results = pickle.load(input_file)\n",
    "\n",
    "else:\n",
    "\n",
    "  print(f'Training model for {total_ksteps} ksteps')\n",
    "  start_time = time.time()\n",
    "\n",
    "  training_results = model.fit(\n",
    "    data_preprocessor.training.batch(batch_size),\n",
    "    validation_data=data_preprocessor.validation.batch(batch_size),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps,\n",
    "    validation_steps=steps,\n",
    "    verbose=1\n",
    "  )\n",
    "\n",
    "  print(f'Training complete in {(time.time() - start_time)/60:.1f} minutes')\n",
    "  model.save(model_save_file)\n",
    "\n",
    "  with open(training_results_save_file, 'wb') as output_file:\n",
    "      pickle.dump(training_results, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up a 1x2 figure for accuracy and binary cross-entropy\n",
    "fig, axs=plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('CNN training curves', size='large')\n",
    "\n",
    "# Plot training and validation loss\n",
    "axs[0].set_title('Training loss (mean squared error)')\n",
    "axs[0].plot(np.array(training_results.history['loss']), alpha=0.5, label='Training')\n",
    "axs[0].plot(np.array(training_results.history['val_loss']), alpha=0.5, label='Validation')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('loss')\n",
    "# axs[0].set_ylim(21, 25)\n",
    "# axs[0].set_yscale('log')\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "# Plot training and validation RMSE\n",
    "axs[1].set_title('Root mean squared error')\n",
    "axs[1].plot(training_results.history['RMSE'], alpha=0.5, label='Training')\n",
    "axs[1].plot(training_results.history['val_RMSE'], alpha=0.5, label='Validation')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('RMSE')\n",
    "# axs[2].set_ylim(top=0.014)\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Show the plot\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f'{figures_dir}/03.2.4-ariel_cnn_training_curves_8.4M-{total_ksteps}ksteps.jpg',\n",
    "    dpi=config.STD_FIG_DPI,\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e0ef4",
   "metadata": {},
   "source": [
    "## 3. Model evaluation (validation set)\n",
    "\n",
    "### 3.1. Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ea55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = data_preprocessor.evaluation.take(planets)\n",
    "\n",
    "signals = np.array([element[0].numpy() for element in evaluation_data])\n",
    "spectra = np.array([element[1].numpy() for element in evaluation_data])\n",
    "\n",
    "print(f'Signals shape: {signals.shape}')\n",
    "print(f'Spectra shape: {spectra.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543fbef",
   "metadata": {},
   "source": [
    "### 3.2. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_predictions = []\n",
    "\n",
    "for planet in signals:\n",
    "    spectrum_predictions.append(model.predict(planet, batch_size=samples, verbose=0))\n",
    "\n",
    "spectrum_predictions = np.array(spectrum_predictions)\n",
    "spectrum_predictions_avg = np.mean(spectrum_predictions, axis=1).flatten()\n",
    "spectrum_predictions = spectrum_predictions.flatten()\n",
    "reference_spectra = spectra[:,0,:].flatten()\n",
    "spectra = spectra.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461785aa",
   "metadata": {},
   "source": [
    "### 3.3. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Predicted vs true spectral signals')\n",
    "plt.scatter(spectra, spectrum_predictions, s=10, alpha=0.5, color='black', label='Sample predictions')\n",
    "plt.scatter(reference_spectra, spectrum_predictions_avg, s=2.5, alpha=0.5, color='red', label='Averaged prediction')\n",
    "plt.xlabel('True spectral signal')\n",
    "plt.ylabel('Predicted spectral signal')\n",
    "plt.legend(loc='best', markerscale=2)\n",
    "\n",
    "plt.savefig(\n",
    "    f'{figures_dir}/03.2.5-ariel_cnn_predicted_vs_true_spectra-8.4M-{total_ksteps}ksteps.jpg',\n",
    "    dpi=config.STD_FIG_DPI,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.8.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
