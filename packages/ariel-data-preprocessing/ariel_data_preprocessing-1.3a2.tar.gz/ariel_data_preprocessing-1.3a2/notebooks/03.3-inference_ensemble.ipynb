{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc17c31",
   "metadata": {},
   "source": [
    "# Inference ensemble\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook root to project root\n",
    "from helper_functions import set_project_root\n",
    "\n",
    "# Silence tensorflow, except for errors\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Run on the GTX1080 GPU - fastest single worker/small memory performance\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "set_project_root()\n",
    "\n",
    "# Standard library imports\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import h5py\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Local imports\n",
    "import configuration as config\n",
    "\n",
    "# Make sure the figures directory exists\n",
    "figures_dir = f'{config.FIGURES_DIRECTORY}/model_training'\n",
    "Path(figures_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sample_size = 883  # Number of time points per sample\n",
    "samples = 10       # Number of samples to draw per planet\n",
    "planets = 550      # Number of planets to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1d9f5",
   "metadata": {},
   "source": [
    "## 1. Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_file = f'{config.MODELS_DIRECTORY}/optimized_cnn-13ksteps.keras'\n",
    "model = tf.keras.models.load_model(model_save_file)\n",
    "\n",
    "# You can then use the loaded model for prediction or other operations\n",
    "# For example, to print a summary of the model's architecture:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97984a8a",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "\n",
    "### 2.1. Training/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5adcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_ids_file = f'{config.METADATA_DIRECTORY}/planet_ids.pkl'\n",
    "\n",
    "if Path(planet_ids_file).exists():\n",
    "\n",
    "    with open(planet_ids_file, 'rb') as input_file:\n",
    "        planet_ids = pickle.load(input_file)\n",
    "        training_planet_ids = planet_ids['training']\n",
    "        validation_planet_ids = planet_ids['validation']\n",
    "\n",
    "    print('Loaded existing training/validation split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a9225",
   "metadata": {},
   "source": [
    "### 2.2. Prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79dd382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_data_loader(planet_ids: list, data_file: str, sample_size: int = 100, n_samples: int = 10):\n",
    "    '''Generator that yields signal, spectrum pairs for training/validation/testing.\n",
    "\n",
    "    Args:\n",
    "        planet_ids (list): List of planet IDs to include in the generator.\n",
    "        data_file (str): Path to the HDF5 file containing the data.\n",
    "        sample_size (int, optional): Number of frames to draw from each planet. Defaults to 100.\n",
    "    '''\n",
    "\n",
    "    with h5py.File(data_file, 'r') as hdf:\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            for planet_id in planet_ids:\n",
    "\n",
    "                signal = hdf[planet_id]['signal'][:]\n",
    "\n",
    "                samples = []\n",
    "                spectra = []\n",
    "\n",
    "                for _ in range(n_samples):\n",
    "\n",
    "                    indices = random.sample(range(signal.shape[0]), sample_size)\n",
    "                    samples.append(signal[sorted(indices), :])\n",
    "                    spectra.append(hdf[planet_id]['spectrum'][:])\n",
    "\n",
    "                yield np.array(samples), np.array(spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data_generator = partial(\n",
    "    prediction_data_loader,\n",
    "    planet_ids=validation_planet_ids,\n",
    "    data_file=f'{config.PROCESSED_DATA_DIRECTORY}/train.h5',\n",
    "    sample_size=sample_size,\n",
    "    n_samples=samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataset = tf.data.Dataset.from_generator(\n",
    "    prediction_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(samples, sample_size, config.WAVELENGTHS), dtype=tf.float64),\n",
    "        tf.TensorSpec(shape=(samples, config.WAVELENGTHS), dtype=tf.float64)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ad343",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = prediction_dataset.take(planets)\n",
    "\n",
    "signals = np.array([element[0].numpy() for element in validation_data])\n",
    "spectra = np.array([element[1].numpy() for element in validation_data])\n",
    "\n",
    "print(f'Signals shape: {signals.shape}')\n",
    "print(f'Spectra shape: {spectra.shape}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
