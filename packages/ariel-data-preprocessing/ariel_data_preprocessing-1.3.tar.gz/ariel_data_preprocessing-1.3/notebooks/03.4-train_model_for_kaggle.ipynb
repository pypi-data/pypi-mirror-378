{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcac6ba",
   "metadata": {},
   "source": [
    "# Train model for upload to Kaggle\n",
    "\n",
    "## Notebook set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e433ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook root to project root\n",
    "from helper_functions import set_project_root\n",
    "\n",
    "# Silence tensorflow, except for errors\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Run on the GTX1080 GPU - fastest single worker/small memory performance\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "set_project_root()\n",
    "\n",
    "# Standard library imports\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import h5py\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Local imports\n",
    "import configuration as config\n",
    "\n",
    "# Make sure the figures directory exists\n",
    "figures_dir = f'{config.FIGURES_DIRECTORY}/model_training'\n",
    "Path(figures_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Make sure models directory exists\n",
    "Path(f'{config.MODELS_DIRECTORY}/kaggle').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Best settings from ~400 Optuna optimization trials\n",
    "# (see model_training/optimize_cnn.py)\n",
    "sample_size = 883\n",
    "batch_size = 2\n",
    "steps = 133\n",
    "learning_rate = 0.0010050627801302561\n",
    "l_one = 0.08483065154593984\n",
    "l_two = 0.030378903662206216\n",
    "cnn_layers = 3\n",
    "first_filter_set = 88\n",
    "second_filter_set = 52\n",
    "third_filter_set = 31\n",
    "first_filter_size = 2\n",
    "second_filter_size = 4\n",
    "third_filter_size = 3\n",
    "dense_units = 52\n",
    "beta_one = 0.7226139295714885\n",
    "beta_two = 0.9265501644462477\n",
    "amsgrad = True\n",
    "weight_decay = 0.01645540788238239\n",
    "use_ema = True\n",
    "\n",
    "# Long training run\n",
    "epochs = 1000\n",
    "\n",
    "total_ksteps = int((epochs * steps) / 1000)\n",
    "model_save_file = f'{config.MODELS_DIRECTORY}/kaggle/ariel_cnn-6.3M-{total_ksteps}ksteps.keras'\n",
    "training_results_save_file = f'{config.MODELS_DIRECTORY}/kaggle/ariel_cnn-6.3M-{total_ksteps}ksteps.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5c8fc",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1. Load planet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b12222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corrected/extracted data for a sample planet\n",
    "with h5py.File(f'{config.PROCESSED_DATA_DIRECTORY}/train.h5', 'r') as hdf:\n",
    "    planet_ids = list(hdf.keys())\n",
    "\n",
    "print(f'Found {len(planet_ids)} planets in training data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff749d",
   "metadata": {},
   "source": [
    "### 1.2. Data loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(planet_ids: list, data_file: str, sample_size: int = 100):\n",
    "    '''Generator that yields signal, spectrum pairs for training/validation/testing.\n",
    "\n",
    "    Args:\n",
    "        planet_ids (list): List of planet IDs to include in the generator.\n",
    "        data_file (str): Path to the HDF5 file containing the data.\n",
    "        sample_size (int, optional): Number of frames to draw from each planet. Defaults to 100.\n",
    "    '''\n",
    "\n",
    "    with h5py.File(data_file, 'r') as hdf:\n",
    "\n",
    "        while True:\n",
    "            np.random.shuffle(planet_ids)\n",
    "            \n",
    "            for planet_id in planet_ids:\n",
    "\n",
    "                signal = hdf[planet_id]['signal'][:]\n",
    "                spectrum = hdf[planet_id]['spectrum'][:]\n",
    "\n",
    "                indices = random.sample(range(signal.shape[0]), sample_size)\n",
    "                sample = signal[sorted(indices), :]\n",
    "\n",
    "                yield sample, spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb4e52",
   "metadata": {},
   "source": [
    "### 1.3. Prefill the arguments to `data_loader()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c59a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_generator = partial(\n",
    "    data_loader,\n",
    "    planet_ids=planet_ids,\n",
    "    data_file=f'{config.PROCESSED_DATA_DIRECTORY}/train.h5',\n",
    "    sample_size=sample_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ba9a3",
   "metadata": {},
   "source": [
    "### 1.4. Create TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de249e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    training_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(sample_size, config.WAVELENGTHS), dtype=tf.float64),\n",
    "        tf.TensorSpec(shape=(config.WAVELENGTHS), dtype=tf.float64)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814b2e9",
   "metadata": {},
   "source": [
    "## 2. CNN model\n",
    "\n",
    "### 2.1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(\n",
    "        samples: int=sample_size,\n",
    "        wavelengths: int=config.WAVELENGTHS,\n",
    "        learning_rate: float=learning_rate,\n",
    "        l1: float=l_one,\n",
    "        l2: float=l_two,\n",
    "        first_filter_set: int=first_filter_set,\n",
    "        second_filter_set: int=second_filter_set,\n",
    "        third_filter_set: int=third_filter_set,\n",
    "        first_filter_size: int=first_filter_size,\n",
    "        second_filter_size: int=second_filter_size,\n",
    "        third_filter_size: int=third_filter_size,\n",
    "        dense_units: int=dense_units,\n",
    "        beta_one: float=beta_one,\n",
    "        beta_two: float=beta_two,\n",
    "        amsgrad: bool=amsgrad,\n",
    "        weight_decay: float=weight_decay,\n",
    "        use_ema: bool=use_ema\n",
    ") -> tf.keras.Model:\n",
    "\n",
    "    '''Builds the convolutional neural network regression model'''\n",
    "\n",
    "    # Set-up the L1L2 for the dense layers\n",
    "    regularizer = tf.keras.regularizers.L1L2(l1=l1, l2=l2)\n",
    "\n",
    "    # Define the model layers in order\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input((samples,wavelengths,1)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            first_filter_set,\n",
    "            first_filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            second_filter_set,\n",
    "            second_filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            third_filter_set,\n",
    "            third_filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            dense_units,\n",
    "            kernel_regularizer=regularizer,\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.Dense(wavelengths, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=beta_one,\n",
    "        beta_2=beta_two,\n",
    "        amsgrad=amsgrad,\n",
    "        weight_decay=weight_decay,\n",
    "        use_ema=use_ema\n",
    "    )\n",
    "\n",
    "    # Compile the model, specifying the type of loss to use during training \n",
    "    # and any extra metrics to evaluate\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.MeanSquaredError(name='MSE'),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.RootMeanSquaredError(name='RMSE')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ab3e5",
   "metadata": {},
   "source": [
    "### 2.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd38351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training model for {total_ksteps} ksteps')\n",
    "start_time = time.time()\n",
    "\n",
    "training_results = model.fit(\n",
    "  training_dataset.batch(batch_size),\n",
    "  epochs=epochs,\n",
    "  steps_per_epoch=steps,\n",
    "  validation_steps=steps,\n",
    "  verbose=0\n",
    ")\n",
    "\n",
    "print(f'Training complete in {(time.time() - start_time)/60:.1f} minutes')\n",
    "model.save(model_save_file)\n",
    "\n",
    "with open(training_results_save_file, 'wb') as output_file:\n",
    "    pickle.dump(training_results, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fdb41",
   "metadata": {},
   "source": [
    "### 2.3. Training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up a 1x2 figure for accuracy and binary cross-entropy\n",
    "fig, axs=plt.subplots(1,3, figsize=(12,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('CNN training curves', size='large')\n",
    "\n",
    "# Plot training and validation loss\n",
    "axs[0].set_title('Training loss (mean squared error)')\n",
    "axs[0].plot(np.array(training_results.history['loss']), alpha=0.5, label='Training')\n",
    "axs[0].plot(np.array(training_results.history['val_loss']), alpha=0.5, label='Validation')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('loss')\n",
    "# axs[0].set_ylim(21, 25)\n",
    "# axs[0].set_yscale('log')\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "# Plot training and validation MSE\n",
    "axs[1].set_title('Mean squared error')\n",
    "axs[1].plot(training_results.history['MSE'], alpha=0.5, label='Training')\n",
    "axs[1].plot(training_results.history['val_MSE'], alpha=0.5, label='Validation')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('MSE')\n",
    "# axs[1].set_ylim(top=0.014)\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Plot training and validation RMSE\n",
    "axs[2].set_title('Root mean squared error')\n",
    "axs[2].plot(training_results.history['RMSE'], alpha=0.5, label='Training')\n",
    "axs[2].plot(training_results.history['val_RMSE'], alpha=0.5, label='Validation')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('RMSE')\n",
    "# axs[2].set_ylim(top=0.014)\n",
    "axs[2].set_yscale('log')\n",
    "\n",
    "# Show the plot\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f'{figures_dir}/03.4.1-kaggle_model_training_curve_{total_ksteps}ksteps.jpg',\n",
    "    dpi=config.STD_FIG_DPI,\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
