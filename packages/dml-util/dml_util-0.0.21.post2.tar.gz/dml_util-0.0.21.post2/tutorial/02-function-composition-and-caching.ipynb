{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Function Composition and Caching\n",
    "\n",
    "In this tutorial, we'll explore how to build more complex computational workflows by composing functions together and understanding DaggerML's powerful caching system.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you've completed Tutorial 1 first! We'll be building on those concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import time\n",
    "\n",
    "from daggerml import Dml\n",
    "\n",
    "from dml_util import funkify\n",
    "\n",
    "# Create a DaggerML instance\n",
    "dml = Dml(repo=\"tutorial\", branch=\"main\")\n",
    "os.environ.update({\"DML_S3_BUCKET\": \"does-not-matter\", \"DML_S3_PREFIX\": \"does-not-matter\"})\n",
    "print(\"DaggerML instance ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Composition\n",
    "\n",
    "One of DaggerML's strengths is the ability to chain functions together, where the output of one function becomes the input to another, regardless of where each function is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a data processing pipeline\n",
    "\n",
    "@funkify\n",
    "def generate_data(dag):\n",
    "    \"\"\"Generate a list of random numbers.\"\"\"\n",
    "    import random\n",
    "\n",
    "    # Get parameters from arguments\n",
    "    size = dag.argv[1].value()\n",
    "    min_val = dag.argv[2].value() if len(dag.argv) > 2 else 1\n",
    "    max_val = dag.argv[3].value() if len(dag.argv) > 3 else 100\n",
    "\n",
    "    # Generate random data\n",
    "    random.seed(42)  # For reproducible results\n",
    "    dag.data = [random.randint(min_val, max_val) for _ in range(size)]\n",
    "    dag.size = size\n",
    "    dag.range = (min_val, max_val)\n",
    "\n",
    "    print(f\"Generated {size} numbers between {min_val} and {max_val}\")\n",
    "    return dag.data\n",
    "\n",
    "@funkify\n",
    "def filter_data(dag):\n",
    "    \"\"\"Filter numbers based on a condition.\"\"\"\n",
    "    data = dag.argv[1].value()\n",
    "    threshold = dag.argv[2].value()\n",
    "\n",
    "    # Filter the data\n",
    "    dag.original_count = len(data)\n",
    "    dag.filtered_data = [x for x in data if x >= threshold]\n",
    "    dag.filtered_count = len(dag.filtered_data)\n",
    "    dag.threshold = threshold\n",
    "\n",
    "    print(f\"Filtered {dag.original_count} numbers, kept {dag.filtered_count} >= {threshold}\")\n",
    "    return dag.filtered_data\n",
    "\n",
    "@funkify\n",
    "def compute_statistics(dag):\n",
    "    \"\"\"Compute comprehensive statistics.\"\"\"\n",
    "    data = dag.argv[1].value()\n",
    "\n",
    "    if not data:\n",
    "        dag.result = {\"error\": \"No data to process\"}\n",
    "        return dag.result\n",
    "\n",
    "    # Calculate statistics\n",
    "    dag.count = len(data)\n",
    "    dag.sum = sum(data)\n",
    "    dag.mean = dag.sum.value() / dag.count.value()\n",
    "    dag.sorted_data = sorted(data)\n",
    "\n",
    "    # Quartiles\n",
    "    n = dag.count.value()\n",
    "    if n % 2 == 1:\n",
    "        dag.median = dag.sorted_data[n//2]\n",
    "    else:\n",
    "        dag.median = (dag.sorted_data[n//2 - 1].value() + dag.sorted_data[n//2].value()) / 2\n",
    "    dag.q1 = dag.sorted_data[n//4]\n",
    "    dag.q3 = dag.sorted_data[3*n//4]\n",
    "\n",
    "    # Standard deviation\n",
    "    dag.std_dev = (sum((x - dag.mean.value()) ** 2 for x in data) / dag.count.value()) ** 0.5\n",
    "\n",
    "    dag.result = {\n",
    "        \"count\": dag.count,\n",
    "        \"sum\": dag.sum,\n",
    "        \"mean\": dag.mean,\n",
    "        \"median\": dag.median,\n",
    "        \"std_dev\": dag.std_dev,\n",
    "        \"q1\": dag.q1,\n",
    "        \"q3\": dag.q3,\n",
    "        \"min\": min(data),\n",
    "        \"max\": max(data)\n",
    "    }\n",
    "\n",
    "    return dag.result\n",
    "\n",
    "# Create a new DAG for our pipeline\n",
    "dag = dml.new(\"02-function-composition\", \"Demonstrating function composition and caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "\n",
    "Now let's chain these functions together to create a data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our functions to the DAG\n",
    "dag.generate_fn = generate_data\n",
    "dag.filter_fn = filter_data\n",
    "dag.stats_fn = compute_statistics\n",
    "\n",
    "# Build the pipeline step by step\n",
    "print(\"=== Step 1: Generate Data ===\")\n",
    "dag.raw_data = dag.generate_fn(50, 1, 100)  # 50 numbers between 1-100\n",
    "\n",
    "print(\"\\n=== Step 2: Filter Data ===\")\n",
    "dag.filtered_data = dag.filter_fn(dag.raw_data, 50)  # Keep numbers >= 50\n",
    "\n",
    "print(\"\\n=== Step 3: Compute Statistics ===\")\n",
    "dag.final_stats = dag.stats_fn(dag.filtered_data)\n",
    "\n",
    "print(\"\\n=== Pipeline Results ===\")\n",
    "print(f\"Raw data count: {len(dag.raw_data.value())}\")\n",
    "print(f\"Filtered data count: {len(dag.filtered_data.value())}\")\n",
    "print(f\"Final statistics: {dag.final_stats.value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Caching\n",
    "\n",
    "DaggerML automatically caches function results based on their inputs. This means if you call the same function with the same arguments, it will return the cached result instead of recomputing it.\n",
    "\n",
    "Let's demonstrate this with timing measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@funkify\n",
    "def slow_computation(dag):\n",
    "    \"\"\"A deliberately slow computation to demonstrate caching.\"\"\"\n",
    "    import time\n",
    "\n",
    "    input_value = dag.argv[1].value()\n",
    "\n",
    "    print(f\"Starting slow computation for input: {input_value}\")\n",
    "\n",
    "    # Simulate a slow computation\n",
    "    time.sleep(2)  # Wait 2 seconds\n",
    "\n",
    "    # Do some computation\n",
    "    dag.result = input_value ** 2 + input_value * 10\n",
    "\n",
    "    print(f\"Finished slow computation, result: {dag.result}\")\n",
    "    return dag.result\n",
    "\n",
    "# Add the slow function to our DAG\n",
    "dag.slow_fn = slow_computation\n",
    "\n",
    "print(\"=== First call (will be slow) ===\")\n",
    "start_time = time.time()\n",
    "dag.slow_result1 = dag.slow_fn(5)\n",
    "first_duration = time.time() - start_time\n",
    "print(f\"First call took: {first_duration:.2f} seconds\")\n",
    "print(f\"Result: {dag.slow_result1.value()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Second call with same input (should be fast due to caching) ===\")\n",
    "start_time = time.time()\n",
    "dag.slow_result2 = dag.slow_fn(5)  # Same input!\n",
    "second_duration = time.time() - start_time\n",
    "print(f\"Second call took: {second_duration:.2f} seconds\")\n",
    "print(f\"Result: {dag.slow_result2.value()}\")\n",
    "\n",
    "print(f\"\\nSpeedup: {first_duration/second_duration:.1f}x faster!\")\n",
    "\n",
    "# Verify the results are the same\n",
    "print(f\"Results are identical: {dag.slow_result1.value() == dag.slow_result2.value()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Third call with different input (will be slow again) ===\")\n",
    "start_time = time.time()\n",
    "dag.slow_result3 = dag.slow_fn(7)  # Different input\n",
    "third_duration = time.time() - start_time\n",
    "print(f\"Third call took: {third_duration:.2f} seconds\")\n",
    "print(f\"Result: {dag.slow_result3.value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing with Composition\n",
    "\n",
    "You can also create multiple parallel branches in your DAG that later combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "@funkify\n",
    "def process_branch_a(dag):\n",
    "    \"\"\"Process data one way.\"\"\"\n",
    "    data = dag.argv[1].value()\n",
    "    dag.result = [x * 2 for x in data]  # Double each number\n",
    "    return dag.result\n",
    "\n",
    "@funkify\n",
    "def process_branch_b(dag):\n",
    "    \"\"\"Process data another way.\"\"\"\n",
    "    data = dag.argv[1].value()\n",
    "    dag.result = [x + 10 for x in data]  # Add 10 to each number\n",
    "    return dag.result\n",
    "\n",
    "@funkify\n",
    "def combine_branches(dag):\n",
    "    \"\"\"Combine results from two branches.\"\"\"\n",
    "    branch_a = dag.argv[1].value()\n",
    "    branch_b = dag.argv[2].value()\n",
    "\n",
    "    # Combine by taking alternating elements\n",
    "    dag.combined = []\n",
    "    for i in range(max(len(branch_a), len(branch_b))):\n",
    "        if i < len(branch_a):\n",
    "            dag.combined.append(branch_a[i])\n",
    "        if i < len(branch_b):\n",
    "            dag.combined.append(branch_b[i])\n",
    "\n",
    "    dag.result = dag.combined\n",
    "    return dag.result\n",
    "\n",
    "# Add parallel processing functions\n",
    "dag.process_a_fn = process_branch_a\n",
    "dag.process_b_fn = process_branch_b\n",
    "dag.combine_fn = combine_branches\n",
    "\n",
    "# Create some test data\n",
    "dag.test_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Process in parallel branches\n",
    "with ThreadPoolExecutor(2) as executor:\n",
    "    future_a = executor.submit(dag.process_a_fn, dag.test_data, name=\"branch_a_result\")\n",
    "    future_b = executor.submit(dag.process_b_fn, dag.test_data, name=\"branch_b_result\")\n",
    "    future_a.result()\n",
    "    future_b.result()\n",
    "\n",
    "# Combine the results\n",
    "dag.combined_result = dag.combine_fn(dag.branch_a_result, dag.branch_b_result)\n",
    "\n",
    "print(\"=== Parallel Processing Results ===\")\n",
    "print(f\"Original data: {dag.test_data.value()}\")\n",
    "print(f\"Branch A (×2): {dag.branch_a_result.value()}\")\n",
    "print(f\"Branch B (+10): {dag.branch_b_result.value()}\")\n",
    "print(f\"Combined: {dag.combined_result.value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Data Flows\n",
    "\n",
    "Let's create a more complex example that shows how multiple functions can work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@funkify\n",
    "def create_datasets(dag):\n",
    "    \"\"\"Create multiple datasets for processing.\"\"\"\n",
    "    import random\n",
    "\n",
    "    random.seed(42)\n",
    "\n",
    "    dag.small_dataset = [random.randint(1, 20) for _ in range(10)]\n",
    "    dag.medium_dataset = [random.randint(1, 100) for _ in range(50)]\n",
    "    dag.large_dataset = [random.randint(1, 1000) for _ in range(100)]\n",
    "\n",
    "    dag.result = {\n",
    "        \"small\": dag.small_dataset,\n",
    "        \"medium\": dag.medium_dataset,\n",
    "        \"large\": dag.large_dataset\n",
    "    }\n",
    "    return dag.result\n",
    "\n",
    "@funkify\n",
    "def analyze_dataset(dag):\n",
    "    \"\"\"Analyze a single dataset.\"\"\"\n",
    "    name = dag.argv[1].value()\n",
    "    data = dag.argv[2].value()\n",
    "\n",
    "    dag.name = name\n",
    "    dag.size = len(data)\n",
    "    dag.mean = sum(data) / len(data)\n",
    "    dag.std = (sum((x - dag.mean.value()) ** 2 for x in data) / len(data)) ** 0.5\n",
    "\n",
    "    dag.result = {\n",
    "        \"name\": name,\n",
    "        \"size\": dag.size,\n",
    "        \"mean\": dag.mean,\n",
    "        \"std\": dag.std,\n",
    "        \"min\": min(data),\n",
    "        \"max\": max(data)\n",
    "    }\n",
    "    return dag.result\n",
    "\n",
    "# Create the complex workflow\n",
    "dag.create_datasets_fn = create_datasets\n",
    "dag.analyze_fn = analyze_dataset\n",
    "\n",
    "# Generate datasets\n",
    "dag.datasets = dag.create_datasets_fn()\n",
    "\n",
    "# Analyze each dataset\n",
    "dag.small_analysis = dag.analyze_fn(\"small\", dag.datasets[\"small\"])\n",
    "dag.medium_analysis = dag.analyze_fn(\"medium\", dag.datasets[\"medium\"])\n",
    "dag.large_analysis = dag.analyze_fn(\"large\", dag.datasets[\"large\"])\n",
    "\n",
    "# Collect all analyses\n",
    "dag.all_analyses = [dag.small_analysis, dag.medium_analysis, dag.large_analysis]\n",
    "\n",
    "print(\"=== Dataset Analysis Results ===\")\n",
    "for analysis in dag.all_analyses.value():\n",
    "    print(f\"{analysis['name']}: {analysis['size']} items, mean={analysis['mean']:.2f}, std={analysis['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Complex Data Structures\n",
    "\n",
    "DaggerML handles complex nested data structures gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex nested structure\n",
    "dag.complex_data = {\n",
    "    \"metadata\": {\n",
    "        \"version\": \"1.0\",\n",
    "        \"created_by\": \"tutorial\",\n",
    "        \"datasets\": [\"small\", \"medium\", \"large\"]\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"analyses\": dag.all_analyses,\n",
    "        \"summary\": {\n",
    "            \"total_datasets\": 3,\n",
    "            \"total_data_points\": sum(analysis[\"size\"] for analysis in dag.all_analyses.value()),\n",
    "            \"average_mean\": sum(analysis[\"mean\"] for analysis in dag.all_analyses.value()) / 3\n",
    "        }\n",
    "    },\n",
    "    \"raw_data\": dag.datasets\n",
    "}\n",
    "\n",
    "print(\"=== Complex Data Structure ===\")\n",
    "print(f\"Metadata version: {dag.complex_data['metadata']['version'].value()}\")\n",
    "print(f\"Total data points: {dag.complex_data['results']['summary']['total_data_points'].value()}\")\n",
    "print(f\"Average mean across datasets: {dag.complex_data['results']['summary']['average_mean'].value():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG Inspection and Debugging\n",
    "\n",
    "Let's look at what our DAG contains after all these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the DAG\n",
    "print(\"=== DAG Contents ===\")\n",
    "all_keys = sorted(dag.keys())\n",
    "print(f\"Total nodes in DAG: {len(all_keys)}\")\n",
    "\n",
    "# Group keys by type\n",
    "function_keys = [k for k in all_keys if k.endswith('_fn')]\n",
    "data_keys = [k for k in all_keys if not k.endswith('_fn') and not k.endswith('_result')]\n",
    "result_keys = [k for k in all_keys if k.endswith('_result')]\n",
    "\n",
    "print(f\"\\nFunctions ({len(function_keys)}):\")\n",
    "for key in function_keys:\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "print(f\"\\nData nodes ({len(data_keys)}):\")\n",
    "for key in data_keys[:10]:  # Show first 10\n",
    "    print(f\"  - {key}\")\n",
    "if len(data_keys) > 10:\n",
    "    print(f\"  ... and {len(data_keys) - 10} more\")\n",
    "\n",
    "print(f\"\\nResult nodes ({len(result_keys)}):\")\n",
    "for key in result_keys:\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Function Composition\n",
    "\n",
    "Here are some key insights from this tutorial:\n",
    "\n",
    "### 1. Cache-Aware Design\n",
    "- Functions with the same inputs will use cached results\n",
    "- This makes experimentation and iteration very fast\n",
    "- Design your functions to be pure (same input → same output)\n",
    "\n",
    "### 2. Granular Functions\n",
    "- Break complex operations into smaller, reusable functions\n",
    "- This improves caching effectiveness and debugging\n",
    "\n",
    "### 3. Clear Data Flow\n",
    "- Name your intermediate results clearly\n",
    "- Store important intermediate values in the DAG for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final summary of our pipeline\n",
    "dag.pipeline_summary = {\n",
    "    \"stages\": [\n",
    "        \"Data Generation\",\n",
    "        \"Data Filtering\",\n",
    "        \"Statistical Analysis\",\n",
    "        \"Parallel Processing\",\n",
    "        \"Complex Workflow\"\n",
    "    ],\n",
    "    \"total_functions\": len(function_keys),\n",
    "    \"total_nodes\": len(all_keys),\n",
    "    \"caching_demonstrated\": True,\n",
    "    \"composition_patterns\": [\n",
    "        \"Sequential chaining\",\n",
    "        \"Parallel branches\",\n",
    "        \"Complex nested workflows\"\n",
    "    ]\n",
    "}\n",
    "dag.result = dag.pipeline_summary\n",
    "\n",
    "print(\"=== Tutorial 2 Complete! ===\")\n",
    "print(\"You've learned:\")\n",
    "print(\"✅ Function composition and chaining\")\n",
    "print(\"✅ How DaggerML's caching system works\")\n",
    "print(\"✅ Parallel processing patterns\")\n",
    "print(\"✅ Working with complex data structures\")\n",
    "print(\"✅ DAG inspection and debugging\")\n",
    "print(f\"\\nFinal DAG size: {len(all_keys)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We've Accomplished\n",
    "\n",
    "In this tutorial, you've mastered:\n",
    "\n",
    "1. ✅ **Function Composition**: Chaining functions where outputs feed into inputs\n",
    "2. ✅ **Caching System**: Understanding how DaggerML caches results automatically\n",
    "3. ✅ **Parallel Processing**: Creating multiple branches that process data independently\n",
    "4. ✅ **Complex Workflows**: Building sophisticated data processing pipelines\n",
    "5. ✅ **Data Structure Handling**: Working with nested and complex data types\n",
    "6. ✅ **DAG Inspection**: Understanding and debugging your computational graphs\n",
    "\n",
    "## Performance Benefits\n",
    "\n",
    "The caching system provides several key benefits:\n",
    "- **Development Speed**: Iterate quickly without recomputing unchanged parts\n",
    "- **Reproducibility**: Same inputs always produce same outputs\n",
    "- **Resource Efficiency**: Avoid redundant computations\n",
    "\n",
    "## Next Tutorial Preview\n",
    "\n",
    "In Tutorial 3, we'll explore:\n",
    "- Error handling and recovery in DAGs\n",
    "- Working with external data sources\n",
    "- Different execution environments (local, cloud, containers)\n",
    "- Real-world data processing examples\n",
    "\n",
    "Great work! You're becoming a DaggerML expert! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
