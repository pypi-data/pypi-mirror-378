{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Context Curation with Real Repository and LLM\n",
    "\n",
    "This notebook tests the context curation functions with an actual repository and real LLM calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "from unittest.mock import AsyncMock, MagicMock\n",
    "\n",
    "# Add the parent directory to path to import yellhorn_mcp\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from yellhorn_mcp.processors.context_processor import (\n",
    "    build_codebase_context,\n",
    "    analyze_with_llm,\n",
    "    parse_llm_directories,\n",
    "    save_context_file,\n",
    "    process_context_curation_async\n",
    ")\n",
    "from yellhorn_mcp.llm_manager import LLMManager\n",
    "from yellhorn_mcp.utils.git_utils import YellhornMCPError, run_git_command_with_set_cwd, run_github_command_with_set_cwd\n",
    "\n",
    "print(\"✅ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LLM Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Using OpenAI model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM Manager with API clients\n",
    "# You need to have your API keys set as environment variables:\n",
    "GEMINI_API_KEY = \"\"\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from google import genai\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Create LLM Manager\n",
    "llm_manager = LLMManager(\n",
    "    openai_client=openai_client,\n",
    "    gemini_client=gemini_client,\n",
    "    config={\n",
    "        \"safety_margin_tokens\": 1000,\n",
    "        \"overlap_ratio\": 0.1,\n",
    "        \"chunk_strategy\": \"sentences\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Determine which model to use\n",
    "if openai_client:\n",
    "    DEFAULT_MODEL = \"gpt-4o-mini\"  # Using a cheaper model for testing\n",
    "    print(f\"🤖 Using OpenAI model: {DEFAULT_MODEL}\")\n",
    "elif gemini_client:\n",
    "    DEFAULT_MODEL = \"gemini-1.5-flash\"  # Fast and cheap Gemini model\n",
    "    print(f\"🤖 Using Gemini model: {DEFAULT_MODEL}\")\n",
    "else:\n",
    "    print(\"❌ No LLM client available. Please set OPENAI_API_KEY or GOOGLE_API_KEY\")\n",
    "    DEFAULT_MODEL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Repository Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Repository path exists: /Users/sravanj/project_work/yellhorn-mcp\n",
      "📁 Repository contents (sample):\n",
      "  📄 .yellhornignore\n",
      "  📂 yellhorn_mcp/\n",
      "  📄 .DS_Store\n",
      "  📄 pyrightconfig.json\n",
      "  📂 .pytest_cache/\n",
      "  📄 CHANGELOG.md\n",
      "  📄 .coverage\n",
      "  📄 LLMManagerREADME.md\n",
      "  📄 .mcp.json\n",
      "  📄 pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "# Use the actual yellhorn-mcp repository path\n",
    "REPO_PATH = Path(\"/Users/sravanj/project_work/yellhorn-mcp\")\n",
    "\n",
    "# Verify the path exists\n",
    "if REPO_PATH.exists():\n",
    "    print(f\"✅ Repository path exists: {REPO_PATH}\")\n",
    "    print(f\"📁 Repository contents (sample):\")\n",
    "    for item in list(REPO_PATH.iterdir())[:10]:\n",
    "        if item.is_dir():\n",
    "            print(f\"  📂 {item.name}/\")\n",
    "        else:\n",
    "            print(f\"  📄 {item.name}\")\n",
    "else:\n",
    "    print(f\"❌ Repository path does not exist: {REPO_PATH}\")\n",
    "    print(\"Please update REPO_PATH to point to your repository\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Git Command Function and Mock Context Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Git command function and mock context helpers created\n",
      "   - git_command_func: Real git command wrapper\n",
      "   - create_mock_context(): Creates mock context with real git commands\n",
      "   - print_log_messages(): Displays logged messages from mock context\n"
     ]
    }
   ],
   "source": [
    "def create_mock_context():\n",
    "    \"\"\"Create a mock context with logging capabilities.\"\"\"\n",
    "    mock_ctx = MagicMock()\n",
    "    mock_ctx.log = AsyncMock()\n",
    "    mock_ctx.request_context.lifespan_context = {\n",
    "        \"git_command_func\": run_git_command_with_set_cwd(REPO_PATH),\n",
    "        \"codebase_reasoning\": \"file_structure\"\n",
    "    }\n",
    "    return mock_ctx\n",
    "\n",
    "def print_log_messages(mock_ctx, limit=100):\n",
    "    \"\"\"Print the log messages from mock context.\"\"\"\n",
    "    if mock_ctx.log.called:\n",
    "        log_messages = [(call[1].get('level', 'info'), call[1]['message']) \n",
    "                       for call in mock_ctx.log.call_args_list]\n",
    "        print(f\"\\n📝 Log messages (showing first {limit}):\")\n",
    "        for level, msg in log_messages[:limit]:\n",
    "            emoji = \"🔵\" if level == \"info\" else \"🟡\" if level == \"warning\" else \"🔴\"\n",
    "            print(f\"  {emoji} [{level}] {msg[:5000]}...\" if len(msg) > 5000 else f\"  {emoji} [{level}] {msg}\")\n",
    "        if len(log_messages) > limit:\n",
    "            print(f\"  ... and {len(log_messages) - limit} more messages\")\n",
    "    else:\n",
    "        print(\"\\n📝 No log messages recorded\")\n",
    "\n",
    "print(\"✅ Git command function and mock context helpers created\")\n",
    "print(\"   - git_command_func: Real git command wrapper\")\n",
    "print(\"   - create_mock_context(): Creates mock context with real git commands\")\n",
    "print(\"   - print_log_messages(): Displays logged messages from mock context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Build Codebase Context with Real Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing build_codebase_context with real repository...\n",
      "\n",
      "⏱️ Time taken: 0.17 seconds\n",
      "\n",
      "📊 Statistics:\n",
      "  - Total files found: 37\n",
      "  - Total directories: 11\n",
      "  - Context size: 1128 characters\n",
      "\n",
      "📁 Top-level directories:\n",
      "  - .\n",
      "  - .github\n",
      "  - docs\n",
      "  - notebooks\n",
      "  - yellhorn_mcp\n",
      "\n",
      "📄 Sample files (first 10):\n",
      "  - .github/workflows/publish.yml\n",
      "  - .github/workflows/tests.yml\n",
      "  - .mcp.json\n",
      "  - .python-version\n",
      "  - CHANGELOG.md\n",
      "  - CLAUDE.md\n",
      "  - LLMManagerREADME.md\n",
      "  - README.md\n",
      "  - coverage_stats.txt\n",
      "  - docs/USAGE.md\n",
      "\n",
      "📝 Context preview (first 1000 chars):\n",
      "<codebase_tree>\n",
      ".\n",
      "├── .mcp.json\n",
      "├── .python-version\n",
      "├── CHANGELOG.md\n",
      "├── CLAUDE.md\n",
      "├── LLMManagerREADME.md\n",
      "├── README.md\n",
      "├── coverage_stats.txt\n",
      "├── pyproject.toml\n",
      "├── pyrightconfig.json\n",
      "│   ├── workflows/\n",
      "│   │   ├── publish.yml\n",
      "│   │   └── tests.yml\n",
      "├── docs/\n",
      "│   ├── USAGE.md\n",
      "│   └── coverage_baseline.md\n",
      "├── notebooks/\n",
      "│   ├── file_structure.ipynb\n",
      "│   └── llm_manager.ipynb\n",
      "├── yellhorn_mcp/\n",
      "│   ├── __init__.py\n",
      "│   ├── cli.py\n",
      "│   ├── llm_manager.py\n",
      "│   ├── metadata_models.py\n",
      "│   ├── server.py\n",
      "│   └── token_counter.py\n",
      "│   ├── formatters/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── codebase_snapshot.py\n",
      "│   │   ├── context_fetcher.py\n",
      "│   │   └── prompt_formatter.py\n",
      "│   ├── integrations/\n",
      "│   │   ├── gemini_integration.py\n",
      "│   │   └── github_integration.py\n",
      "│   ├── models/\n",
      "│   │   └── metadata_models.py\n",
      "│   ├── processors/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── context_processor.py\n",
      "│   │   ├── judgement_processor.py\n",
      "│   │   └── workplan_processor.py\n",
      "│   ├── utils/\n",
      "│   │   ├── comment_utils.py\n",
      "│   │   ├─\n",
      "\n",
      "📝 Log messages (showing first 100):\n",
      "  🔵 [info] Getting codebase context using file_structure mode\n",
      "  🔵 [info] Getting codebase snapshot in mode: paths\n",
      "  🔵 [info] Found .gitignore with 86 patterns\n",
      "  🔵 [info] Found .yellhornignore with 1 patterns\n",
      "  🔵 [info] File categorization results out of 70 files:\n",
      "  🔵 [info]   - 5 always ignored (images, binaries, configs, etc.)\n",
      "  🔵 [info]   - 0 in yellhorncontext whitelist (included)\n",
      "  🔵 [info]   - 0 in yellhorncontext blacklist (excluded)\n",
      "  🔵 [info]   - 0 in yellhornignore whitelist (included)\n",
      "  🔵 [info]   - 28 in yellhornignore blacklist (excluded)\n",
      "  🔵 [info]   - 37 other files (included - no .yellhorncontext)\n",
      "  🔵 [info] Total included: 37 files (excluded 5 always-ignored files)\n",
      "  🔵 [info] Codebase context metrics: 49 lines, 432 tokens (gpt-4o-mini)\n",
      "  🔵 [info] Extracted 11 directories from 37 filtered files\n"
     ]
    }
   ],
   "source": [
    "async def test_real_build_context():\n",
    "    \"\"\"Test building context from real repository.\"\"\"\n",
    "    print(\"\\n🧪 Testing build_codebase_context with real repository...\")\n",
    "    \n",
    "    # Create mock context\n",
    "    mock_ctx = create_mock_context()\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Build context for the repository with mock context\n",
    "    directory_context, file_paths, all_dirs = await build_codebase_context(\n",
    "        repo_path=REPO_PATH,\n",
    "        codebase_reasoning_mode=\"file_structure\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        ctx=mock_ctx,\n",
    "        git_command_func=mock_ctx.request_context.lifespan_context['git_command_func']\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n⏱️ Time taken: {elapsed:.2f} seconds\")\n",
    "    print(f\"\\n📊 Statistics:\")\n",
    "    print(f\"  - Total files found: {len(file_paths)}\")\n",
    "    print(f\"  - Total directories: {len(all_dirs)}\")\n",
    "    print(f\"  - Context size: {len(directory_context)} characters\")\n",
    "    \n",
    "    print(f\"\\n📁 Top-level directories:\")\n",
    "    top_dirs = [d for d in sorted(all_dirs) if '/' not in d]\n",
    "    for dir_name in top_dirs[:10]:\n",
    "        print(f\"  - {dir_name}\")\n",
    "    \n",
    "    print(f\"\\n📄 Sample files (first 10):\")\n",
    "    for file_path in file_paths[:10]:\n",
    "        print(f\"  - {file_path}\")\n",
    "    \n",
    "    print(f\"\\n📝 Context preview (first 1000 chars):\")\n",
    "    print(directory_context[:1000])\n",
    "    \n",
    "    # Print log messages\n",
    "    print_log_messages(mock_ctx)\n",
    "    \n",
    "    return directory_context, file_paths, all_dirs, mock_ctx\n",
    "\n",
    "# Run the test\n",
    "if REPO_PATH.exists():\n",
    "    context_data = await test_real_build_context()\n",
    "else:\n",
    "    print(\"⚠️ Skipping test - repository path not found\")\n",
    "    context_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Analyze with Real LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing analyze_with_llm with real LLM...\n",
      "🤖 Using model: gpt-4o-mini\n",
      "\n",
      "📋 Task: Improve the context curation system to better identify important directories for AI workplan generation. Include .python-version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 15:27:47,958 INFO HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ LLM analysis complete!\n",
      "⏱️ Time taken: 2.77 seconds\n",
      "\n",
      "🤖 LLM Response:\n",
      "------------------------------------------------------------\n",
      "```context\n",
      ".python-version\n",
      "yellhorn_mcp/\n",
      "yellhorn_mcp/cli.py\n",
      "yellhorn_mcp/llm_manager.py\n",
      "yellhorn_mcp/metadata_models.py\n",
      "yellhorn_mcp/server.py\n",
      "yellhorn_mcp/token_counter.py\n",
      "yellhorn_mcp/formatters/\n",
      "yellhorn_mcp/integrations/\n",
      "yellhorn_mcp/models/\n",
      "yellhorn_mcp/processors/\n",
      "yellhorn_mcp/utils/\n",
      "```\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Log messages (showing first 100):\n",
      "  🔵 [info] Analyzing directory structure with gpt-4o-mini\n",
      "  🔵 [info] [DEBUG] System message: You are an expert software developer tasked with analyzing a codebase structure to identify important directories for building and executing a workplan.\n",
      "\n",
      "Your goal is to identify the most important directories that should be included for the user's task.\n",
      "\n",
      "Analyze the directories and identify the ones that:\n",
      "1. Contain core application code relevant to the user's task\n",
      "2. Likely contain important business logic\n",
      "3. Would be essential for understanding the codebase architecture\n",
      "4. Are needed to implement the requested task\n",
      "5. Contain SDKs or libraries relevant to the user's task\n",
      "\n",
      "Ignore directories that:\n",
      "1. Contain only build artifacts or generated code\n",
      "2. Store dependencies or vendor code\n",
      "3. Contain temporary or cache files\n",
      "4. Probably aren't relevant to the user's specific task\n",
      "\n",
      "User Task: Improve the context curation system to better identify important directories for AI workplan generation. Include .python-version\n",
      "\n",
      "Return your analysis as a list of important directories, one per line, without any additional text or formatting as below:\n",
      "\n",
      "```context\n",
      "dir1/subdir1/\n",
      "dir2/\n",
      "dir3/subdir3/file3.filetype\n",
      "```\n",
      "\n",
      "Prefer to include directories, and not just file paths but include just file paths when appropriate.\n",
      "Don't include explanations for your choices, just return the list in the specified format.\n",
      "  🔵 [info] [DEBUG] User prompt (1128 chars): <codebase_tree>\n",
      ".\n",
      "├── .mcp.json\n",
      "├── .python-version\n",
      "├── CHANGELOG.md\n",
      "├── CLAUDE.md\n",
      "├── LLMManagerREADME.md\n",
      "├── README.md\n",
      "├── coverage_stats.txt\n",
      "├── pyproject.toml\n",
      "├── pyrightconfig.json\n",
      "│   ├── workflows/\n",
      "│   │   ├── publish.yml\n",
      "│   │   └── tests.yml\n",
      "├── docs/\n",
      "│   ├── USAGE.md\n",
      "│   └── coverage_baseline.md\n",
      "├── notebooks/\n",
      "│   ├── file_structure.ipynb\n",
      "│   └── llm_manager.ipynb\n",
      "├── yellhorn_mcp/\n",
      "│   ├── __init__.py\n",
      "│   ├── cli.py\n",
      "│   ├── llm_manager.py\n",
      "│   ├── metadata_models.py\n",
      "│   ├── server.py\n",
      "│   └── token_counter.py\n",
      "│   ├── formatters/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── codebase_snapshot.py\n",
      "│   │   ├── context_fetcher.py\n",
      "│   │   └── prompt_formatter.py\n",
      "│   ├── integrations/\n",
      "│   │   ├── gemini_integration.py\n",
      "│   │   └── github_integration.py\n",
      "│   ├── models/\n",
      "│   │   └── metadata_models.py\n",
      "│   ├── processors/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── context_processor.py\n",
      "│   │   ├── judgement_processor.py\n",
      "│   │   └── workplan_processor.py\n",
      "│   ├── utils/\n",
      "│   │   ├── comment_utils.py\n",
      "│   │   ├── cost_tracker_utils.py\n",
      "│   │   ├── git_utils.py\n",
      "│   │   ├── lsp_utils.py\n",
      "│   │   └── search_grounding_utils.py\n",
      "</codebase_tree>...\n",
      "\n",
      "📊 Token Usage:\n",
      "  - Prompt tokens: 702\n",
      "  - Completion tokens: 101\n",
      "  - Total tokens: 803\n"
     ]
    }
   ],
   "source": [
    "async def test_real_llm_analysis():\n",
    "    \"\"\"Test analyzing codebase with real LLM.\"\"\"\n",
    "    if not DEFAULT_MODEL or not context_data:\n",
    "        print(\"⚠️ Skipping test - LLM or context not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n🧪 Testing analyze_with_llm with real LLM...\")\n",
    "    print(f\"🤖 Using model: {DEFAULT_MODEL}\")\n",
    "    \n",
    "    # Create new mock context for this test\n",
    "    mock_ctx = create_mock_context()\n",
    "    \n",
    "    directory_context = context_data[0]\n",
    "    user_task = \"Improve the context curation system to better identify important directories for AI workplan generation. Include .python-version\"\n",
    "    \n",
    "    print(f\"\\n📋 Task: {user_task}\")    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Call real LLM with mock context\n",
    "        llm_result = await analyze_with_llm(\n",
    "            llm_manager=llm_manager,\n",
    "            model=DEFAULT_MODEL,\n",
    "            directory_context=directory_context,\n",
    "            user_task=user_task,\n",
    "            debug=True,  # Set to True to see debug logs\n",
    "            ctx=mock_ctx\n",
    "        )\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n✅ LLM analysis complete!\")\n",
    "        print(f\"⏱️ Time taken: {elapsed:.2f} seconds\")\n",
    "        print(f\"\\n🤖 LLM Response:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(llm_result)\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Print log messages\n",
    "        print_log_messages(mock_ctx)\n",
    "        \n",
    "        # Check for usage metadata\n",
    "        usage = llm_manager.get_last_usage_metadata()\n",
    "        if usage:\n",
    "            print(f\"\\n📊 Token Usage:\")\n",
    "            print(f\"  - Prompt tokens: {usage.prompt_tokens}\")\n",
    "            print(f\"  - Completion tokens: {usage.completion_tokens}\")\n",
    "            print(f\"  - Total tokens: {usage.total_tokens}\")\n",
    "        \n",
    "        return llm_result, mock_ctx\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error calling LLM: {e}\")\n",
    "        print_log_messages(mock_ctx)\n",
    "        return None, mock_ctx\n",
    "\n",
    "# Run the test\n",
    "llm_data = await test_real_llm_analysis()\n",
    "if llm_data:\n",
    "    llm_result, llm_ctx = llm_data\n",
    "else:\n",
    "    llm_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Parse LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing parse_llm_directories with real output...\n",
      "\n",
      "📊 Parsing Results:\n",
      "  - Total directories available: 11\n",
      "  - Important directories identified: 12\n",
      "  - [SHOULD HAPPEN] Reduction: -9.1%\n",
      "\n",
      "📁 Important directories identified:\n",
      "  - .python-version\n",
      "  - yellhorn_mcp\n",
      "  - yellhorn_mcp/cli.py\n",
      "  - yellhorn_mcp/formatters\n",
      "  - yellhorn_mcp/integrations\n",
      "  - yellhorn_mcp/llm_manager.py\n",
      "  - yellhorn_mcp/metadata_models.py\n",
      "  - yellhorn_mcp/models\n",
      "  - yellhorn_mcp/processors\n",
      "  - yellhorn_mcp/server.py\n",
      "  - yellhorn_mcp/token_counter.py\n",
      "  - yellhorn_mcp/utils\n",
      "\n",
      "🔍 Selection Analysis:\n",
      "  ✅ yellhorn_mcp - included\n",
      "  ✅ yellhorn_mcp/processors - included\n",
      "  ✅ yellhorn_mcp/formatters - included\n",
      "  ⚠️ tests - not found in repository\n",
      "\n",
      "📝 Log messages (showing first 100):\n",
      "  🔵 [info] Matched '.python-version' to directories: .python-version\n",
      "  🔵 [info] Matched 'yellhorn_mcp' to directories: yellhorn_mcp\n",
      "  🔵 [info] Matched 'yellhorn_mcp/cli.py' to directories: yellhorn_mcp/cli.py\n",
      "  🔵 [info] Matched 'yellhorn_mcp/llm_manager.py' to directories: yellhorn_mcp/llm_manager.py\n",
      "  🔵 [info] Matched 'yellhorn_mcp/metadata_models.py' to directories: yellhorn_mcp/metadata_models.py\n",
      "  🔵 [info] Matched 'yellhorn_mcp/server.py' to directories: yellhorn_mcp/server.py\n",
      "  🔵 [info] Matched 'yellhorn_mcp/token_counter.py' to directories: yellhorn_mcp/token_counter.py\n",
      "  🔵 [info] Matched 'yellhorn_mcp/formatters' to directories: yellhorn_mcp/formatters\n",
      "  🔵 [info] Matched 'yellhorn_mcp/integrations' to directories: yellhorn_mcp/integrations\n",
      "  🔵 [info] Matched 'yellhorn_mcp/models' to directories: yellhorn_mcp/models\n",
      "  🔵 [info] Matched 'yellhorn_mcp/processors' to directories: yellhorn_mcp/processors\n",
      "  🔵 [info] Matched 'yellhorn_mcp/utils' to directories: yellhorn_mcp/utils\n"
     ]
    }
   ],
   "source": [
    "async def test_parse_real_output():\n",
    "    \"\"\"Test parsing real LLM output.\"\"\"\n",
    "    if not llm_result or not context_data:\n",
    "        print(\"⚠️ Skipping test - LLM result not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n🧪 Testing parse_llm_directories with real output...\")\n",
    "    \n",
    "    # Create new mock context for this test\n",
    "    mock_ctx = create_mock_context()\n",
    "    \n",
    "    all_dirs = context_data[2]\n",
    "    \n",
    "    # Parse the LLM output with mock context\n",
    "    important_dirs = await parse_llm_directories(\n",
    "        llm_result=llm_result,\n",
    "        all_dirs=all_dirs,\n",
    "        ctx=mock_ctx\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Parsing Results:\")\n",
    "    print(f\"  - Total directories available: {len(all_dirs)}\")\n",
    "    print(f\"  - Important directories identified: {len(important_dirs)}\")\n",
    "    print(f\"  - [SHOULD HAPPEN] Reduction: {100 * (1 - len(important_dirs)/len(all_dirs)):.1f}%\")\n",
    "    \n",
    "    print(f\"\\n📁 Important directories identified:\")\n",
    "    for dir_name in sorted(important_dirs)[:20]:\n",
    "        print(f\"  - {dir_name}\")\n",
    "    \n",
    "    if len(important_dirs) > 20:\n",
    "        print(f\"  ... and {len(important_dirs) - 20} more\")\n",
    "    \n",
    "    # Analyze the selection\n",
    "    print(f\"\\n🔍 Selection Analysis:\")\n",
    "    \n",
    "    # Check if key directories were selected\n",
    "    key_dirs = [\"yellhorn_mcp\", \"yellhorn_mcp/processors\", \"yellhorn_mcp/formatters\", \"tests\"]\n",
    "    for key_dir in key_dirs:\n",
    "        if key_dir in important_dirs:\n",
    "            print(f\"  ✅ {key_dir} - included\")\n",
    "        elif key_dir in all_dirs:\n",
    "            print(f\"  ❌ {key_dir} - excluded\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ {key_dir} - not found in repository\")\n",
    "    \n",
    "    # Print log messages\n",
    "    print_log_messages(mock_ctx)\n",
    "    \n",
    "    return important_dirs, mock_ctx\n",
    "\n",
    "# Run the test\n",
    "parse_data = await test_parse_real_output()\n",
    "if parse_data:\n",
    "    parsed_dirs, parse_ctx = parse_data\n",
    "else:\n",
    "    parsed_dirs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '.github',\n",
       " '.github/workflows',\n",
       " 'docs',\n",
       " 'notebooks',\n",
       " 'yellhorn_mcp',\n",
       " 'yellhorn_mcp/formatters',\n",
       " 'yellhorn_mcp/integrations',\n",
       " 'yellhorn_mcp/models',\n",
       " 'yellhorn_mcp/processors',\n",
       " 'yellhorn_mcp/utils'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```context\n",
      ".python-version\n",
      "yellhorn_mcp/\n",
      "yellhorn_mcp/cli.py\n",
      "yellhorn_mcp/llm_manager.py\n",
      "yellhorn_mcp/metadata_models.py\n",
      "yellhorn_mcp/server.py\n",
      "yellhorn_mcp/token_counter.py\n",
      "yellhorn_mcp/formatters/\n",
      "yellhorn_mcp/integrations/\n",
      "yellhorn_mcp/models/\n",
      "yellhorn_mcp/processors/\n",
      "yellhorn_mcp/utils/\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.python-version', 'yellhorn_mcp', 'yellhorn_mcp/cli.py', 'yellhorn_mcp/formatters', 'yellhorn_mcp/integrations', 'yellhorn_mcp/models', 'yellhorn_mcp/utils', 'yellhorn_mcp/llm_manager.py', 'yellhorn_mcp/processors', 'yellhorn_mcp/metadata_models.py', 'yellhorn_mcp/server.py', 'yellhorn_mcp/token_counter.py'}\n"
     ]
    }
   ],
   "source": [
    "print(parsed_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: End-to-End Context Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing end-to-end context curation...\n",
      "📁 Repository: /Users/sravanj/project_work/yellhorn-mcp\n",
      "🤖 Model: gpt-4o-mini\n",
      "\n",
      "📋 Task: Refactor the context processor module to improve modularity and add better error handling\n",
      "📄 Output file: .yellhorncontext.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 15:35:37,264 INFO HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Context curation complete!\n",
      "⏱️ Time taken: 1.42 seconds\n",
      "\n",
      "📊 Result: Successfully created .yellhorncontext file at /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext.test with 3 important directories.\n",
      "\n",
      "📄 Generated context file content:\n",
      "============================================================\n",
      "# Yellhorn Context File - AI context optimization\n",
      "# Generated by yellhorn-mcp curate_context tool\n",
      "# Based on task: Refactor the context processor module to improve modularity and add better error\n",
      "\n",
      "# Important directories to specifically include\n",
      "yellhorn_mcp/\n",
      "yellhorn_mcp/processors/\n",
      "yellhorn_mcp/utils/\n",
      "\n",
      "============================================================\n",
      "\n",
      "📊 File Analysis:\n",
      "  - Total lines: 9\n",
      "  - Comment lines: 6\n",
      "  - Directory patterns: 3\n",
      "\n",
      "📝 Log messages (showing first 100):\n",
      "  🔵 [info] Starting context curation process\n",
      "  🔵 [info] Getting codebase context using file_structure mode\n",
      "  🔵 [info] Getting codebase snapshot in mode: paths\n",
      "  🔵 [info] Found .gitignore with 86 patterns\n",
      "  🔵 [info] Found .yellhornignore with 1 patterns\n",
      "  🔵 [info] File categorization results out of 70 files:\n",
      "  🔵 [info]   - 5 always ignored (images, binaries, configs, etc.)\n",
      "  🔵 [info]   - 0 in yellhorncontext whitelist (included)\n",
      "  🔵 [info]   - 0 in yellhorncontext blacklist (excluded)\n",
      "  🔵 [info]   - 0 in yellhornignore whitelist (included)\n",
      "  🔵 [info]   - 28 in yellhornignore blacklist (excluded)\n",
      "  🔵 [info]   - 37 other files (included - no .yellhorncontext)\n",
      "  🔵 [info] Total included: 37 files (excluded 5 always-ignored files)\n",
      "  🔵 [info] Codebase context metrics: 49 lines, 432 tokens (gpt-4o-mini)\n",
      "  🔵 [info] Extracted 11 directories from 37 filtered files\n",
      "  🔵 [info] Directory context:\n",
      "<codebase_tree>\n",
      ".\n",
      "├── .mcp.json\n",
      "├── .python-version\n",
      "├── CHANGELOG.md\n",
      "├── CLAUDE.md\n",
      "├── LLMManagerREADME.md\n",
      "├── README.md\n",
      "├── coverage_stats.txt\n",
      "├── pyproject.toml\n",
      "├── pyrightconfig.json\n",
      "│   ├── workflows/\n",
      "│   │   ├── publish.yml\n",
      "│   │   └── tests.yml\n",
      "├── docs/\n",
      "│   ├── USAGE.md\n",
      "│   └── coverage_baseline.md\n",
      "├── notebooks/\n",
      "│   ├── file_structure.ipynb\n",
      "│   └── llm_manager.ipynb\n",
      "├── yellhorn_mcp/\n",
      "│   ├── __init__.py\n",
      "│   ├── cli.py\n",
      "│   ├── llm_manager.py\n",
      "│   ├── metadata_models.py\n",
      "│   ├── server.py\n",
      "│ ...\n",
      "  🔵 [info] Analyzing directory structure with gpt-4o-mini\n",
      "  🔵 [info] Matched 'yellhorn_mcp/processors' to directories: yellhorn_mcp/processors\n",
      "  🔵 [info] Matched 'yellhorn_mcp' to directories: yellhorn_mcp\n",
      "  🔵 [info] Matched 'yellhorn_mcp/utils' to directories: yellhorn_mcp/utils\n",
      "  🔵 [info] Analysis complete, found 3 important directories: yellhorn_mcp, yellhorn_mcp/processors, yellhorn_mcp/utils\n",
      "  🔵 [info] Processing complete, identified 3 important directories\n",
      "  🔵 [info] Successfully wrote .yellhorncontext file to /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext.test\n",
      "\n",
      "🧹 Cleaning up test file...\n",
      "✅ Test file removed\n",
      "\n",
      "📊 Total Token Usage:\n",
      "  - Prompt tokens: 696\n",
      "  - Completion tokens: 26\n",
      "  - Total tokens: 722\n"
     ]
    }
   ],
   "source": [
    "async def test_end_to_end_real():\n",
    "    \"\"\"Test complete context curation with real repository and LLM.\"\"\"\n",
    "    if not DEFAULT_MODEL or not REPO_PATH.exists():\n",
    "        print(\"⚠️ Skipping test - LLM or repository not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n🧪 Testing end-to-end context curation...\")\n",
    "    print(f\"📁 Repository: {REPO_PATH}\")\n",
    "    print(f\"🤖 Model: {DEFAULT_MODEL}\")\n",
    "    \n",
    "    # Create mock context for this test\n",
    "    mock_ctx = create_mock_context()\n",
    "    \n",
    "    user_task = \"Refactor the context processor module to improve modularity and add better error handling\"\n",
    "    output_path = \".yellhorncontext.test\"\n",
    "    \n",
    "    print(f\"\\n📋 Task: {user_task}\")\n",
    "    print(f\"📄 Output file: {output_path}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Run the complete process with mock context\n",
    "        result = await process_context_curation_async(\n",
    "            repo_path=REPO_PATH,\n",
    "            llm_manager=llm_manager,\n",
    "            model=DEFAULT_MODEL,\n",
    "            user_task=user_task,\n",
    "            output_path=output_path,\n",
    "            codebase_reasoning=\"file_structure\",\n",
    "            disable_search_grounding=False,\n",
    "            debug=False,\n",
    "            ctx=mock_ctx\n",
    "        )\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n✅ Context curation complete!\")\n",
    "        print(f\"⏱️ Time taken: {elapsed:.2f} seconds\")\n",
    "        print(f\"\\n📊 Result: {result}\")\n",
    "        \n",
    "        # Read and display the generated file\n",
    "        context_file = REPO_PATH / output_path\n",
    "        if context_file.exists():\n",
    "            content = context_file.read_text()\n",
    "            print(f\"\\n📄 Generated context file content:\")\n",
    "            print(\"=\" * 60)\n",
    "            print(content)\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Analyze the content\n",
    "            lines = content.split('\\n')\n",
    "            non_comment_lines = [l for l in lines if l.strip() and not l.strip().startswith('#')]\n",
    "            \n",
    "            print(f\"\\n📊 File Analysis:\")\n",
    "            print(f\"  - Total lines: {len(lines)}\")\n",
    "            print(f\"  - Comment lines: {len(lines) - len(non_comment_lines)}\")\n",
    "            print(f\"  - Directory patterns: {len(non_comment_lines)}\")\n",
    "            \n",
    "            # Print log messages\n",
    "            print_log_messages(mock_ctx)\n",
    "            \n",
    "            # Clean up test file\n",
    "            print(f\"\\n🧹 Cleaning up test file...\")\n",
    "            context_file.unlink()\n",
    "            print(f\"✅ Test file removed\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Context file was not created\")\n",
    "            print_log_messages(mock_ctx)\n",
    "        \n",
    "        # Check usage\n",
    "        usage = llm_manager.get_last_usage_metadata()\n",
    "        if usage:\n",
    "            print(f\"\\n📊 Total Token Usage:\")\n",
    "            print(f\"  - Prompt tokens: {usage.prompt_tokens}\")\n",
    "            print(f\"  - Completion tokens: {usage.completion_tokens}\")\n",
    "            print(f\"  - Total tokens: {usage.total_tokens}\")\n",
    "        \n",
    "        return result, mock_ctx\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during context curation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print_log_messages(mock_ctx)\n",
    "        return None, mock_ctx\n",
    "\n",
    "# Run the test\n",
    "end_to_end_data = await test_end_to_end_real()\n",
    "if end_to_end_data:\n",
    "    end_to_end_result, end_to_end_ctx = end_to_end_data\n",
    "else:\n",
    "    end_to_end_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Reasoning Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing different reasoning modes...\n",
      "\n",
      "📋 Testing mode: file_structure\n",
      "  ✅ Completed in 0.03s\n",
      "  📊 Context size: 1128 chars\n",
      "  📝 Log messages: 14\n",
      "  Sample logs:\n",
      "    - [info] Getting codebase context using file_structure mode...\n",
      "    - [info] Getting codebase snapshot in mode: paths...\n",
      "    - [info] Found .gitignore with 86 patterns...\n",
      "\n",
      "📋 Testing mode: lsp\n",
      "  ✅ Completed in 0.04s\n",
      "  📊 Context size: 17126 chars\n",
      "  📝 Log messages: 14\n",
      "  Sample logs:\n",
      "    - [info] Getting codebase context using lsp mode...\n",
      "    - [info] Getting codebase snapshot in mode: paths...\n",
      "    - [info] Found .gitignore with 86 patterns...\n",
      "\n",
      "📋 Testing mode: full\n",
      "  ✅ Completed in 0.08s\n",
      "  📊 Context size: 407588 chars\n",
      "  📝 Log messages: 15\n",
      "  Sample logs:\n",
      "    - [info] Getting codebase context using full mode...\n",
      "    - [info] Getting codebase snapshot in mode: full...\n",
      "    - [info] Found .gitignore with 86 patterns...\n",
      "\n",
      "📊 Comparison of reasoning modes:\n",
      "Mode            Time (s)   Context Size    Files      Dirs       Logs      \n",
      "----------------------------------------------------------------------\n",
      "file_structure  0.03       1,128           37         11         14        \n",
      "lsp             0.04       17,126          37         11         14        \n",
      "full            0.08       407,588         37         11         15        \n"
     ]
    }
   ],
   "source": [
    "async def test_reasoning_modes():\n",
    "    \"\"\"Test different codebase reasoning modes.\"\"\"\n",
    "    if not REPO_PATH.exists():\n",
    "        print(\"⚠️ Skipping test - repository not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n🧪 Testing different reasoning modes...\")\n",
    "    \n",
    "    modes = [\"file_structure\", \"lsp\", \"full\"]\n",
    "    results = {}\n",
    "    \n",
    "    for mode in modes:\n",
    "        print(f\"\\n📋 Testing mode: {mode}\")\n",
    "        \n",
    "        # Create new mock context for each mode\n",
    "        mock_ctx = create_mock_context()\n",
    "        mock_ctx.request_context.lifespan_context[\"codebase_reasoning\"] = mode\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            directory_context, file_paths, all_dirs = await build_codebase_context(\n",
    "                repo_path=REPO_PATH,\n",
    "                codebase_reasoning_mode=mode,\n",
    "                model=DEFAULT_MODEL,\n",
    "                ctx=mock_ctx,\n",
    "                git_command_func=mock_ctx.request_context.lifespan_context['git_command_func']\n",
    "            )\n",
    "            \n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            results[mode] = {\n",
    "                \"time\": elapsed,\n",
    "                \"context_size\": len(directory_context),\n",
    "                \"files\": len(file_paths),\n",
    "                \"dirs\": len(all_dirs),\n",
    "                \"logs\": mock_ctx.log.call_count\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✅ Completed in {elapsed:.2f}s\")\n",
    "            print(f\"  📊 Context size: {len(directory_context)} chars\")\n",
    "            print(f\"  📝 Log messages: {mock_ctx.log.call_count}\")\n",
    "            \n",
    "            # Show sample log messages\n",
    "            if mock_ctx.log.called:\n",
    "                print(\"  Sample logs:\")\n",
    "                for i, call in enumerate(mock_ctx.log.call_args_list[:3]):\n",
    "                    level = call[1].get('level', 'info')\n",
    "                    msg = call[1]['message'][:80]\n",
    "                    print(f\"    - [{level}] {msg}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "            results[mode] = {\"error\": str(e)}\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n📊 Comparison of reasoning modes:\")\n",
    "    print(f\"{'Mode':<15} {'Time (s)':<10} {'Context Size':<15} {'Files':<10} {'Dirs':<10} {'Logs':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for mode, data in results.items():\n",
    "        if \"error\" not in data:\n",
    "            print(f\"{mode:<15} {data['time']:<10.2f} {data['context_size']:<15,} {data['files']:<10} {data['dirs']:<10} {data['logs']:<10}\")\n",
    "        else:\n",
    "            print(f\"{mode:<15} Error: {data['error'][:40]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "reasoning_results = await test_reasoning_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Debug Mode Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing with debug mode enabled...\n",
      "📁 Repository: /Users/sravanj/project_work/yellhorn-mcp\n",
      "🤖 Model: gpt-4o-mini\n",
      "\n",
      "📋 Task: Test debug logging functionality\n",
      "\n",
      "1️⃣ Building codebase context...\n",
      "   ✅ Context built: 1128 chars, 37 files\n",
      "\n",
      "2️⃣ Analyzing with LLM (debug=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 15:37:51,726 INFO HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ LLM analysis complete\n",
      "\n",
      "3️⃣ Parsing LLM output...\n",
      "   ✅ Parsed 8 important directories\n",
      "{'yellhorn_mcp/utils', 'yellhorn_mcp', 'yellhorn_mcp/llm_manager.py', 'yellhorn_mcp/processors', 'yellhorn_mcp/cli.py', 'yellhorn_mcp/formatters', 'yellhorn_mcp/integrations', 'yellhorn_mcp/server.py'}\n",
      "\n",
      "📝 Complete Log Messages:\n",
      "============================================================\n",
      "  1. 🔵 [info   ] Getting codebase context using file_structure mode\n",
      "  2. 🔵 [info   ] Getting codebase snapshot in mode: paths\n",
      "  3. 🔵 [info   ] Found .gitignore with 86 patterns\n",
      "  4. 🔵 [info   ] Found .yellhornignore with 1 patterns\n",
      "  5. 🔵 [info   ] File categorization results out of 70 files:\n",
      "  6. 🔵 [info   ]   - 5 always ignored (images, binaries, configs, etc.)\n",
      "  7. 🔵 [info   ]   - 0 in yellhorncontext whitelist (included)\n",
      "  8. 🔵 [info   ]   - 0 in yellhorncontext blacklist (excluded)\n",
      "  9. 🔵 [info   ]   - 0 in yellhornignore whitelist (included)\n",
      " 10. 🔵 [info   ]   - 28 in yellhornignore blacklist (excluded)\n",
      " 11. 🔵 [info   ]   - 37 other files (included - no .yellhorncontext)\n",
      " 12. 🔵 [info   ] Total included: 37 files (excluded 5 always-ignored files)\n",
      " 13. 🔵 [info   ] Codebase context metrics: 49 lines, 432 tokens (gpt-4o-mini)\n",
      " 14. 🔵 [info   ] Extracted 11 directories from 37 filtered files\n",
      " 15. 🔵 [info   ] Analyzing directory structure with gpt-4o-mini\n",
      " 16. 🔵 [info   ] [DEBUG] System message: You are an expert software developer tasked with analyzing a codebase structure to identify important directories for building and executing a workplan.\n",
      "\n",
      "Your goal is to identify the most important directories that should be included for the user's task.\n",
      "\n",
      "Analyze the directories and identify the ones that:\n",
      "1. Contain core application code relevant to the user's task\n",
      "2. Likely contain important business logic\n",
      "3. Would be essential for understanding the codebase architecture\n",
      "4. Are needed to implement the requested task\n",
      "5. Contain SDKs or libraries relevant to the user's task\n",
      "\n",
      "Ignore directories that:\n",
      "1. Contain only build artifacts or generated code\n",
      "2. Store dependencies or vendor code\n",
      "3. Contain temporary or cache files\n",
      "4. Probably aren't relevant to the user's specific task\n",
      "\n",
      "User Task: Test debug logging functionality\n",
      "\n",
      "Return your analysis as a list of important directories, one per line, without any additional text or formatting as below:\n",
      "\n",
      "```context\n",
      "dir1/subdir1/\n",
      "dir2/\n",
      "dir3/subdir3/file3.filetype\n",
      "```\n",
      "\n",
      "Prefer to include directories, and not just file paths but include just file paths when appropriate.\n",
      "Don't include explanations for your choices, just return the list in the specified format.\n",
      " 17. 🔵 [info   ] [DEBUG] User prompt (1128 chars): <codebase_tree>\n",
      ".\n",
      "├── .mcp.json\n",
      "├── .python-version\n",
      "├── CHANGELOG.md\n",
      "├── CLAUDE.md\n",
      "├── LLMManagerREADME.md\n",
      "├── README.md\n",
      "├── coverage_stats.txt\n",
      "├── pyproject.toml\n",
      "├── pyrightconfig.json\n",
      "│   ├── workflows/\n",
      "│   │   ├── publish.yml\n",
      "│   │   └── tests.yml\n",
      "├── docs/\n",
      "│   ├── USAGE.md\n",
      "│   └── coverage_baseline.md\n",
      "├── notebooks/\n",
      "│   ├── file_structure.ipynb\n",
      "│   └── llm_manager.ipynb\n",
      "├── yellhorn_mcp/\n",
      "│   ├── __init__.py\n",
      "│   ├── cli.py\n",
      "│   ├── llm_manager.py\n",
      "│   ├── metadata_models.py\n",
      "│   ├── server.py\n",
      "│   └── token_counter.py\n",
      "│   ├── formatters/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── codebase_snapshot.py\n",
      "│   │   ├── context_fetcher.py\n",
      "│   │   └── prompt_formatter.py\n",
      "│   ├── integrations/\n",
      "│   │   ├── gemini_integration.py\n",
      "│   │   └── github_integration.py\n",
      "│   ├── models/\n",
      "│   │   └── metadata_models.py\n",
      "│   ├── processors/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── context_processor.py\n",
      "│   │   ├── judgement_processor.py\n",
      "│   │   └── workplan_processor.py\n",
      "│   ├── utils/\n",
      "│   │   ├── comment_utils.py\n",
      "│   │   ├── cost_tracker_utils.py\n",
      "│   │   ├── git_utils.py\n",
      "│   │   ├── lsp_utils.py\n",
      "│   │   └── search_grounding_utils.py\n",
      "</codebase_tree>...\n",
      "============================================================\n",
      "\n",
      "📊 Summary:\n",
      "  - Total log messages: 17\n",
      "  - Info messages: 17\n",
      "  - Warning messages: 0\n",
      "  - Error messages: 0\n"
     ]
    }
   ],
   "source": [
    "async def test_with_debug_mode():\n",
    "    \"\"\"Test with debug logging enabled to see detailed information.\"\"\"\n",
    "    if not DEFAULT_MODEL or not REPO_PATH.exists():\n",
    "        print(\"⚠️ Skipping test - LLM or repository not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n🧪 Testing with debug mode enabled...\")\n",
    "    print(f\"📁 Repository: {REPO_PATH}\")\n",
    "    print(f\"🤖 Model: {DEFAULT_MODEL}\")\n",
    "    \n",
    "    # Create mock context\n",
    "    mock_ctx = create_mock_context()\n",
    "    \n",
    "    # Simple task for debugging\n",
    "    user_task = \"Test debug logging functionality\"\n",
    "    \n",
    "    print(f\"\\n📋 Task: {user_task}\")\n",
    "    \n",
    "    try:\n",
    "        # Build context\n",
    "        print(\"\\n1️⃣ Building codebase context...\")\n",
    "        directory_context, file_paths, all_dirs = await build_codebase_context(\n",
    "            repo_path=REPO_PATH,\n",
    "            codebase_reasoning_mode=\"file_structure\",\n",
    "            model=DEFAULT_MODEL,\n",
    "            ctx=mock_ctx,\n",
    "            git_command_func=mock_ctx.request_context.lifespan_context['git_command_func']\n",
    "        )\n",
    "        print(f\"   ✅ Context built: {len(directory_context)} chars, {len(file_paths)} files\")\n",
    "        \n",
    "        # Analyze with LLM (with debug=True)\n",
    "        print(\"\\n2️⃣ Analyzing with LLM (debug=True)...\")\n",
    "        llm_result = await analyze_with_llm(\n",
    "            llm_manager=llm_manager,\n",
    "            model=DEFAULT_MODEL,\n",
    "            directory_context=directory_context[:5000],  # Use smaller context for debug\n",
    "            user_task=user_task,\n",
    "            debug=True,  # Enable debug logging\n",
    "            ctx=mock_ctx\n",
    "        )\n",
    "        print(f\"   ✅ LLM analysis complete\")\n",
    "        \n",
    "        # Parse directories\n",
    "        print(\"\\n3️⃣ Parsing LLM output...\")\n",
    "        important_dirs = await parse_llm_directories(\n",
    "            llm_result=llm_result,\n",
    "            all_dirs=all_dirs,\n",
    "            ctx=mock_ctx\n",
    "        )\n",
    "        print(f\"   ✅ Parsed {len(important_dirs)} important directories\")\n",
    "        print(important_dirs)\n",
    "        \n",
    "        # Print all log messages\n",
    "        print(\"\\n📝 Complete Log Messages:\")\n",
    "        print(\"=\" * 60)\n",
    "        if mock_ctx.log.called:\n",
    "            for i, call in enumerate(mock_ctx.log.call_args_list):\n",
    "                level = call[1].get('level', 'info')\n",
    "                msg = call[1]['message']\n",
    "                emoji = \"🔵\" if level == \"info\" else \"🟡\" if level == \"warning\" else \"🔴\"\n",
    "                print(f\"{i+1:3}. {emoji} [{level:7}] {msg}\")\n",
    "        else:\n",
    "            print(\"No log messages recorded\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n📊 Summary:\")\n",
    "        print(f\"  - Total log messages: {mock_ctx.log.call_count}\")\n",
    "        print(f\"  - Info messages: {sum(1 for c in mock_ctx.log.call_args_list if c[1].get('level') == 'info')}\")\n",
    "        print(f\"  - Warning messages: {sum(1 for c in mock_ctx.log.call_args_list if c[1].get('level') == 'warning')}\")\n",
    "        print(f\"  - Error messages: {sum(1 for c in mock_ctx.log.call_args_list if c[1].get('level') == 'error')}\")\n",
    "        \n",
    "        return mock_ctx\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during debug test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print_log_messages(mock_ctx)\n",
    "        return mock_ctx\n",
    "\n",
    "# Run the debug test\n",
    "debug_ctx = await test_with_debug_mode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sravan-yellhorn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
