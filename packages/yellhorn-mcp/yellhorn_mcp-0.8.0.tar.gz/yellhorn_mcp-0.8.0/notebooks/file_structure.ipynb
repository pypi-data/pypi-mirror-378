{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure Functionality Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from yellhorn_mcp.token_counter import TokenCounter\n",
    "from yellhorn_mcp.formatters import get_codebase_snapshot, get_codebase_context\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in with repo path\n",
    "repo_path = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths from codebase snapshot\n",
    "file_paths, file_contents = await get_codebase_snapshot(repo_path, just_paths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Counter for Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize token counter\n",
    "tc = TokenCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume dir_chunk is your list of directory paths (e.g. ['.', 'src', 'tests', …])\n",
    "# and file_paths is the full list of files (e.g. ['app.py', 'src/main.py', 'tests/test_main.py', …])\n",
    "\n",
    "codebase_tree = await get_codebase_context(repo_path, \"full\", token_limit=256000, model=\"gpt-4o\")\n",
    "\n",
    "# Now you can inject `directory_tree` into your prompt\n",
    "print(codebase_tree[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(codebase_tree[-2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the tokens\n",
    "token_count = tc.count_tokens(codebase_tree, \"gpt-4o\")\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate Context: Process Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze directories from filtered files\n",
    "all_dirs = set()\n",
    "for file_path in file_paths:\n",
    "    # Get all parent directories of this file\n",
    "    parts = file_path.split('/')\n",
    "    for i in range(1, len(parts)):\n",
    "        dir_path = '/'.join(parts[:i])\n",
    "        if dir_path:  # Skip empty strings\n",
    "            all_dirs.add(dir_path)\n",
    "\n",
    "# Add root directory ('.') if there are files at the root level\n",
    "if any('/' not in f for f in file_paths):\n",
    "    all_dirs.add('.')\n",
    "    \n",
    "# Sort directories for consistent output\n",
    "sorted_dirs = sorted(list(all_dirs))\n",
    "\n",
    "# Set chunk size based on reasoning mode\n",
    "chunk_size = 3000  # Process more files per chunk for file structure mode\n",
    "    \n",
    "# Calculate number of chunks needed\n",
    "total_chunks = (len(sorted_dirs) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "\n",
    "# Create chunks of directories\n",
    "dir_chunks = []\n",
    "for i in range(0, len(sorted_dirs), chunk_size):\n",
    "    dir_chunks.append(sorted_dirs[i:i + chunk_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track important directories\n",
    "all_important_dirs = set()\n",
    "\n",
    "# Helper function to process a single chunk\n",
    "async def process_chunk(chunk_idx, dir_chunk):    \n",
    "    lines = []\n",
    "    for dir_path in dir_chunk:\n",
    "        # Choose a nicer label for the root directory\n",
    "        dir_label = 'top_directory' if dir_path == '.' else dir_path\n",
    "        lines.append(dir_label)\n",
    "\n",
    "        # Gather up to 5 direct children files of this directory\n",
    "        if dir_path == '.':\n",
    "            dir_files = [f for f in file_paths if '/' not in f]\n",
    "        else:\n",
    "            prefix = dir_path.rstrip('/') + '/'\n",
    "            dir_files = [\n",
    "                f for f in file_paths \n",
    "                if f.startswith(prefix) and '/' not in f[len(prefix):]\n",
    "            ]\n",
    "        samples = dir_files[:5]\n",
    "\n",
    "        # Append each sample file under the directory, indented with a tab\n",
    "        for f in samples:\n",
    "            lines.append(f\"\\t{os.path.basename(f)}\")\n",
    "\n",
    "    # Final single representation\n",
    "    directory_tree = \"\\n\".join(lines)\n",
    "    \n",
    "    # Construct the prompt for this chunk\n",
    "    prompt = f\"\"\"You are an expert software developer tasked with analyzing a codebase structure to identify important directories for AI context.\n",
    "\n",
    "<user_task>\n",
    "{user_task}\n",
    "</user_task>\n",
    "\n",
    "Your goal is to identify the most important directories that should be included when an AI assistant analyzes this codebase for the user's task.\n",
    "\n",
    "Below is a list of directories from the codebase (chunk {chunk_idx + 1} of {total_chunks}):\n",
    "\n",
    "<directories>\n",
    "{directory_tree}\n",
    "</directories>\n",
    "\n",
    "Analyze these directories and identify the ones that:\n",
    "1. Contain core application code relevant to the user's task\n",
    "2. Likely contain important business logic\n",
    "3. Would be essential for understanding the codebase architecture\n",
    "4. Are needed to implement the requested task\n",
    "\n",
    "Ignore directories that:\n",
    "1. Contain only build artifacts or generated code\n",
    "2. Store dependencies or vendor code\n",
    "3. Contain temporary or cache files\n",
    "4. Probably aren't relevant to the user's specific task\n",
    "\n",
    "Return your analysis as a list of important directories, one per line, in this format:\n",
    "\n",
    "```context\n",
    "dir1\n",
    "dir2\n",
    "dir3\n",
    "```\n",
    "\n",
    "Don't include explanations for your choices, just return the list in the specified format.\n",
    "\"\"\"\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_chunk(0, dir_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSP Prompt Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lsp mode, format with tree and LSP file contents\n",
    "codebase_info = await get_codebase_context(repo_path, \"lsp\", token_limit=256000, model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect File Structure Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lsp mode, format with tree and LSP file contents\n",
    "codebase_info = await get_codebase_context(repo_path, \"file_structure\", token_limit=256000, model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(codebase_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the File Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path(\"/Users/sravanj/project_work/yellhorn-mcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellhorn_mcp.server import run_git_command\n",
    "# import fnmatch\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_codebase_snapshot(repo_path: Path, _mode: str = \"full\", log_function = print) -> tuple[list[str], dict[str, str]]:\n",
    "    # Get list of all tracked and untracked files (respects .gitignore by default)\n",
    "    files_output = await run_git_command(repo_path, [\"ls-files\", \"-c\", \"-o\", \"--exclude-standard\"])\n",
    "    file_paths = [f for f in files_output.split(\"\\n\") if f]\n",
    "\n",
    "    # Priority order: .yellhorncontext overrides .yellhornignore\n",
    "    yellhorncontext_path = repo_path / \".yellhorncontext\"\n",
    "    context_exists = yellhorncontext_path.exists() and yellhorncontext_path.is_file()\n",
    "\n",
    "    yellhornignore_path = repo_path / \".yellhornignore\"\n",
    "    ignore_exists = yellhornignore_path.exists() and yellhornignore_path.is_file()\n",
    "\n",
    "    # Initialize pattern lists\n",
    "    ignore_patterns = []\n",
    "    whitelist_patterns = []\n",
    "\n",
    "    # First try .yellhorncontext, then fall back to .yellhornignore\n",
    "    if context_exists:\n",
    "        # Read patterns from .yellhorncontext\n",
    "        with open(yellhorncontext_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                pattern = line.strip()\n",
    "                if pattern:\n",
    "                    if pattern.startswith(\"!\"):\n",
    "                        whitelist_patterns.append(pattern[1:])\n",
    "                    else:\n",
    "                        ignore_patterns.append(pattern)\n",
    "    elif ignore_exists:\n",
    "        # Read patterns from .yellhornignore\n",
    "        with open(yellhornignore_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                pattern = line.strip()\n",
    "                if pattern:\n",
    "                    ignore_patterns.append(pattern)\n",
    "\n",
    "    print(ignore_patterns)\n",
    "    # Apply filtering with fnmatch\n",
    "    if ignore_patterns or whitelist_patterns:\n",
    "        def is_ignored(file_path: str) -> bool:\n",
    "            # Check whitelist patterns first (take precedence)\n",
    "            for pattern in whitelist_patterns:\n",
    "                if fnmatch.fnmatch(file_path, pattern):\n",
    "                    return False  # Don't ignore whitelisted files\n",
    "\n",
    "            # Then check blacklist patterns\n",
    "            for pattern in ignore_patterns:\n",
    "                if fnmatch.fnmatch(file_path, pattern):\n",
    "                    return True  # Ignore matching files\n",
    "            return False\n",
    "\n",
    "        # Filter files\n",
    "        filtered_paths = [f for f in file_paths if not is_ignored(f)]\n",
    "        file_paths = filtered_paths\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = await get_codebase_snapshot(repo_path)\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'hello/.github/workflows/publish.yml'\n",
    "pat = '*.github/'\n",
    "fnmatch.fnmatch(path, pat.rstrip(\"/\") + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "whitelist_patterns = [\"python/**/*.py\", \"*.py\", \"*.ipynb\", \"*.md\"]\n",
    "ignore_patterns = [\".gitignore\", \".yellhornignore\", \"hello/*\"]\n",
    "\n",
    "# Use the same is_ignored function that get_codebase_snapshot uses\n",
    "def is_ignored(file_path: str) -> bool:\n",
    "    # First check if the file is whitelisted\n",
    "    for pattern in whitelist_patterns:\n",
    "        # Regular pattern matching (e.g., \"*.py\")\n",
    "        if fnmatch.fnmatch(file_path, pattern) or fnmatch.fnmatch(file_path, pattern.rstrip(\"/\") + \"/*\"):\n",
    "            return False  # Whitelisted, don't ignore\n",
    "    \n",
    "    # Then check if it matches any ignore patterns\n",
    "    for pattern in ignore_patterns:\n",
    "        # Regular pattern matching (e.g., \"*.log\")\n",
    "        if fnmatch.fnmatch(file_path, pattern) or fnmatch.fnmatch(file_path, pattern.rstrip(\"/\") + \"/*\"):\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnmatch.fnmatch(\"hello/hello/poetry.lock\", \"poetry.lock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ignored(\"hello/hello/hello.js\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sravan-yellhorn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
